[
  {
    "objectID": "10_Tutorials_LatentGrowthCurveModels.html",
    "href": "10_Tutorials_LatentGrowthCurveModels.html",
    "title": "Latent Growth Curve Models",
    "section": "",
    "text": "Latent growth curve models (LGCMs) are an advanced statistical approach that allows researchers to explore the trajectory of growth, decline, or stability over time within individuals. LGCMs have become indispensable in disciplines such as psychology, public health, and developmental research, owing to their capability to model individual differences in change trajectories and capture the underlying structure of growth. These models are an enhancement of structural equation modeling, designed specifically for longitudinal data, where the essence of change across multiple time points is of interest. LGCMs offer a flexible framework that can model both the average trajectory in the population (fixed effects) and individual deviations from this average (random effects). By representing time as a latent construct, these models capture the nuances of individual change while accounting for measurement error. This combination of capturing overarching growth patterns while also emphasizing individual differences is the hallmark of the “latent” growth curve models, making them a vital tool for understanding longitudinal patterns.\n\n\n\nLatent Growth Curve Model 1_Tutorial\n\n\n\n\nYou should consider latent growth curve models in the following situations:\n\nYou aim to explore: The trajectory of change over time for an observed variable.\nYour data: Contains repeated measures taken at multiple time points.\nYou seek to: Capture both the average growth pattern in the population and individual differences in that growth.\n\n\n\n\nIn this tutorial, we will guide you through an example of a latent growth curve model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of latent growth curve models.\nImplement these models on your own longitudinal data using appropriate statistical tools.\nAnalyze and draw meaningful inferences from the latent growth curve model outputs."
  },
  {
    "objectID": "10_Tutorials_LatentGrowthCurveModels.html#overview",
    "href": "10_Tutorials_LatentGrowthCurveModels.html#overview",
    "title": "Latent Growth Curve Models",
    "section": "",
    "text": "Latent growth curve models (LGCMs) are an advanced statistical approach that allows researchers to explore the trajectory of growth, decline, or stability over time within individuals. LGCMs have become indispensable in disciplines such as psychology, public health, and developmental research, owing to their capability to model individual differences in change trajectories and capture the underlying structure of growth. These models are an enhancement of structural equation modeling, designed specifically for longitudinal data, where the essence of change across multiple time points is of interest. LGCMs offer a flexible framework that can model both the average trajectory in the population (fixed effects) and individual deviations from this average (random effects). By representing time as a latent construct, these models capture the nuances of individual change while accounting for measurement error. This combination of capturing overarching growth patterns while also emphasizing individual differences is the hallmark of the “latent” growth curve models, making them a vital tool for understanding longitudinal patterns.\n\n\n\nLatent Growth Curve Model 1_Tutorial\n\n\n\n\nYou should consider latent growth curve models in the following situations:\n\nYou aim to explore: The trajectory of change over time for an observed variable.\nYour data: Contains repeated measures taken at multiple time points.\nYou seek to: Capture both the average growth pattern in the population and individual differences in that growth.\n\n\n\n\nIn this tutorial, we will guide you through an example of a latent growth curve model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of latent growth curve models.\nImplement these models on your own longitudinal data using appropriate statistical tools.\nAnalyze and draw meaningful inferences from the latent growth curve model outputs."
  },
  {
    "objectID": "10_Tutorials_LatentGrowthCurveModels.html#basic-example",
    "href": "10_Tutorials_LatentGrowthCurveModels.html#basic-example",
    "title": "Latent Growth Curve Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# Setting the seed for reproducibility\nset.seed(123)\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.3, 0.3, 0.3, \n                      0.3, 1, 0.3, 0.3,\n                      0.3, 0.3, 1, 0.3,\n                      0.3, 0.3, 0.3, 1), \n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame in wide format\ndata_wide &lt;- data.frame(\n  Individual = 1:n,\n  Job_Satisfaction_T1 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T2 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T3 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T4 = round(runif(n, 5, 10))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_wide)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Latent Growth Curve Model (LGCM), we will leverage the lavaan syntax to delineate both the measurement model and the structural model. The LGCM allows us to capture the trajectory of an outcome variable across multiple time points. By doing so, we can understand the inherent growth patterns and the variability around this growth, offering insights into individual differences and their development over time.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lavaan\" %in% installed.packages())) {\n  install.packages(\"lavaan\")\n}\n\nlibrary(lavaan)\n\n\n\n\nModel\n\n\nCode\n# Define the Latent Growth Curve Model\nmodel &lt;- '\n  # Latent variables\n    i =~ 1 * Job_Satisfaction_T1 + 1 * Job_Satisfaction_T2 + 1 * Job_Satisfaction_T3 + 1 * Job_Satisfaction_T4\n    s =~ 0 * Job_Satisfaction_T1 + 1 * Job_Satisfaction_T2 + 2 * Job_Satisfaction_T3 + 3 * Job_Satisfaction_T4\n\n  # Variances\n    i ~~ var_i * i\n    s ~~ var_s * s\n\n  # Covariance\n    i ~~ cov_is * s\n'\n\n# Fit the model\nfit &lt;- sem(model, data = data_wide)\n\n# Summary of the model\nsummary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)\n\n\nlavaan 0.6.16 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.904\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.593\n\nModel Test Baseline Model:\n\n  Test statistic                                 4.780\n  Degrees of freedom                                 6\n  P-value                                        0.572\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                      -0.797\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3605.325\n  Loglikelihood unrestricted model (H1)      -3604.373\n                                                      \n  Akaike (AIC)                                7224.650\n  Bayesian (BIC)                              7254.153\n  Sample-size adjusted Bayesian (SABIC)       7231.934\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.063\n  P-value H_0: RMSEA &lt;= 0.050                    0.887\n  P-value H_0: RMSEA &gt;= 0.080                    0.013\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.020\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Job_Stsfctn_T1    1.000                                  NA       NA\n    Job_Stsfctn_T2    1.000                                  NA       NA\n    Job_Stsfctn_T3    1.000                                  NA       NA\n    Job_Stsfctn_T4    1.000                                  NA       NA\n  s =~                                                                  \n    Job_Stsfctn_T1    0.000                                  NA       NA\n    Job_Stsfctn_T2    1.000                                  NA       NA\n    Job_Stsfctn_T3    2.000                                  NA       NA\n    Job_Stsfctn_T4    3.000                                  NA       NA\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s       (cv_s)    0.085    0.064    1.322    0.186    0.843    0.843\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (var_)   -0.168    0.140   -1.201    0.230       NA       NA\n    s       (vr_s)   -0.060    0.038   -1.573    0.116       NA       NA\n   .Jb_S_T1           2.241    0.199   11.287    0.000    2.241    1.081\n   .Jb_S_T2           2.234    0.151   14.813    0.000    2.234    1.027\n   .Jb_S_T3           2.220    0.150   14.752    0.000    2.220    1.032\n   .Jb_S_T4           2.433    0.209   11.642    0.000    2.433    1.089\n\nR-Square:\n                   Estimate\n    Job_Stsfctn_T1   -0.081\n    Job_Stsfctn_T2   -0.027\n    Job_Stsfctn_T3   -0.032\n    Job_Stsfctn_T4   -0.089\n\n\n\n\nFit Model\n\n\nCode\n# Fit the model\nfit &lt;- sem(model, data = data_wide, missing = \"FIML\")\n\n\n\n\nModel Summary\n\n\nCode\nsummary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)\n\n\nlavaan 0.6.16 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        11\n\n  Number of observations                           500\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 1.904\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.593\n\nModel Test Baseline Model:\n\n  Test statistic                                 4.780\n  Degrees of freedom                                 6\n  P-value                                        0.572\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                      -0.797\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)               -0.797\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3605.325\n  Loglikelihood unrestricted model (H1)      -3604.373\n                                                      \n  Akaike (AIC)                                7232.650\n  Bayesian (BIC)                              7279.011\n  Sample-size adjusted Bayesian (SABIC)       7244.096\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.063\n  P-value H_0: RMSEA &lt;= 0.050                    0.887\n  P-value H_0: RMSEA &gt;= 0.080                    0.013\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.063\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.887\n  P-value H_0: Robust RMSEA &gt;= 0.080             0.013\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.017\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    Job_Stsfctn_T1    1.000                                  NA       NA\n    Job_Stsfctn_T2    1.000                                  NA       NA\n    Job_Stsfctn_T3    1.000                                  NA       NA\n    Job_Stsfctn_T4    1.000                                  NA       NA\n  s =~                                                                  \n    Job_Stsfctn_T1    0.000                                  NA       NA\n    Job_Stsfctn_T2    1.000                                  NA       NA\n    Job_Stsfctn_T3    2.000                                  NA       NA\n    Job_Stsfctn_T4    3.000                                  NA       NA\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s       (cv_s)    0.085    0.063    1.342    0.180    0.843    0.843\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Job_Stsfctn_T1    7.456    0.064  115.822    0.000    7.456    5.180\n   .Job_Stsfctn_T2    7.446    0.066  112.900    0.000    7.446    5.049\n   .Job_Stsfctn_T3    7.488    0.066  114.172    0.000    7.488    5.106\n   .Job_Stsfctn_T4    7.550    0.067  112.931    0.000    7.550    5.050\n    i                 0.000                                  NA       NA\n    s                 0.000                                  NA       NA\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (var_)   -0.168    0.139   -1.209    0.227       NA       NA\n    s       (vr_s)   -0.060    0.037   -1.598    0.110       NA       NA\n   .Jb_S_T1           2.241    0.197   11.401    0.000    2.241    1.081\n   .Jb_S_T2           2.234    0.151   14.797    0.000    2.234    1.027\n   .Jb_S_T3           2.220    0.152   14.563    0.000    2.220    1.032\n   .Jb_S_T4           2.433    0.210   11.601    0.000    2.433    1.089\n\nR-Square:\n                   Estimate\n    Job_Stsfctn_T1   -0.081\n    Job_Stsfctn_T2   -0.027\n    Job_Stsfctn_T3   -0.032\n    Job_Stsfctn_T4   -0.089\n\n\n\n\n\nInterpreting the Results\nA Latent Growth Curve Model (LGCM) was used to understand the trajectory of scores over time. This model separates the growth into two latent factors: - Intercept (i): Represents the initial status or starting point. - Slope (s): Represents the rate of change over time.\nFrom the sem output, we can interpret: - The average starting point (intercept) for the scores: r round(fit@coef[“mu_i”], 3) - The average rate of change (slope) for the scores: r round(fit@coef[“mu_s”], 3)\nThe variance in the intercept and slope, r round(fit@coef[“var_i”], 3) and r round(fit@coef[“var_s”], 3) respectively, gives us information on individual differences in the initial status and rate of change. Additionally, the covariance between the intercept and slope, r round(fit@coef[“cov_is”], 3), informs us about the relationship between the initial status and the rate of change."
  },
  {
    "objectID": "10_Tutorials_LatentGrowthCurveModels.html#conclusion",
    "href": "10_Tutorials_LatentGrowthCurveModels.html#conclusion",
    "title": "Latent Growth Curve Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe LGCM provides a comprehensive understanding of the trajectory of scores over time. By examining the estimated intercept and slope, we can determine the general trend of scores across time points. Additionally, the variance and covariance parameters provide insights into individual differences in growth trajectories."
  },
  {
    "objectID": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html",
    "href": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html",
    "title": "Autoregressive Crosslagged Panel Models",
    "section": "",
    "text": "Autoregressive cross-lagged panel models (ARCL) are advanced statistical techniques tailored for dissecting the dynamic interplay between variables over time. ARCL models have become indispensable in disciplines like psychology, economics, and sociology, as they can unravel the bidirectional relationships and causality between multiple variables across various time points. These models extend the concept of autoregressive models, which predict a variable based on its past values, by also integrating cross-lagged effects, thereby capturing the influence of one variable on another in subsequent time points. Especially potent for longitudinal designs, ARCL models provide a nuanced understanding of how variables evolve and influence each other over time, shedding light on potential causal pathways. In essence, the ARCL framework seizes upon the time-ordered data to not only track how a variable relates to its past self but also how it might affect or be affected by other variables. This dual capability of capturing both autoregressive and cross-lagged effects makes ARCL models a go-to choice for researchers aiming to understand intricate temporal dynamics and reciprocal relationships in their data.\n\n\n\nAutoregressive Crosslagged Panel 1_Tutorial\n\n\n\n\nYou should consider autoregressive crosslagged panel models in the following situations:\n\nYou want to understand: The reciprocal relationships between variables over time.\nYour variables: Are repeatedly measured across multiple time points.\nYou aim to: Disentangle the cause-and-effect relationship between these variables over successive time intervals.\n\n\n\n\nIn this tutorial, we will guide you through an example of an autoregressive crosslagged panel model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of autoregressive crosslagged panel models.\nImplement these models on longitudinal panel data using R.\nAnalyze and discern the implications derived from the autoregressive crosslagged panel model analysis."
  },
  {
    "objectID": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#overview",
    "href": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#overview",
    "title": "Autoregressive Crosslagged Panel Models",
    "section": "",
    "text": "Autoregressive cross-lagged panel models (ARCL) are advanced statistical techniques tailored for dissecting the dynamic interplay between variables over time. ARCL models have become indispensable in disciplines like psychology, economics, and sociology, as they can unravel the bidirectional relationships and causality between multiple variables across various time points. These models extend the concept of autoregressive models, which predict a variable based on its past values, by also integrating cross-lagged effects, thereby capturing the influence of one variable on another in subsequent time points. Especially potent for longitudinal designs, ARCL models provide a nuanced understanding of how variables evolve and influence each other over time, shedding light on potential causal pathways. In essence, the ARCL framework seizes upon the time-ordered data to not only track how a variable relates to its past self but also how it might affect or be affected by other variables. This dual capability of capturing both autoregressive and cross-lagged effects makes ARCL models a go-to choice for researchers aiming to understand intricate temporal dynamics and reciprocal relationships in their data.\n\n\n\nAutoregressive Crosslagged Panel 1_Tutorial\n\n\n\n\nYou should consider autoregressive crosslagged panel models in the following situations:\n\nYou want to understand: The reciprocal relationships between variables over time.\nYour variables: Are repeatedly measured across multiple time points.\nYou aim to: Disentangle the cause-and-effect relationship between these variables over successive time intervals.\n\n\n\n\nIn this tutorial, we will guide you through an example of an autoregressive crosslagged panel model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of autoregressive crosslagged panel models.\nImplement these models on longitudinal panel data using R.\nAnalyze and discern the implications derived from the autoregressive crosslagged panel model analysis."
  },
  {
    "objectID": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#basic-example",
    "href": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#basic-example",
    "title": "Autoregressive Crosslagged Panel Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nSubject ID\nTime (in years)\nVariable 1 (e.g., job satisfaction)\n\n\nCreate Example Dataset\n\n\nCode\n# Setting the seed for reproducibility\nset.seed(123)\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.3, 0.3, 0.3, \n                      0.3, 1, 0.3, 0.3,\n                      0.3, 0.3, 1, 0.3,\n                      0.3, 0.3, 0.3, 1), \n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame in wide format\ndata_wide &lt;- data.frame(\n  Individual = 1:n,\n  Job_Satisfaction_T1 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T2 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T3 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T4 = round(runif(n, 5, 10)),\n  Life_Satisfaction_T1 = round(correlated_data[,1] * 2 + 7),\n  Life_Satisfaction_T2 = round(correlated_data[,2] * 2 + 7),\n  Life_Satisfaction_T3 = round(correlated_data[,3] * 2 + 7),\n  Life_Satisfaction_T4 = round(correlated_data[,4] * 2 + 7)\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_wide)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify an Autoregressive Crosslagged Panel Model (ARCL), we will utilize the lavaan syntax to focus on understanding the reciprocal influences between multiple variables over time. This approach allows us to discern how a change in one variable at a given time point can influence changes in another variable at subsequent time points, while also accounting for the autoregressive effects of each variable on itself across time.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lavaan\" %in% installed.packages())) install.packages(\"lavaan\")\nlibrary(lavaan)\n\n\n\n\nModel\n\n\nCode\nmodel &lt;- '\n  # Autoregressive paths\n  Job_Satisfaction_T2 ~ a1 * Job_Satisfaction_T1\n  Job_Satisfaction_T3 ~ a2 * Job_Satisfaction_T2\n  Job_Satisfaction_T4 ~ a3 * Job_Satisfaction_T3\n  \n  Life_Satisfaction_T2 ~ a4 * Life_Satisfaction_T1\n  Life_Satisfaction_T3 ~ a5 * Life_Satisfaction_T2\n  Life_Satisfaction_T4 ~ a6 * Life_Satisfaction_T3\n\n  # Cross-lagged paths\n  Job_Satisfaction_T2 ~ b1 * Life_Satisfaction_T1\n  Job_Satisfaction_T3 ~ b2 * Life_Satisfaction_T2\n  Job_Satisfaction_T4 ~ b3 * Life_Satisfaction_T3\n  \n  Life_Satisfaction_T2 ~ b4 * Job_Satisfaction_T1\n  Life_Satisfaction_T3 ~ b5 * Job_Satisfaction_T2\n  Life_Satisfaction_T4 ~ b6 * Job_Satisfaction_T3\n'\n\n\n\n\nFit Model\n\n\nCode\n# Fitting the model\nfit &lt;- sem(model, data = data_wide)\n\n\n\n\nModel Summary\n\n\nCode\n# Summarizing the model\nsummary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)\n\n\nlavaan 0.6.16 ended normally after 12 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               110.474\n  Degrees of freedom                                14\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               214.805\n  Degrees of freedom                                27\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.486\n  Tucker-Lewis Index (TLI)                       0.009\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5846.702\n  Loglikelihood unrestricted model (H1)      -5791.465\n                                                      \n  Akaike (AIC)                               11731.404\n  Bayesian (BIC)                             11811.481\n  Sample-size adjusted Bayesian (SABIC)      11751.174\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.117\n  90 Percent confidence interval - lower         0.098\n  90 Percent confidence interval - upper         0.138\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.999\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.076\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Job_Satisfaction_T2 ~                                                       \n    Jb_Sts_T1 (a1)         -0.042    0.046   -0.925    0.355   -0.042   -0.042\n  Job_Satisfaction_T3 ~                                                       \n    Jb_Sts_T2 (a2)         -0.019    0.044   -0.424    0.672   -0.019   -0.019\n  Job_Satisfaction_T4 ~                                                       \n    Jb_Sts_T3 (a3)         -0.023    0.046   -0.494    0.621   -0.023   -0.022\n  Life_Satisfaction_T2 ~                                                      \n    Lf_Sts_T1 (a4)          0.246    0.045    5.504    0.000    0.246    0.240\n  Life_Satisfaction_T3 ~                                                      \n    Lf_Sts_T2 (a5)          0.342    0.044    7.813    0.000    0.342    0.330\n  Life_Satisfaction_T4 ~                                                      \n    Lf_Sts_T3 (a6)          0.153    0.043    3.533    0.000    0.153    0.156\n  Job_Satisfaction_T2 ~                                                       \n    Lf_Sts_T1 (b1)         -0.042    0.034   -1.246    0.213   -0.042   -0.056\n  Job_Satisfaction_T3 ~                                                       \n    Lf_Sts_T2 (b2)         -0.013    0.033   -0.390    0.697   -0.013   -0.017\n  Job_Satisfaction_T4 ~                                                       \n    Lf_Sts_T3 (b3)          0.043    0.032    1.321    0.186    0.043    0.059\n  Life_Satisfaction_T2 ~                                                      \n    Jb_Sts_T1 (b4)         -0.001    0.060   -0.009    0.992   -0.001   -0.000\n  Life_Satisfaction_T3 ~                                                      \n    Jb_Sts_T2 (b5)         -0.013    0.059   -0.224    0.822   -0.013   -0.009\n  Life_Satisfaction_T4 ~                                                      \n    Jb_Sts_T3 (b6)          0.024    0.061    0.392    0.695    0.024    0.017\n\nCovariances:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .Job_Satisfaction_T4 ~~                                                      \n   .Lif_Stsfctn_T4         -0.003    0.133   -0.023    0.981   -0.003   -0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Job_Stsfctn_T2    2.166    0.137   15.811    0.000    2.166    0.996\n   .Job_Stsfctn_T3    2.144    0.136   15.811    0.000    2.144    0.999\n   .Job_Stsfctn_T4    2.227    0.141   15.811    0.000    2.227    0.996\n   .Lif_Stsfctn_T2    3.727    0.236   15.811    0.000    3.727    0.942\n   .Lif_Stsfctn_T3    3.784    0.239   15.811    0.000    3.784    0.891\n   .Lif_Stsfctn_T4    3.963    0.251   15.811    0.000    3.963    0.975\n\nR-Square:\n                   Estimate\n    Job_Stsfctn_T2    0.004\n    Job_Stsfctn_T3    0.001\n    Job_Stsfctn_T4    0.004\n    Lif_Stsfctn_T2    0.058\n    Lif_Stsfctn_T3    0.109\n    Lif_Stsfctn_T4    0.025\n\n\n\n\n\nInterpreting the Results\nAn Autoregressive cross-lagged panel model (ARCL) was used to explore the reciprocal relationship between Job Satisfaction and Life Satisfaction across multiple time points. This model allows us to understand how past values of a variable can predict its future values and how one variable can influence the future values of another variable.\nFrom the sem output, we can interpret: - Autoregressive paths: How past values of Job Satisfaction and Life Satisfaction predict their future values. - Coefficient for Job Satisfaction from T1 to T2: r round(fit@coef[“a1”], 3) - Coefficient for Life Satisfaction from T1 to T2: r round(fit@coef[“a4”], 3) (and so on for other time points) - Cross-lagged paths: How Job Satisfaction influences future values of Life Satisfaction and vice versa. - Coefficient of Job Satisfaction at T1 influencing Life Satisfaction at T2: r round(fit@coef[“b4”], 3) - Coefficient of Life Satisfaction at T1 influencing Job Satisfaction at T2: r round(fit@coef[“b1”], 3) (and so on for other time points)"
  },
  {
    "objectID": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#conclusion",
    "href": "8_Tutorials_AutoregressiveCrosslaggedPanelModels.html#conclusion",
    "title": "Autoregressive Crosslagged Panel Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe ARCL model provides insights into the dynamic relationship between Job Satisfaction and Life Satisfaction. If the coefficients of the cross-lagged paths are significant, it suggests that there’s a reciprocal influence between the two variables over time. The direction (positive or negative) and magnitude of these coefficients offer further understanding about the strength and nature of this relationship."
  },
  {
    "objectID": "6_Tutorials_GeneralizedEstimatingEquations.html",
    "href": "6_Tutorials_GeneralizedEstimatingEquations.html",
    "title": "Generalized Estimating Equations",
    "section": "",
    "text": "Generalized estimating equations (GEEs) are a pivotal statistical methodology, designed specifically to analyze correlated data often encountered in longitudinal studies. GEEs have become a staple in a plethora of disciplines, spanning public health, epidemiology, and social sciences, because of their proficiency in managing repeated measurements on the same subjects. While reminiscent of the general linear model (GLM), GEEs extend their capabilities by addressing correlations within clusters of data, such as measurements within the same individual. This makes GEEs particularly invaluable for longitudinal data, where observations are chronologically ordered and dependencies between them are inherent. The essence of GEEs lies in their population-averaged approach, focusing on modeling the average response over individuals while accounting for within-subject correlations. Unlike models that emphasize subject-specific effects, GEEs provide insights into the broader trends across the population. This emphasis on population-level patterns, coupled with its adeptness at handling correlated data, solidifies GEEs as a cornerstone methodology for longitudinal analyses.\n\n\nYou should use longitudinal generalized estimating equations in the following scenario:\n\nYou want to know: How the average outcome of a population evolves over time, especially when dealing with correlated or clustered data.\nYour variable: Is measured repeatedly across various time points or settings, capturing the inherent correlations within subjects.\nYou have: Repeated measurements on subjects that could be correlated, and you’re interested in the population-averaged effects rather than subject-specific effects.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal generalized estimating equations (GEEs) and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic principles of generalized estimating equations in longitudinal settings.\nFit a GEE model using example data in R.\nAnalyze and interpreting the results from the GEE analysis."
  },
  {
    "objectID": "6_Tutorials_GeneralizedEstimatingEquations.html#overview",
    "href": "6_Tutorials_GeneralizedEstimatingEquations.html#overview",
    "title": "Generalized Estimating Equations",
    "section": "",
    "text": "Generalized estimating equations (GEEs) are a pivotal statistical methodology, designed specifically to analyze correlated data often encountered in longitudinal studies. GEEs have become a staple in a plethora of disciplines, spanning public health, epidemiology, and social sciences, because of their proficiency in managing repeated measurements on the same subjects. While reminiscent of the general linear model (GLM), GEEs extend their capabilities by addressing correlations within clusters of data, such as measurements within the same individual. This makes GEEs particularly invaluable for longitudinal data, where observations are chronologically ordered and dependencies between them are inherent. The essence of GEEs lies in their population-averaged approach, focusing on modeling the average response over individuals while accounting for within-subject correlations. Unlike models that emphasize subject-specific effects, GEEs provide insights into the broader trends across the population. This emphasis on population-level patterns, coupled with its adeptness at handling correlated data, solidifies GEEs as a cornerstone methodology for longitudinal analyses.\n\n\nYou should use longitudinal generalized estimating equations in the following scenario:\n\nYou want to know: How the average outcome of a population evolves over time, especially when dealing with correlated or clustered data.\nYour variable: Is measured repeatedly across various time points or settings, capturing the inherent correlations within subjects.\nYou have: Repeated measurements on subjects that could be correlated, and you’re interested in the population-averaged effects rather than subject-specific effects.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal generalized estimating equations (GEEs) and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic principles of generalized estimating equations in longitudinal settings.\nFit a GEE model using example data in R.\nAnalyze and interpreting the results from the GEE analysis."
  },
  {
    "objectID": "6_Tutorials_GeneralizedEstimatingEquations.html#basic-example",
    "href": "6_Tutorials_GeneralizedEstimatingEquations.html#basic-example",
    "title": "Generalized Estimating Equations",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 2000 rows (4 rows for each of the 500 individuals).\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, 0.30, 0.30, \n                      0.30, 1, 0.30, 0.30, \n                      0.30, 0.30, 1, 0.30,\n                      0.30, 0.30, 0.30, 1),\n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data for 4 timepoints\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Convert the correlated data to probabilities using the plogis function\ncorrelated_probs &lt;- plogis(correlated_data)\n\n# Convert the probabilities to binary outcomes (using 0.5 as the threshold)\nbinary_outcomes &lt;- ifelse(correlated_probs &gt; 0.5, 1, 0)\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=4),\n  TimePoint = rep(c(\"T1\", \"T2\", \"T3\", \"T4\"), times=n),\n  Job_Satisfaction = as.vector(t(binary_outcomes))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Generalized Estimating Equation (GEE) model, we will utilize the geepack package in R. GEE models are designed to handle correlated data, making them suitable for longitudinal and other clustered data structures. In GEE, we focus on estimating the population-averaged effects of predictors while accounting for the within-cluster correlations using a specified working correlation structure.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"geepack\" %in% installed.packages())) install.packages(\"geepack\")\nlibrary(geepack)\n\n\n\n\nModel\n\n\nCode\n# Running the model\nmodel &lt;- geeglm(Job_Satisfaction ~ TimePoint, data = data_large, id = Individual, family = binomial(link = \"logit\"), corstr = \"exchangeable\")\n\n\n\n\nModel Summary\n\n\nCode\nprint(model)\n\n\n\nCall:\ngeeglm(formula = Job_Satisfaction ~ TimePoint, family = binomial(link = \"logit\"), \n    data = data_large, id = Individual, corstr = \"exchangeable\")\n\nCoefficients:\n(Intercept) TimePointT2 TimePointT3 TimePointT4 \n-0.10409389 -0.01605042  0.11209393  0.24834350 \n\nDegrees of Freedom: 2000 Total (i.e. Null);  1996 Residual\n\nScale Link:                   identity\nEstimated Scale Parameters:  [1] 1\n\nCorrelation:  Structure = exchangeable    Link = identity \nEstimated Correlation Parameters:\n    alpha \n0.1913765 \n\nNumber of clusters:   500   Maximum cluster size: 4 \n\n\n\n\n\nInterpreting the Results\nGeneralized Estimating Equations (GEE) were used to model the relationship between Job Satisfaction and TimePoint, taking into account the correlation of repeated measures within individuals. The model assumes a binomial distribution for Job Satisfaction with a logit link function and uses an exchangeable correlation structure. The output of the geeglm function provides estimates for the coefficients, along with their standard errors, Wald statistics, and p-values. The coefficient for TimePoint represents the change in the log odds of Job Satisfaction for a unit change in TimePoint.\nFor a detailed understanding, we can refer to: - Coefficient (Estimate) for TimePoint: NA - Associated p-value: NA"
  },
  {
    "objectID": "6_Tutorials_GeneralizedEstimatingEquations.html#conclusion",
    "href": "6_Tutorials_GeneralizedEstimatingEquations.html#conclusion",
    "title": "Generalized Estimating Equations",
    "section": "Conclusion",
    "text": "Conclusion\nIf the p-value for the TimePoint coefficient is less than 0.05, we can conclude that there is a statistically significant change in Job Satisfaction across the TimePoints according to the GEE model. Otherwise, we do not find evidence for such a change. The direction (increase or decrease) of this change can be determined by the sign of the coefficient for TimePoint."
  },
  {
    "objectID": "4_Tutorials_SignedRankTest.html",
    "href": "4_Tutorials_SignedRankTest.html",
    "title": "Signed-Rank Test",
    "section": "",
    "text": "The Signed-rank test, commonly referred to as the Wilcoxon signed-rank test, is a non-parametric statistical method used yo compare two related samples or repeated measurements on a single sample. Favored across fields like psychology, medicine, and economics, this test shines when data deviates from the assumptions of parametric tests, offering a robust alternative for evaluating median differences. Particularly beneficial for paired data, the signed-rank test delves into the depth of data, unveiling the nuances of change across two points in time. By addressing the ties and ranks within paired observations, this test imparts a comprehensive understanding of data distributions. It’s the “ranking” and “signing” of these paired differences that give this method its distinctive name.\n ### When to use a Signed-Rank Test? You should use a Wilcoxon Signed-Rank Test in the following scenario:\n\nYou want to know if two groups are different on your variable of interest\nYour variable of interest is continuous\nYou have two and only two groups\nYou have independent samples\nYou have a skewed variable of interest\n\n\n\nIn this tutorial, we will introduce the concept of the longitudinal signed-rank test and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of the longitudinal signed-rank test.\nPerform a longitudinal signed-rank test using example data in R.\nInterpret the results of the test."
  },
  {
    "objectID": "4_Tutorials_SignedRankTest.html#overview",
    "href": "4_Tutorials_SignedRankTest.html#overview",
    "title": "Signed-Rank Test",
    "section": "",
    "text": "The Signed-rank test, commonly referred to as the Wilcoxon signed-rank test, is a non-parametric statistical method used yo compare two related samples or repeated measurements on a single sample. Favored across fields like psychology, medicine, and economics, this test shines when data deviates from the assumptions of parametric tests, offering a robust alternative for evaluating median differences. Particularly beneficial for paired data, the signed-rank test delves into the depth of data, unveiling the nuances of change across two points in time. By addressing the ties and ranks within paired observations, this test imparts a comprehensive understanding of data distributions. It’s the “ranking” and “signing” of these paired differences that give this method its distinctive name.\n ### When to use a Signed-Rank Test? You should use a Wilcoxon Signed-Rank Test in the following scenario:\n\nYou want to know if two groups are different on your variable of interest\nYour variable of interest is continuous\nYou have two and only two groups\nYou have independent samples\nYou have a skewed variable of interest\n\n\n\nIn this tutorial, we will introduce the concept of the longitudinal signed-rank test and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of the longitudinal signed-rank test.\nPerform a longitudinal signed-rank test using example data in R.\nInterpret the results of the test."
  },
  {
    "objectID": "4_Tutorials_SignedRankTest.html#basic-example",
    "href": "4_Tutorials_SignedRankTest.html#basic-example",
    "title": "Signed-Rank Test",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at two time points: T1-T2.\n\nCreate Example Dataset\n\n\nCode\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data (2x2 for two time points)\ncor_matrix &lt;- matrix(c(1, 0.30, \n                      0.30, 1),\n                      nrow=2)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data for 2 timepoints\nrandom_data &lt;- matrix(rnorm(n * 2), ncol=2)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame in wide format\ndata_wide &lt;- data.frame(\n  Individual = 1:n,\n  Job_Satisfaction_T1 = correlated_data[,1],\n  Job_Satisfaction_T2 = correlated_data[,2]\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_wide)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo conduct a Signed Rank Test, we will leverage the stats package in R, specifically the wilcox.test function. This non-parametric test is employed when tracking changes in a single group over two time points. In the Signed Rank Test, we aim to test the hypothesis that the distribution of differences between paired measurements remains symmetric about zero, without making assumptions about the specific form of the population distribution.\n\nInstall and Load Necessary Libraries\n\n\nCode\n# Load necessary packages\nif (!(\"stats\" %in% installed.packages())) install.packages(\"stats\")\nlibrary(stats)\n\n\n\n\nModel\n\n\nCode\n# Conducting the Wilcoxon Signed-Rank Test\ntest_result &lt;- wilcox.test(data_wide$Job_Satisfaction_T1, data_wide$Job_Satisfaction_T2, paired = TRUE)\n\n\n\n\nModel Summary\n\n\nCode\n# Displaying the result\nprint(test_result)\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data_wide$Job_Satisfaction_T1 and data_wide$Job_Satisfaction_T2\nV = 61071, p-value = 0.6308\nalternative hypothesis: true location shift is not equal to 0\n\n\nCode\nsummary(test_result)\n\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   0      -none- NULL     \np.value     1      -none- numeric  \nnull.value  1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\n\n\n\n\nInterpreting the Results\nThe Wilcoxon signed-rank test was conducted to compare Job Satisfaction scores at two time points: T1 and T2. This test is a non-parametric alternative to the paired t-test and is suitable when the data might not meet the normality assumption. The test yielded a W statistic of W=6.1071^{4}. The associated p-value is p=0.631. If this p-value is less than the common significance level of 0.05, we would reject the null hypothesis, indicating a statistically significant difference between the two time points.\nBy using the summary() and plot() functions in R, we can also obtain additional details about the test and visually inspect the differences between the paired observations."
  },
  {
    "objectID": "4_Tutorials_SignedRankTest.html#conclusion",
    "href": "4_Tutorials_SignedRankTest.html#conclusion",
    "title": "Signed-Rank Test",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the Wilcoxon signed-rank test results, if the p-value is less than 0.05, we conclude that there is a statistically significant difference in Job Satisfaction scores between the two time points, T1 and T2. Otherwise, there is no significant difference between the two time points."
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html",
    "href": "3a_Examples_LinearMixedModels.html",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "The LMM:ri is similar to traditional (fixed-effect) linear regression extending on this approach by including a subject-specific random-effect that allows each participant to have their own unique intercept value, in addition to the overall mean-level (fixed-effect) intercept value zzzzt\nIn this example, we will use the LMM:ri to analyze trajectories of height obtained across multiple measurement occasions in a sample of youth taking part in the ABCD Study. Our primary aim is to characterize stability and change in height assessments, while accounting for observations that are clustered within youth over time. To do so, we will use the LMM:ri to simultaneously model an overall sample mean trajectory (fixed effect) and subject-specific (random) effects that vary randomly about the sample mean trajectory.\n\n\n\n\n\nInstall PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\",\"report\",\"broom\",\"gridExtra\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\nlibrary(report)       # Easy reporting of regression analyses\nlibrary(broom)        # Tidy and augment statistical models output\nlibrary(gridExtra)    # Arrange multiple grid-based plots on a page\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)\n\n\n\n\n\n\n\n\n\n\nRead and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: panel-tabset ### Build Model {.tabset .tabset-fade .tabset-pills}\n::: blue\nThe code fits a linear mixed model to predict the ‘Height’ variable based on time points (‘eventname’) and handedness (‘Handedness’), while accounting for individual-level variability by including random intercepts for each participant (‘src_subject_id’). The results of the model are then printed to provide a summary of the fitted model parameters.\nSTEP 1: Compute LMM Model with Random Intercepts\n\n\nCode\n## Linear Mixed Model with a random intercept (LMM-ri)\nrandom_intercepts &lt;- lmer(anthroheightcalc ~ 1 + eventname + sex + (1|src_subject_id), data = df, REML=T)\n\nprint(random_intercepts)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + sex + (1 | src_subject_id)\n   Data: df\nREML criterion at convergence: 201984.7\nRandom effects:\n Groups         Name        Std.Dev.\n src_subject_id (Intercept) 2.960   \n Residual                   2.263   \nNumber of obs: 40172, groups:  src_subject_id, 11867\nFixed Effects:\n     (Intercept)       eventname.L       eventname.Q       eventname.C  \n        60.02482           7.54116          -0.17358          -0.07034  \n     eventname^4  sexMale or other  \n         0.15413           0.07247  \n\n\n\n\ntesting\n## Output and reports extending from the LMM-ri analyses\nsummary(random_intercepts)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + sex + (1 | src_subject_id)\n   Data: df\n\nREML criterion at convergence: 201984.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-20.999  -0.306  -0.011   0.315  65.930 \n\nRandom effects:\n Groups         Name        Variance Std.Dev.\n src_subject_id (Intercept) 8.760    2.960   \n Residual                   5.123    2.263   \nNumber of obs: 40172, groups:  src_subject_id, 11867\n\nFixed effects:\n                 Estimate Std. Error  t value\n(Intercept)      60.02482    0.04345 1381.536\neventname.L       7.54116    0.03084  244.546\neventname.Q      -0.17358    0.02898   -5.990\neventname.C      -0.07034    0.03198   -2.200\neventname^4       0.15413    0.02863    5.383\nsexMale or other  0.07247    0.05932    1.222\n\nCorrelation of Fixed Effects:\n            (Intr) evnt.L evnt.Q evnt.C evnt^4\neventname.L  0.154                            \neventname.Q  0.016  0.258                     \neventname.C -0.052 -0.031  0.389              \neventname^4 -0.049 -0.089  0.208  0.435       \nsexMlorothr -0.713 -0.002  0.001 -0.001 -0.002\n\n\ntesting\nconfint(random_intercepts, level = 0.95, method = \"Wald\")\n\n\n                       2.5 %       97.5 %\n.sig01                    NA           NA\n.sigma                    NA           NA\n(Intercept)      59.93966182 60.109974376\neventname.L       7.48072019  7.601600409\neventname.Q      -0.23037985 -0.116784560\neventname.C      -0.13301716 -0.007663936\neventname^4       0.09800919  0.210243694\nsexMale or other -0.04379154  0.188725847\n\n\ntesting\nreport_performance(random_intercepts)\n\n\nThe model's total explanatory power is substantial (conditional R2 = 0.78) and\nthe part related to the fixed effects alone (marginal R2) is of 0.41\n\n\nThe code provided executes a linear mixed model (LMM) to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4), and also takes into account their handedness (Handedness). This model accounts for individual variability in height by including a random intercept for each subject (src_subject_id). The output indicates that the model was fit using the REML (Restricted Maximum Likelihood) criterion. The random intercept for the src_subject_id has a standard deviation of 2.147 units, and the residual standard deviation is 2.257 units. The fixed effects section provides the coefficients for the intercept, eventname, and two levels of the Handedness variable.\nFrom the model summary: The intercept, corresponding to the reference levels of the predictors (presumably the baseline event and right-handedness), is estimated at r fixed_effects(random_intercepts)[1] units, and this effect is highly significant. The effect of the linear term for eventname is positive and statistically significant, suggesting an increase of approximately r eventname_effect units in height for each unit change in eventname. Regarding handedness, children who are left-handed show a non-significant increase in height of about r left_handed_effect units compared to right-handed children, while those with mixed-handedness show a significant increase of r mixed_handed_effect units.\nThe model’s explanatory power is highlighted by a conditional R2 of r conditional_R2, meaning it explains **r conditional_R2*100% of the total variance when considering both fixed and random effects. The marginal R2, which considers only the fixed effects, is r marginal_R2**.\n\n\nThe following set of plots are used to facilitate model diagnostics. The first is a histogram showcasing the distribution of random intercepts for individual subjects, indicating variations in height not explained by the fixed effects. The second depicts residuals versus fitted values, helping assess the model’s fit and potential heteroscedasticity. The third contrasts observed and predicted height values across different time points, offering a side-by-side evaluation of the model’s predictions against actual observations.\n::: blue\n\n\nCode\n# Extract the random effects\nrandom_effects &lt;- ranef(random_intercepts)[[1]]\n\n# Convert to dataframe\nrandom_effects_df &lt;- data.frame(Intercept = random_effects$`(Intercept)`)\n\n# Plot 1: Histogram\nhist_plot &lt;- ggplot(random_effects_df, aes(x = Intercept)) +\n  geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", fill = \"lightblue\") + labs(title = \"Histogram of Random Effects\", x = \"Random Intercept Values\", y = \"Density\") +\n  theme_minimal()\n\n# Plot 2: Residuals vs Fitted Values\nresid_plot &lt;- ggplot(NULL, aes(x = fitted_values, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residuals vs Fitted Values\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\nCode\n# Extract the data frame used in the model\nmodel_data &lt;- random_intercepts@frame\n\n# Extract unique subject IDs from the model's data\noriginal_subject_ids &lt;- unique(model_data$src_subject_id)\n\n# Subset the original data to include only those subjects\ndf_subset &lt;- df %&gt;% filter(src_subject_id %in% original_subject_ids)\n\neventname_map &lt;- c(\n  \"baseline_year_1_arm_1\" = \"Baseline\",\n  \"1_year_follow_up_y_arm_1\" = \"Year_1\",\n  \"2_year_follow_up_y_arm_1\" = \"Year_2\",\n  \"3_year_follow_up_y_arm_1\" = \"Year_3\",\n  \"4_year_follow_up_y_arm_1\" = \"Year_4\"\n)\n\n# Apply the recoding to the eventname variable\ndf_subset$eventname &lt;- factor(df_subset$eventname, levels = names(eventname_map), labels = eventname_map)\n\n# Verify the recoding\ntable(df_subset$eventname)\n\n\n\nBaseline   Year_1   Year_2   Year_3   Year_4 \n   11867    11220    10973    10336     4754 \n\n\nCode\n# Generate the plot\nggplot(df_subset, aes(x = eventname, y = anthroheightcalc, group = src_subject_id)) +\n  # Individual estimated height trajectories in faded lines\n  geom_line(aes(group = src_subject_id), alpha = 0.3, color = \"grey50\") +\n  # Overall group-mean trajectory in blue with increased thickness\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\", color = \"blue\", linewidth = 1) +\n  labs(title = \"Individual and Group-Mean Height Trajectories\",\n       x = \"Event Name\",\n       y = \"Height\") +\n  theme_minimal()\n\n\n\n\n\nThe provided code visualizes individual and group-mean height trajectories over different event names. Individual height trajectories for each subject are depicted as faded gray lines, allowing for a clear view of the variability among subjects. In contrast, the overall group-mean trajectory, which represents the average trend across all individuals for each event name, is highlighted in blue. The average height at r mean(df_descriptableHeight) has increased by **r mean(df_descriptableHeight_followup - df_descriptable$Height)** units from baseline. The attached chart effectively communicates these trajectories, emphasizing the average trend of the group while also showcasing individual differences.\n\n\n\n\n\n\n\nWrite-up\n\n\n\nThe linear mixed model analysis was conducted to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4) using the event name (eventname) and handedness (Handedness). The eventname predictor was statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[2], digits = 3). For the handedness variable, children with mixed-handedness showed a significant increase in height of about r round(fixed_effectsEstimate[4], 4) units compared to right-handed children with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[4], digits = 3). On the other hand, left-handed children's height increase, although positive at r round(fixed_effectsEstimate[3], 4) units, was not statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[3], digits = 3). The model's overall ability to explain the variance in height was substantial, with a conditional R^2 of r report_performance(random_intercepts)Conditional R2[1], indicating that it accounted for this proportion of the variability in height when considering both fixed and random effects. The marginal R^2 was r report_performance(random_intercepts)$Marginal R2[1], meaning that the fixed effects alone explained this proportion of the variability."
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html#overview",
    "href": "3a_Examples_LinearMixedModels.html#overview",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "The LMM:ri is similar to traditional (fixed-effect) linear regression extending on this approach by including a subject-specific random-effect that allows each participant to have their own unique intercept value, in addition to the overall mean-level (fixed-effect) intercept value zzzzt\nIn this example, we will use the LMM:ri to analyze trajectories of height obtained across multiple measurement occasions in a sample of youth taking part in the ABCD Study. Our primary aim is to characterize stability and change in height assessments, while accounting for observations that are clustered within youth over time. To do so, we will use the LMM:ri to simultaneously model an overall sample mean trajectory (fixed effect) and subject-specific (random) effects that vary randomly about the sample mean trajectory."
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html#preliminary-setup",
    "href": "3a_Examples_LinearMixedModels.html#preliminary-setup",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "Install PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\",\"report\",\"broom\",\"gridExtra\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\nlibrary(report)       # Easy reporting of regression analyses\nlibrary(broom)        # Tidy and augment statistical models output\nlibrary(gridExtra)    # Arrange multiple grid-based plots on a page\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)"
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html#descriptives-overview",
    "href": "3a_Examples_LinearMixedModels.html#descriptives-overview",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "Read and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title"
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html#results",
    "href": "3a_Examples_LinearMixedModels.html#results",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "::: panel-tabset ### Build Model {.tabset .tabset-fade .tabset-pills}\n::: blue\nThe code fits a linear mixed model to predict the ‘Height’ variable based on time points (‘eventname’) and handedness (‘Handedness’), while accounting for individual-level variability by including random intercepts for each participant (‘src_subject_id’). The results of the model are then printed to provide a summary of the fitted model parameters.\nSTEP 1: Compute LMM Model with Random Intercepts\n\n\nCode\n## Linear Mixed Model with a random intercept (LMM-ri)\nrandom_intercepts &lt;- lmer(anthroheightcalc ~ 1 + eventname + sex + (1|src_subject_id), data = df, REML=T)\n\nprint(random_intercepts)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + sex + (1 | src_subject_id)\n   Data: df\nREML criterion at convergence: 201984.7\nRandom effects:\n Groups         Name        Std.Dev.\n src_subject_id (Intercept) 2.960   \n Residual                   2.263   \nNumber of obs: 40172, groups:  src_subject_id, 11867\nFixed Effects:\n     (Intercept)       eventname.L       eventname.Q       eventname.C  \n        60.02482           7.54116          -0.17358          -0.07034  \n     eventname^4  sexMale or other  \n         0.15413           0.07247  \n\n\n\n\ntesting\n## Output and reports extending from the LMM-ri analyses\nsummary(random_intercepts)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + sex + (1 | src_subject_id)\n   Data: df\n\nREML criterion at convergence: 201984.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-20.999  -0.306  -0.011   0.315  65.930 \n\nRandom effects:\n Groups         Name        Variance Std.Dev.\n src_subject_id (Intercept) 8.760    2.960   \n Residual                   5.123    2.263   \nNumber of obs: 40172, groups:  src_subject_id, 11867\n\nFixed effects:\n                 Estimate Std. Error  t value\n(Intercept)      60.02482    0.04345 1381.536\neventname.L       7.54116    0.03084  244.546\neventname.Q      -0.17358    0.02898   -5.990\neventname.C      -0.07034    0.03198   -2.200\neventname^4       0.15413    0.02863    5.383\nsexMale or other  0.07247    0.05932    1.222\n\nCorrelation of Fixed Effects:\n            (Intr) evnt.L evnt.Q evnt.C evnt^4\neventname.L  0.154                            \neventname.Q  0.016  0.258                     \neventname.C -0.052 -0.031  0.389              \neventname^4 -0.049 -0.089  0.208  0.435       \nsexMlorothr -0.713 -0.002  0.001 -0.001 -0.002\n\n\ntesting\nconfint(random_intercepts, level = 0.95, method = \"Wald\")\n\n\n                       2.5 %       97.5 %\n.sig01                    NA           NA\n.sigma                    NA           NA\n(Intercept)      59.93966182 60.109974376\neventname.L       7.48072019  7.601600409\neventname.Q      -0.23037985 -0.116784560\neventname.C      -0.13301716 -0.007663936\neventname^4       0.09800919  0.210243694\nsexMale or other -0.04379154  0.188725847\n\n\ntesting\nreport_performance(random_intercepts)\n\n\nThe model's total explanatory power is substantial (conditional R2 = 0.78) and\nthe part related to the fixed effects alone (marginal R2) is of 0.41\n\n\nThe code provided executes a linear mixed model (LMM) to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4), and also takes into account their handedness (Handedness). This model accounts for individual variability in height by including a random intercept for each subject (src_subject_id). The output indicates that the model was fit using the REML (Restricted Maximum Likelihood) criterion. The random intercept for the src_subject_id has a standard deviation of 2.147 units, and the residual standard deviation is 2.257 units. The fixed effects section provides the coefficients for the intercept, eventname, and two levels of the Handedness variable.\nFrom the model summary: The intercept, corresponding to the reference levels of the predictors (presumably the baseline event and right-handedness), is estimated at r fixed_effects(random_intercepts)[1] units, and this effect is highly significant. The effect of the linear term for eventname is positive and statistically significant, suggesting an increase of approximately r eventname_effect units in height for each unit change in eventname. Regarding handedness, children who are left-handed show a non-significant increase in height of about r left_handed_effect units compared to right-handed children, while those with mixed-handedness show a significant increase of r mixed_handed_effect units.\nThe model’s explanatory power is highlighted by a conditional R2 of r conditional_R2, meaning it explains **r conditional_R2*100% of the total variance when considering both fixed and random effects. The marginal R2, which considers only the fixed effects, is r marginal_R2**.\n\n\nThe following set of plots are used to facilitate model diagnostics. The first is a histogram showcasing the distribution of random intercepts for individual subjects, indicating variations in height not explained by the fixed effects. The second depicts residuals versus fitted values, helping assess the model’s fit and potential heteroscedasticity. The third contrasts observed and predicted height values across different time points, offering a side-by-side evaluation of the model’s predictions against actual observations.\n::: blue\n\n\nCode\n# Extract the random effects\nrandom_effects &lt;- ranef(random_intercepts)[[1]]\n\n# Convert to dataframe\nrandom_effects_df &lt;- data.frame(Intercept = random_effects$`(Intercept)`)\n\n# Plot 1: Histogram\nhist_plot &lt;- ggplot(random_effects_df, aes(x = Intercept)) +\n  geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", fill = \"lightblue\") + labs(title = \"Histogram of Random Effects\", x = \"Random Intercept Values\", y = \"Density\") +\n  theme_minimal()\n\n# Plot 2: Residuals vs Fitted Values\nresid_plot &lt;- ggplot(NULL, aes(x = fitted_values, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residuals vs Fitted Values\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\nCode\n# Extract the data frame used in the model\nmodel_data &lt;- random_intercepts@frame\n\n# Extract unique subject IDs from the model's data\noriginal_subject_ids &lt;- unique(model_data$src_subject_id)\n\n# Subset the original data to include only those subjects\ndf_subset &lt;- df %&gt;% filter(src_subject_id %in% original_subject_ids)\n\neventname_map &lt;- c(\n  \"baseline_year_1_arm_1\" = \"Baseline\",\n  \"1_year_follow_up_y_arm_1\" = \"Year_1\",\n  \"2_year_follow_up_y_arm_1\" = \"Year_2\",\n  \"3_year_follow_up_y_arm_1\" = \"Year_3\",\n  \"4_year_follow_up_y_arm_1\" = \"Year_4\"\n)\n\n# Apply the recoding to the eventname variable\ndf_subset$eventname &lt;- factor(df_subset$eventname, levels = names(eventname_map), labels = eventname_map)\n\n# Verify the recoding\ntable(df_subset$eventname)\n\n\n\nBaseline   Year_1   Year_2   Year_3   Year_4 \n   11867    11220    10973    10336     4754 \n\n\nCode\n# Generate the plot\nggplot(df_subset, aes(x = eventname, y = anthroheightcalc, group = src_subject_id)) +\n  # Individual estimated height trajectories in faded lines\n  geom_line(aes(group = src_subject_id), alpha = 0.3, color = \"grey50\") +\n  # Overall group-mean trajectory in blue with increased thickness\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\", color = \"blue\", linewidth = 1) +\n  labs(title = \"Individual and Group-Mean Height Trajectories\",\n       x = \"Event Name\",\n       y = \"Height\") +\n  theme_minimal()\n\n\n\n\n\nThe provided code visualizes individual and group-mean height trajectories over different event names. Individual height trajectories for each subject are depicted as faded gray lines, allowing for a clear view of the variability among subjects. In contrast, the overall group-mean trajectory, which represents the average trend across all individuals for each event name, is highlighted in blue. The average height at r mean(df_descriptableHeight) has increased by **r mean(df_descriptableHeight_followup - df_descriptable$Height)** units from baseline. The attached chart effectively communicates these trajectories, emphasizing the average trend of the group while also showcasing individual differences."
  },
  {
    "objectID": "3a_Examples_LinearMixedModels.html#wrapping-up",
    "href": "3a_Examples_LinearMixedModels.html#wrapping-up",
    "title": "Linear Mixed Models: Random Intercept",
    "section": "",
    "text": "Write-up\n\n\n\nThe linear mixed model analysis was conducted to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4) using the event name (eventname) and handedness (Handedness). The eventname predictor was statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[2], digits = 3). For the handedness variable, children with mixed-handedness showed a significant increase in height of about r round(fixed_effectsEstimate[4], 4) units compared to right-handed children with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[4], digits = 3). On the other hand, left-handed children's height increase, although positive at r round(fixed_effectsEstimate[3], 4) units, was not statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[3], digits = 3). The model's overall ability to explain the variance in height was substantial, with a conditional R^2 of r report_performance(random_intercepts)Conditional R2[1], indicating that it accounted for this proportion of the variability in height when considering both fixed and random effects. The marginal R^2 was r report_performance(random_intercepts)$Marginal R2[1], meaning that the fixed effects alone explained this proportion of the variability."
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "This example assesses whether growth in a subsample of ABCD participants from Baseline (T0) to the 1-Year follow-up (T1) differs significantly based on handedness, using height (“anthroheightcalc”) as a representative metric of growth. The analysis is conducted in two primary steps: 1) a difference score is calculated between baseline and Year_1 height measurements for each participant; 2) a simple regression analysis is used to test whether sex (boy, girl) predicts the average difference value in participants height from baseline to the 1-Year follow-up. Finally, a visual inspection is further conducted via a violin plot to graphically represent the relationship between difference scores and sex. The ensuing analysis and interpretations are detailed in the subsequent sections.\n\n\n\n\n\nInstall PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# rm(list = ls())\n\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\")\n# ,\"rstatix\"\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)\n\n\n\n\n\n\n\n\n\n\nRead and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild Model: Step 1Build Model: Step 2Model Plots\n\n\n\nThe code snippet below tells R to compute a difference score by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nSTEP 1: Compute Difference Score\n\n\nCode\n# Define the function to compute difference scores for a given variable and provide a summary\n# Function to compute difference scores for a given variable and provide a summary\ncompute_difference_and_summary &lt;- function(df, variable_name) {\n    # Define the event names of interest\n    baseline_event &lt;- \"baseline_year_1_arm_1\"\n    followup_event &lt;- \"1_year_follow_up_y_arm_1\"\n\n    # Compute the difference between Baseline and Year 1 data for the given variable\n    diff_data &lt;- df %&gt;%\n        filter(eventname %in% c(baseline_event, followup_event)) %&gt;% # Filter for specific event names\n        select(src_subject_id, eventname, all_of(variable_name)) %&gt;% # Select required columns\n        spread(eventname, variable_name) %&gt;% # Convert data from long to wide format\n        mutate(diff = get(followup_event) - get(baseline_event)) %&gt;% # Compute difference between the two time points\n        drop_na(diff) # Exclude rows with NA in the computed difference\n\n    # Summarize the computed difference scores\n    diff_summary &lt;- summary(diff_data$diff)\n\n    # Return the difference data and its summary\n    list(data = diff_data, summary = diff_summary)\n}\n\n# List of variables for which difference scores are to be computed\nvariables_of_interest &lt;- c(\"anthroheightcalc\")\n\n# Compute and store difference scores and summaries for each variable in a list\ndifference_and_summary_list &lt;- lapply(variables_of_interest, function(var) {\n    compute_difference_and_summary(df, var)\n})\n\n# Extract the difference data for the 'anthroheightcalc' variable\nheight_diff_data &lt;- difference_and_summary_list[[1]]$data\n\n# Merge the 'diff' column back to the main df using 'src_subject_id' as the key\ndf &lt;- left_join(df, height_diff_data %&gt;% select(src_subject_id, diff), by = \"src_subject_id\")\n\n\nDescriptive Statistics for the Difference Score\n\n\nCode\n# Compute statistical summaries for the difference score variable\nlapply(difference_and_summary_list, function(item) {\n    print(item$summary)\n})\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\n[[1]]\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\nThis summary of the difference score variable indicates that the differences range from a decrease of -51.875 units to an increase of 67 units. The median difference is 2.25 units, and the average difference is approximately 2.3660611 units. There are 0 missing values in this difference score dataset.\nDescriptive Statistics for the Height Variable\n\n\nCode\n## Summary statistics\n# Compute summary statistics for Height by eventname\nsummary &lt;- df %&gt;%\n    group_by(eventname) %&gt;%\n    get_summary_stats(anthroheightcalc, type = \"mean_sd\")\n\ndata.frame(summary)\n\n\n\n\n  \n\n\n\nThe summary statistics provide insights into the Height variable across different eventname categories. At baseline, the average height is approximately 55.241 units with a standard deviation of 3.331. Over the years, there’s a noticeable increase in average height: by the first year, it’s about 57.595 units, and it continues to rise, reaching approximately 64.694 units by the fourth year. The standard deviation remains relatively consistent over the years, suggesting similar variability in height measurements across these time points.\n\n\n\n\nA simple regression analyses is conducted to examine whether a grouping variable (sex) significantly predicts the difference score value (indicating significant group differences in the average difference score).\nSTEP 2: Conduct regression on Difference Score\n\n\nCode\n# Merge the 'sex' variable from the original dataframe 'df' to 'height_diff_data'\nmerged_data &lt;- height_diff_data %&gt;%\n  left_join(df %&gt;% select(src_subject_id, sex), by = \"src_subject_id\")\n\n# Ensure 'sex' is a factor\nmerged_data$sex &lt;- as.factor(merged_data$sex)\n\n# Run the regression\nmodel &lt;- lm(diff ~ sex, data = merged_data)\n\n# Get the summary of the regression model\nmodel_summary &lt;- summary(model)\n\nmodel_summary\n\n\n\nCall:\nlm(formula = diff ~ sex, data = merged_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-54.41  -0.59  -0.11   0.47  64.47 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.5308     0.0167   151.8   &lt;2e-16 ***\nsexMale or other  -0.3376     0.0230   -14.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.51 on 47586 degrees of freedom\nMultiple R-squared:  0.00449,   Adjusted R-squared:  0.00447 \nF-statistic:  215 on 1 and 47586 DF,  p-value: &lt;2e-16\n\n\nThis regression analysis evaluates whether sex (boy, girl) predicts the average difference in participants’ height from baseline to the 1-Year follow-up. The output from our model provides:\n\nan F-statistic of 214.6560227;\ndegrees of freedom of 47586;\na parameter estimate of -0.3376126;\nstandard error of 0.0230434;\np-value of 1.6900378^{-48}.\n\nCompared to boys (the reference group), girls have an average increase in height difference of approximately -0.338 units. This effect was marginally significant with a p-value of 0. Overall, the model explained a very small portion of the variance in height difference, with an adjusted R-squared value of 0.00447, and the overall model significance was not statistically significant with a p-value of 1.6900378^{-48}.\n\n\n\n\n\n\ntesting\n# Visualize the difference scores across different levels of sex\n\n# Create a violin plot to show the distribution of difference scores by sex\n# Jittered points are added to provide a more granular view of individual observations\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n# Merge the 'sex' variable from the original dataframe 'df' to 'height_diff_data'\nmerged_data &lt;- height_diff_data %&gt;%\n  left_join(df %&gt;% select(src_subject_id, sex), by = \"src_subject_id\")\n\n# Plotting using ggplot2\nggplot(merged_data, aes(x = sex, y = diff, fill = sex)) +\n  geom_violin() +\n  geom_jitter(position = position_jitter(width = 0.2), size = 1, alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") + \n  labs(\n    title = \"Difference Scores by Sex\",\n    x = \"Sex\",\n    y = \"Difference Score\"\n  ) +\n  theme_minimal() + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nThe violin plot visualizes the distribution of difference scores in children’s heights by their sex: Right, Left, and Mixed. Each violin shape provides insight into the density of the data at different height differences, with wider sections representing higher densities of data points. Superimposed jittered points offer a granular view of individual observations. From the plot, it appears that the distributions of height differences across the three handedness categories are somewhat similar, though there might be subtle variations in median and spread.\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\nDifferences in children’s heights between baseline and a subsequent 1-year follow-up, we sought to understand the potential influence of sex on this difference. Descriptive statistics revealed that height differences ranged from a decrease of -51.875 units to an increase of 67 units, with a median difference of 2.25 units and an average difference of approximately 2.3542189 units. Subsequent regression analysis indicated that, when compared to boys, girls had an average increase in height difference of approximately -0.338 units, though this effect was marginally significant (p = 0). Complementary to these findings, violin plots visually underscored the subtle variations in height differences across sex categories, suggesting broadly similar distributions but with nuanced variations in central tendency and spread. In conclusion, while sex exhibited a potential influence on height difference over the year, the observed effect was notably minor."
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html#overview",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html#overview",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "This example assesses whether growth in a subsample of ABCD participants from Baseline (T0) to the 1-Year follow-up (T1) differs significantly based on handedness, using height (“anthroheightcalc”) as a representative metric of growth. The analysis is conducted in two primary steps: 1) a difference score is calculated between baseline and Year_1 height measurements for each participant; 2) a simple regression analysis is used to test whether sex (boy, girl) predicts the average difference value in participants height from baseline to the 1-Year follow-up. Finally, a visual inspection is further conducted via a violin plot to graphically represent the relationship between difference scores and sex. The ensuing analysis and interpretations are detailed in the subsequent sections."
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html#preliminary-setup",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html#preliminary-setup",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "Install PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# rm(list = ls())\n\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\")\n# ,\"rstatix\"\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)"
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html#descriptives-overview",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html#descriptives-overview",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "Read and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title"
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html#results",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html#results",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "Build Model: Step 1Build Model: Step 2Model Plots\n\n\n\nThe code snippet below tells R to compute a difference score by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nSTEP 1: Compute Difference Score\n\n\nCode\n# Define the function to compute difference scores for a given variable and provide a summary\n# Function to compute difference scores for a given variable and provide a summary\ncompute_difference_and_summary &lt;- function(df, variable_name) {\n    # Define the event names of interest\n    baseline_event &lt;- \"baseline_year_1_arm_1\"\n    followup_event &lt;- \"1_year_follow_up_y_arm_1\"\n\n    # Compute the difference between Baseline and Year 1 data for the given variable\n    diff_data &lt;- df %&gt;%\n        filter(eventname %in% c(baseline_event, followup_event)) %&gt;% # Filter for specific event names\n        select(src_subject_id, eventname, all_of(variable_name)) %&gt;% # Select required columns\n        spread(eventname, variable_name) %&gt;% # Convert data from long to wide format\n        mutate(diff = get(followup_event) - get(baseline_event)) %&gt;% # Compute difference between the two time points\n        drop_na(diff) # Exclude rows with NA in the computed difference\n\n    # Summarize the computed difference scores\n    diff_summary &lt;- summary(diff_data$diff)\n\n    # Return the difference data and its summary\n    list(data = diff_data, summary = diff_summary)\n}\n\n# List of variables for which difference scores are to be computed\nvariables_of_interest &lt;- c(\"anthroheightcalc\")\n\n# Compute and store difference scores and summaries for each variable in a list\ndifference_and_summary_list &lt;- lapply(variables_of_interest, function(var) {\n    compute_difference_and_summary(df, var)\n})\n\n# Extract the difference data for the 'anthroheightcalc' variable\nheight_diff_data &lt;- difference_and_summary_list[[1]]$data\n\n# Merge the 'diff' column back to the main df using 'src_subject_id' as the key\ndf &lt;- left_join(df, height_diff_data %&gt;% select(src_subject_id, diff), by = \"src_subject_id\")\n\n\nDescriptive Statistics for the Difference Score\n\n\nCode\n# Compute statistical summaries for the difference score variable\nlapply(difference_and_summary_list, function(item) {\n    print(item$summary)\n})\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\n[[1]]\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\nThis summary of the difference score variable indicates that the differences range from a decrease of -51.875 units to an increase of 67 units. The median difference is 2.25 units, and the average difference is approximately 2.3660611 units. There are 0 missing values in this difference score dataset.\nDescriptive Statistics for the Height Variable\n\n\nCode\n## Summary statistics\n# Compute summary statistics for Height by eventname\nsummary &lt;- df %&gt;%\n    group_by(eventname) %&gt;%\n    get_summary_stats(anthroheightcalc, type = \"mean_sd\")\n\ndata.frame(summary)\n\n\n\n\n  \n\n\n\nThe summary statistics provide insights into the Height variable across different eventname categories. At baseline, the average height is approximately 55.241 units with a standard deviation of 3.331. Over the years, there’s a noticeable increase in average height: by the first year, it’s about 57.595 units, and it continues to rise, reaching approximately 64.694 units by the fourth year. The standard deviation remains relatively consistent over the years, suggesting similar variability in height measurements across these time points.\n\n\n\n\nA simple regression analyses is conducted to examine whether a grouping variable (sex) significantly predicts the difference score value (indicating significant group differences in the average difference score).\nSTEP 2: Conduct regression on Difference Score\n\n\nCode\n# Merge the 'sex' variable from the original dataframe 'df' to 'height_diff_data'\nmerged_data &lt;- height_diff_data %&gt;%\n  left_join(df %&gt;% select(src_subject_id, sex), by = \"src_subject_id\")\n\n# Ensure 'sex' is a factor\nmerged_data$sex &lt;- as.factor(merged_data$sex)\n\n# Run the regression\nmodel &lt;- lm(diff ~ sex, data = merged_data)\n\n# Get the summary of the regression model\nmodel_summary &lt;- summary(model)\n\nmodel_summary\n\n\n\nCall:\nlm(formula = diff ~ sex, data = merged_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-54.41  -0.59  -0.11   0.47  64.47 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.5308     0.0167   151.8   &lt;2e-16 ***\nsexMale or other  -0.3376     0.0230   -14.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.51 on 47586 degrees of freedom\nMultiple R-squared:  0.00449,   Adjusted R-squared:  0.00447 \nF-statistic:  215 on 1 and 47586 DF,  p-value: &lt;2e-16\n\n\nThis regression analysis evaluates whether sex (boy, girl) predicts the average difference in participants’ height from baseline to the 1-Year follow-up. The output from our model provides:\n\nan F-statistic of 214.6560227;\ndegrees of freedom of 47586;\na parameter estimate of -0.3376126;\nstandard error of 0.0230434;\np-value of 1.6900378^{-48}.\n\nCompared to boys (the reference group), girls have an average increase in height difference of approximately -0.338 units. This effect was marginally significant with a p-value of 0. Overall, the model explained a very small portion of the variance in height difference, with an adjusted R-squared value of 0.00447, and the overall model significance was not statistically significant with a p-value of 1.6900378^{-48}.\n\n\n\n\n\n\ntesting\n# Visualize the difference scores across different levels of sex\n\n# Create a violin plot to show the distribution of difference scores by sex\n# Jittered points are added to provide a more granular view of individual observations\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\n# Merge the 'sex' variable from the original dataframe 'df' to 'height_diff_data'\nmerged_data &lt;- height_diff_data %&gt;%\n  left_join(df %&gt;% select(src_subject_id, sex), by = \"src_subject_id\")\n\n# Plotting using ggplot2\nggplot(merged_data, aes(x = sex, y = diff, fill = sex)) +\n  geom_violin() +\n  geom_jitter(position = position_jitter(width = 0.2), size = 1, alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") + \n  labs(\n    title = \"Difference Scores by Sex\",\n    x = \"Sex\",\n    y = \"Difference Score\"\n  ) +\n  theme_minimal() + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nThe violin plot visualizes the distribution of difference scores in children’s heights by their sex: Right, Left, and Mixed. Each violin shape provides insight into the density of the data at different height differences, with wider sections representing higher densities of data points. Superimposed jittered points offer a granular view of individual observations. From the plot, it appears that the distributions of height differences across the three handedness categories are somewhat similar, though there might be subtle variations in median and spread."
  },
  {
    "objectID": "1b_Examples_DifferenceScores_SimpleRegression.html#wrapping-up",
    "href": "1b_Examples_DifferenceScores_SimpleRegression.html#wrapping-up",
    "title": "Difference Scores: Simple Regression",
    "section": "",
    "text": "Write-up\n\n\n\nDifferences in children’s heights between baseline and a subsequent 1-year follow-up, we sought to understand the potential influence of sex on this difference. Descriptive statistics revealed that height differences ranged from a decrease of -51.875 units to an increase of 67 units, with a median difference of 2.25 units and an average difference of approximately 2.3542189 units. Subsequent regression analysis indicated that, when compared to boys, girls had an average increase in height difference of approximately -0.338 units, though this effect was marginally significant (p = 0). Complementary to these findings, violin plots visually underscored the subtle variations in height differences across sex categories, suggesting broadly similar distributions but with nuanced variations in central tendency and spread. In conclusion, while sex exhibited a potential influence on height difference over the year, the observed effect was notably minor."
  },
  {
    "objectID": "3_Tutorials_LinearMixedModels.html",
    "href": "3_Tutorials_LinearMixedModels.html",
    "title": "Linear Mixed Models",
    "section": "",
    "text": "Linear mixed models (LMMs) are a powerful statistical tool that allows the analysis of complex data structures that contain both fixed and random effects. LMMs are widely used in various fields, including social sciences, biology, and engineering, due to their ability to handle hierarchical data structures and account for within-subject correlations. LMMs are an extension of the general linear model (GLM), where both fixed and random effects can be included in the model, making them more flexible and robust. These models are particularly useful when analyzing longitudinal data, where measurements are taken repeatedly over time, and correlations between observations must be accounted for. Specifically, the LMM framework accounts for these dependencies among data by extending the general regression “fixed effects” model to allow both, fixed and random effects. This approach simultaneously models an overall sample mean trajectory (fixed effect) and subject-specific (random) effects that vary randomly about the sample mean trajectory. It is this “mixture” of fixed and random effects from which these models derive their name.\n\n\n\nLinear Mixed Models\n\n\n\n\nYou should consider using LMMs in the following situations:\n\nYou want to know: The effects of interventions, time, or other predictors on a continuous outcome over multiple time points.\nYour data: Contains repeated measures taken on subjects at different time points or under various conditions.\nYou have: Both fixed effects (overall population effects) and random effects (individual variations) that need to be accounted for in the analysis.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal linear mixed models (LLMMs) and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of longitudinal linear mixed models.\nFit a longitudinal linear mixed model using example data in R.\nInterpret the results of the LLMM analysis."
  },
  {
    "objectID": "3_Tutorials_LinearMixedModels.html#overview",
    "href": "3_Tutorials_LinearMixedModels.html#overview",
    "title": "Linear Mixed Models",
    "section": "",
    "text": "Linear mixed models (LMMs) are a powerful statistical tool that allows the analysis of complex data structures that contain both fixed and random effects. LMMs are widely used in various fields, including social sciences, biology, and engineering, due to their ability to handle hierarchical data structures and account for within-subject correlations. LMMs are an extension of the general linear model (GLM), where both fixed and random effects can be included in the model, making them more flexible and robust. These models are particularly useful when analyzing longitudinal data, where measurements are taken repeatedly over time, and correlations between observations must be accounted for. Specifically, the LMM framework accounts for these dependencies among data by extending the general regression “fixed effects” model to allow both, fixed and random effects. This approach simultaneously models an overall sample mean trajectory (fixed effect) and subject-specific (random) effects that vary randomly about the sample mean trajectory. It is this “mixture” of fixed and random effects from which these models derive their name.\n\n\n\nLinear Mixed Models\n\n\n\n\nYou should consider using LMMs in the following situations:\n\nYou want to know: The effects of interventions, time, or other predictors on a continuous outcome over multiple time points.\nYour data: Contains repeated measures taken on subjects at different time points or under various conditions.\nYou have: Both fixed effects (overall population effects) and random effects (individual variations) that need to be accounted for in the analysis.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal linear mixed models (LLMMs) and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of longitudinal linear mixed models.\nFit a longitudinal linear mixed model using example data in R.\nInterpret the results of the LLMM analysis."
  },
  {
    "objectID": "3_Tutorials_LinearMixedModels.html#basic-example",
    "href": "3_Tutorials_LinearMixedModels.html#basic-example",
    "title": "Linear Mixed Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 2000 rows (4 rows for each of the 500 individuals).\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, 0.30, 0.30, \n                      0.30, 1, 0.30, 0.30, \n                      0.30, 0.30, 1, 0.30,\n                      0.30, 0.30, 0.30, 1),\n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data for 4 timepoints\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=4),\n  TimePoint = rep(c(\"T1\", \"T2\", \"T3\", \"T4\"), times=n),\n  Job_Satisfaction = as.vector(t(correlated_data))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Linear Mixed Model (LMM), we will use the lme4 package in R, specifically the lmer function. In LMMs, we account for both fixed effects, which are consistent across individuals, and random effects, which can vary. By doing so, LMMs allow us to model the trajectory of an outcome variable over time while considering the inter-individual differences and intra-individual changes.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lme4\" %in% installed.packages())) install.packages(\"lme4\")\nlibrary(lme4)\n\n\n\n\nModel\n\n\nCode\nmodel &lt;- lmer(Job_Satisfaction ~ TimePoint + (1 | Individual), data = data_large)\n\n\n\n\nModel Summary\n\n\nCode\nsummary(model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Job_Satisfaction ~ TimePoint + (1 | Individual)\n   Data: data_large\n\nREML criterion at convergence: 5490\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1846 -0.6437 -0.0167  0.6699  2.8774 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n Individual (Intercept) 0.2905   0.5390  \n Residual               0.7102   0.8427  \nNumber of obs: 2000, groups:  Individual, 500\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.02551    0.04474  -0.570\nTimePointT2  0.10879    0.05330   2.041\nTimePointT3  0.03065    0.05330   0.575\nTimePointT4  0.01649    0.05330   0.309\n\nCorrelation of Fixed Effects:\n            (Intr) TmPnT2 TmPnT3\nTimePointT2 -0.596              \nTimePointT3 -0.596  0.500       \nTimePointT4 -0.596  0.500  0.500\n\n\nCode\nplot(model)\n\n\n\n\n\n\n\n\nInterpreting the Results\nA Linear mixed model (LMM) was fitted to analyze the Job Satisfaction scores over different time points, accounting for random effects due to individual differences. By using the summary() function in R, we can obtain detailed results about the fixed and random effects. The TimePoint effect will indicate how Job Satisfaction varies across different time points. The model’s coefficient for TimePoint (r coef(model)[‘TimePoint’]) provides insights into the average change in Job Satisfaction for each unit increase in TimePoint. Its associated p-value (r summary(model)$coefficients[‘TimePoint’,‘Pr(&gt;|t|)’]) will indicate its significance. Additionally, using the plot() function, we can visually inspect the diagnostic plots, which can give insights into the residuals’ behavior, influential observations, and the model’s overall fit."
  },
  {
    "objectID": "3_Tutorials_LinearMixedModels.html#conclusion",
    "href": "3_Tutorials_LinearMixedModels.html#conclusion",
    "title": "Linear Mixed Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe LMM allows us to analyze the trend in Job Satisfaction scores across time points while considering individual variability. Depending on the p-value for the TimePoint effect, we can determine if there’s a statistically significant change in Job Satisfaction scores over the time points. Moreover, the diagnostic plots can guide potential model refinements or highlight areas for further investigation."
  },
  {
    "objectID": "1_Tutorials_DifferenceScores.html",
    "href": "1_Tutorials_DifferenceScores.html",
    "title": "Difference Scores",
    "section": "",
    "text": "Difference scores offer a straightforward statistical approach to compare data collected from the same individual across two measurement occasions. The difference between scores at two time points is taken as a measure of change. It is common to then perform statistical tests on the difference scores, such as being included as an outcome in a GLM analysis to test for differences in patterns of change over time and between groups. For example, difference scores may be used in a paired-samples t-test to compare mean test scores of students before and after attending a math workshop, or in a simple regression analysis to assess the effectiveness of a weight loss program by calculating the difference in weight between between groups of interest, before and after the program. This tutorial will guide you through computing and interpreting difference scores using R.\n\n\n\nFigure x. Difference score add explanation or tag\n\n\n\n\nYou should use difference scores in the following scenario:\n\nYou want to know: The impact of interventions or time on a measured variable.\nYour variable: Has paired measurements, like pre-test and post-test scores.\nYou have: Matched data for each set, ensuring valid difference computations.\n\n\n\n\nIn this tutorial, we will guide you through two simple examples of using difference scores. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of difference scores.\nCalculate difference scores using example data.\nInterpret the results of the difference scores analysis."
  },
  {
    "objectID": "1_Tutorials_DifferenceScores.html#overview",
    "href": "1_Tutorials_DifferenceScores.html#overview",
    "title": "Difference Scores",
    "section": "",
    "text": "Difference scores offer a straightforward statistical approach to compare data collected from the same individual across two measurement occasions. The difference between scores at two time points is taken as a measure of change. It is common to then perform statistical tests on the difference scores, such as being included as an outcome in a GLM analysis to test for differences in patterns of change over time and between groups. For example, difference scores may be used in a paired-samples t-test to compare mean test scores of students before and after attending a math workshop, or in a simple regression analysis to assess the effectiveness of a weight loss program by calculating the difference in weight between between groups of interest, before and after the program. This tutorial will guide you through computing and interpreting difference scores using R.\n\n\n\nFigure x. Difference score add explanation or tag\n\n\n\n\nYou should use difference scores in the following scenario:\n\nYou want to know: The impact of interventions or time on a measured variable.\nYour variable: Has paired measurements, like pre-test and post-test scores.\nYou have: Matched data for each set, ensuring valid difference computations.\n\n\n\n\nIn this tutorial, we will guide you through two simple examples of using difference scores. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of difference scores.\nCalculate difference scores using example data.\nInterpret the results of the difference scores analysis."
  },
  {
    "objectID": "1_Tutorials_DifferenceScores.html#basic-example",
    "href": "1_Tutorials_DifferenceScores.html#basic-example",
    "title": "Difference Scores",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at two time points: T1-T2.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 1000 rows (2 rows for each of the 500 individuals).\n\n# Creating the example dataset\nset.seed(123) # for reproducibility\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, \n                      0.30, 1), \n                      nrow=2)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data\nrandom_data &lt;- matrix(rnorm(n * 2), ncol=2)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=2),\n  TimePoint = rep(c(\"T1\", \"T2\"), times=n),\n  Job_Satisfaction = c(correlated_data[,1], correlated_data[,2])\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Difference Score, we will quantify the change in a variable by subtracting its initial value from subsequent measurements. This approach captures the absolute change in the variable over time.\n\nModel\n\n\nCode\n# Performing the paired t-test to compare Life Satisfaction scores at two time points\n# Extracting Job_Satisfaction scores for the two time points\nt1_scores_large &lt;- data_large$Job_Satisfaction[data_large$TimePoint == \"T1\"]\nt2_scores_large &lt;- data_large$Job_Satisfaction[data_large$TimePoint == \"T2\"]\n\n\n\n\nFit Model\n\n\nCode\ntest_result &lt;- t.test(t1_scores_large, t2_scores_large, mu = 0, paired = TRUE)\n\n\n\n\nModel Summary\n\n\nCode\nprint(test_result)\n\n\n\n    Paired t-test\n\ndata:  t1_scores_large and t2_scores_large\nt = 2.2206, df = 499, p-value = 0.02682\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.01659222 0.27137979\nsample estimates:\nmean difference \n       0.143986 \n\n\n\n\n\nInterpreting the Results\nThe paired t-test was conducted to compare Job Satisfaction scores at two time points: T1 and T2. The test yielded a t-value of t=2.221 with 499 degrees of freedom. The associated p-value is p=0.027. This p-value is greater than the common significance level of 0.05, which means that we do not have enough evidence to reject the null hypothesis. The 95% confidence interval for the mean difference ranges from 0.017 to 0.271. The observed mean difference between the two time points is 0.144."
  },
  {
    "objectID": "1_Tutorials_DifferenceScores.html#conclusion",
    "href": "1_Tutorials_DifferenceScores.html#conclusion",
    "title": "Difference Scores",
    "section": "Conclusion",
    "text": "Conclusion\nThere is no statistically significant difference in Job Satisfaction scores between the two time points, T1 and T2, based on the paired t-test results. The mean difference, although positive (0.144), is not significant at the 0.05 level."
  },
  {
    "objectID": "manuscript.html",
    "href": "manuscript.html",
    "title": "Longitudinal Analysis Manuscript: Working Draft",
    "section": "",
    "text": "The Adolescent Brain Cognitive Development (ABCD) Study® is the largest long-term investigation of neurodevelopment and child health in the United States. Conceived and initiated by the National Institutes of Health (NIH), this landmark prospective longitudinal study aims to transform our understanding of the genetic and environmental factors impacting neurodevelopment and their roles in behavioral and health outcomes across ten years of adolescence (Volkow et al. 2018). At its heart, the study is designed to chart the course of human development across multiple interacting domains from late childhood to early adulthood and to identify factors that lead to both positive and negative outcomes. Central to achieving these goals is the commitment of the ABCD Study® and its NIH funders to an open science framework, intended to facilitate sharing of data and analytical methods, by espousing practices that increase access, integrity, and reproducibility of scientific research. In this sense, the ABCD Study is a collaboration with the larger research community.\nThe size and scope of the ABCD Study data allow the research community to perform a large variety of developmental analyses of both substantive and methodological interest, presenting a unique opportunity to significantly advance our understanding of how a multitude of biopsychosocial processes unfold across critical periods of development. In this paper, we describe models and methods for longitudinal analysis of ABCD Study data that can address these fundamental scientific aims, including: 1) characterization of the genetic and environmental factors associated with variation in developmental trajectories; 2) assessment of how the level and timing of exposures may impact subsequent neurodevelopment; 3) quantification of how variation in developmental domains may be associated with outcomes, including mediation models and reciprocal relationships. We instantiate these longitudinal analyses in worked examples using the ABCD Release 5.0 data with accompanying R scripts. Worked examples are available in Quarto files, accessible at XXXX.\n\n\nThe ABCD Study enrolled a cohort of n=11,880 participants born between 2006-2008 and aged between 9-11 years at baseline, as well as their parents/guardians. The study sample was recruited from household populations in defined catchment areas for each of the 21 (originally 22) study sites across the United States. Information regarding funding agencies, recruitment sites, investigators, and project organization can be obtained at https://abcdstudy.org/. The ABCD Study design is described in more detail in Garavan et al. (2018) and Dick et al. (2021).\nThe ABCD Study is currently collecting longitudinal data on a rich variety of outcomes that will enable the construction of complex statistical models potentially incorporating factors from many domains. Each new wave of data collection provides another building block for characterizing developmental trajectories and implementing longitudinal analyses that allow researchers to characterize normative development, to identify variables that presage deviations from normative development, and to assess a range of variables associated with biopsychosocial outcomes of interest. These data include: 1) a neurocognitive battery (Luciana et al. 2018; Wesley K. Thompson et al. 2019); 2) mental and physical health assessments (Barch et al. 2018); 3) measures of culture and environment (Gonzalez et al. 2021; Zucker et al. 2018); 4) substance use (Lisdahl et al. 2021); 5) gender identity and sexual health (Potter et al. 2022); 6) biospecimens (Uban et al. 2018); 7) structural and functional brain imaging (Casey et al. 2018; Hagler et al. 2019); 8) geolocation-based environmental exposure data (Fan et al. 2021); 9) wearables, and mobile technology (Bagot et al. 2018); and 10) whole genome genotyping (Loughnan et al. 2020). Many of these measures are collected at in-person annual visits, with brain imaging collected at baseline and every other year going forward. A limited number of assessments are collected in semi-annual telephone interviews between in-person visits.\nData are publicly released roughly annually, currently through the NIMH Data Archive (NDA). By necessity, the study’s earliest data releases consisted primarily of one or two visits per participant. However, the most recent public release as of the writing of this paper (Release 5.0) contains data collected across five annual visits, including three brain imaging assessments (baseline, year 2 follow-up, and year 4 follow-up visits) for at least a subset of the cohort. Hence, starting with Release 5.0, it is feasible for researchers to begin focusing on the estimation of neurodevelopmental and other trajectories.\n\n\n\n\n\n\n\n\n\nOrganization and Aims\n\n\n\n\n\n• Part I. Longitudinal Research\n\nIdentify fundamental concepts\n\n• Part II. Longitudinal Data\n\nHighlight key challenges\n\n• Part III. Longitudinal Analysis\n\nMethods & Analysis\n\n• Part IV. Supplemental materials\n\nLinked open-source resources"
  },
  {
    "objectID": "manuscript.html#the-abcd-study-data",
    "href": "manuscript.html#the-abcd-study-data",
    "title": "Longitudinal Analysis Manuscript: Working Draft",
    "section": "",
    "text": "The ABCD Study enrolled a cohort of n=11,880 participants born between 2006-2008 and aged between 9-11 years at baseline, as well as their parents/guardians. The study sample was recruited from household populations in defined catchment areas for each of the 21 (originally 22) study sites across the United States. Information regarding funding agencies, recruitment sites, investigators, and project organization can be obtained at https://abcdstudy.org/. The ABCD Study design is described in more detail in Garavan et al. (2018) and Dick et al. (2021).\nThe ABCD Study is currently collecting longitudinal data on a rich variety of outcomes that will enable the construction of complex statistical models potentially incorporating factors from many domains. Each new wave of data collection provides another building block for characterizing developmental trajectories and implementing longitudinal analyses that allow researchers to characterize normative development, to identify variables that presage deviations from normative development, and to assess a range of variables associated with biopsychosocial outcomes of interest. These data include: 1) a neurocognitive battery (Luciana et al. 2018; Wesley K. Thompson et al. 2019); 2) mental and physical health assessments (Barch et al. 2018); 3) measures of culture and environment (Gonzalez et al. 2021; Zucker et al. 2018); 4) substance use (Lisdahl et al. 2021); 5) gender identity and sexual health (Potter et al. 2022); 6) biospecimens (Uban et al. 2018); 7) structural and functional brain imaging (Casey et al. 2018; Hagler et al. 2019); 8) geolocation-based environmental exposure data (Fan et al. 2021); 9) wearables, and mobile technology (Bagot et al. 2018); and 10) whole genome genotyping (Loughnan et al. 2020). Many of these measures are collected at in-person annual visits, with brain imaging collected at baseline and every other year going forward. A limited number of assessments are collected in semi-annual telephone interviews between in-person visits.\nData are publicly released roughly annually, currently through the NIMH Data Archive (NDA). By necessity, the study’s earliest data releases consisted primarily of one or two visits per participant. However, the most recent public release as of the writing of this paper (Release 5.0) contains data collected across five annual visits, including three brain imaging assessments (baseline, year 2 follow-up, and year 4 follow-up visits) for at least a subset of the cohort. Hence, starting with Release 5.0, it is feasible for researchers to begin focusing on the estimation of neurodevelopmental and other trajectories."
  },
  {
    "objectID": "manuscript.html#section",
    "href": "manuscript.html#section",
    "title": "Longitudinal Analysis Manuscript: Working Draft",
    "section": "",
    "text": "Organization and Aims\n\n\n\n\n\n• Part I. Longitudinal Research\n\nIdentify fundamental concepts\n\n• Part II. Longitudinal Data\n\nHighlight key challenges\n\n• Part III. Longitudinal Analysis\n\nMethods & Analysis\n\n• Part IV. Supplemental materials\n\nLinked open-source resources"
  },
  {
    "objectID": "manuscript.html#basic-concepts-and-considerations",
    "href": "manuscript.html#basic-concepts-and-considerations",
    "title": "Longitudinal Analysis Manuscript: Working Draft",
    "section": "2.1 Basic Concepts and Considerations",
    "text": "2.1 Basic Concepts and Considerations\nThere are several important concepts to consider when conducting longitudinal analyses in a developmental context. These include different ways of thinking about developmental course, whether certain periods of development are relatively sensitive or insensitive to various types of insults or stressors, whether some time periods or situations inhibit the expression of individual differences due to extreme environmental pressures, and whether the same behavior manifested at different times represents the same or different phenomena.\nMoreover, in the case of developmentally-focused longitudinal research, each new measurement occasion not only provides a more extended portrait of the child’s life course but also brings with it greater methodological opportunities to make use of statistical models that distinguish within- from between-person effects and that loosen constraints that need to be imposed on the furtherance of critical scientific questions.\nFor example, collecting two or more within-person observations on the same construct at different times enables estimation of individual rates of change (slopes); more observations allow for more precise estimates of individual slopes, as well as characterization of non-linear development. Rate of change or other trajectory characteristics may be more informative about individuals than the simple snapshots of level differences that cross-sectional data are limited to informing. Cross-sectional age-related differences across individuals are poor substitutes for longitudinal trajectory estimates except under highly restrictive assumptions, e.g., parallel trajectories and lack of age cohort and experience effects (Wesley K. Thompson et al. 2011). Appreciation of these and other issues can help to guide the analysis and interpretation of data and aid translation to clinical and public health applications.\n\n2.1.1 Vulnerable periods.\nAdolescent development progresses normatively from less mature to more mature levels of functioning. However, unique epochs and experiences can alter the course of this idealized form of development. Consider research that shows cannabis use during adolescence is associated with later psychosis to a greater degree than cannabis use initiated later in development (Arseneault et al. 2002; Bechtold et al. 2016; Hasan et al. 2020; Semple, McIntosh, and Lawrie 2005); or similarly, experimental research on rodents that shows rodent brains to be especially sensitive to the neurotoxic effects of alcohol on brain structure and learning early in development, corresponding to early adolescence in humans (Spear 2016; Crews et al. 2000; Ji et al. 2018). In another example, longitudinal data from the National Consortium on Alcohol in Adolescence (NCANDA) show that binge drinking is associated more strongly with decrements in gray matter volume early in adolescence compared to later (Infante et al. 2022). These examples highlight the importance of considering the role of vulnerable periods – e.g., temporal windows of rapid brain development or remodeling during which the effects of environmental stimuli on the developing brain may be particularly pronounced– when trying to establish an accurate understanding of the association between exposures and outcomes.\n\n\n2.1.2 Developmental disturbances.\nWhereas vulnerable periods heighten neurobiological susceptibility to environmental influences, at other times, environmental exposures will tend to suppress stability and disrupt the orderly stochastic process of normative development (e.g., Schulenberg et al. 2019). This situation reflects a developmental disturbance in that the normal course of development is “altered” for a time by some time-limited process. In such cases, we might find that prediction of behavior in the period of the disturbance is reduced and/or, similarly, the behavior exhibited during the disturbance might have less predictive power with respect to distal outcomes compared to the behavior exhibited before and following the disrupted period. That is, once the environmental presses are removed (or the individual is removed from the environment), patterns of individual differences (and autoregressive effects) recover to levels similar to those prior to entering the environment. For example, in Infante et al. (2022), recent binge drinking appears to be most predictive of gray matter volume trajectories, as opposed to more distal prior binge drinking or cumulative number of binge drinks, suggesting the potential for recovery of gray matter trajectories to prior levels of growth if binge drinking levels subside.\n\n\n2.1.3 Developmental snares and cascade effects.\nNormative development can also be upended by experiences (e.g., drug use) that, through various mechanisms, disrupt the normal flow of development wherein each stage establishes a platform for the next. For instance, substance use could lead to association with deviant peers, precluding opportunities for learning various adaptive skills and prosocial behaviors, in effect creating a “snare” that delays psychosocial development, such as maturing out of adolescent antisocial behavior (Moffitt 2015). Relatedly, the consequences of these types of events can cascade (e.g., school dropout, involvement in the criminal justice system) so that the effects of the snare are amplified (e.g., Masten et al. 2005; Rogosch, Oshri, and Cicchetti 2010). Although conceptually distinct from vulnerable periods, both types of developmental considerations highlight the importance of viewing behavior in the context of development and attempting to determine how various developmental pathways unfold. Longitudinal data are crucial in this context to assess individual levels of development prior to and following onset of experiences or other environmental factors; e.g., the ABCD Study collected data starting at ages 9-10 and hence before the onset of substance use for the vast majority of participants.\n2.1.4 Intermediate processes and feedback loops Something here on the concept of mediation and cross-lagged relationships?"
  },
  {
    "objectID": "manuscript.html#interpretation-issues-pitfalls-assumption",
    "href": "manuscript.html#interpretation-issues-pitfalls-assumption",
    "title": "Longitudinal Analysis Manuscript: Working Draft",
    "section": "3.1 Interpretation / Issues / Pitfalls & Assumption",
    "text": "3.1 Interpretation / Issues / Pitfalls & Assumption\nThe hallmark characteristic of longitudinal data analysis is the administration of repeated measurements of the same constructs on assessment targets (e.g., individuals, families) across time. The primary rationale for collecting longitudinal data is to assess within-person change over time, allowing researchers to estimate individual developmental trajectories and the genetic and/or environmental factors that may impact these trajectories. Administering repeated measurements more frequently or over longer time spans enables researchers to ask more nuanced questions and to make stronger inferences.\n\n3.1.1 Two Time Points versus Three or More.\nAlthough the clear leap from cross-sectional to the realm of longitudinal data involves going from one assessment to two or more assessments, there are also notable distinctions in designs based on two-assessment points versus three or more measurement occasions. Just as cross-sectional data can be informative in some situations, two waves of data can be beneficial in contexts such as when experimental manipulation is involved (e.g., pre/post tests), or if the central goal is prediction (e.g., trying to predict scores on Variable A at time T as a function of prior scores on Variable A and Variable B at time T-1). At the same time, data based on two assessments are inherently limited on multiple fronts. As Rogosa, Brandt, and Zimowski (1982) noted over forty years ago, “Two waves of data are better than one, but maybe not much better” (p. 744).\nThese sentiments are reflected in more contemporary recommendations regarding best-practice guidelines for prospective data, which increasingly emphasize the benefits of additional measurement occasions for trajectory estimation, model identification and accurate parameter inferences. It is also consistent with research recommending that developmental studies include three or more assessment points, given it is impossible for data based on two-time points to determine the shape of development (given that linear change is the only estimable form for two assessments waves; (see Duncan and Duncan 2009)). Research designs that include three (but preferably more) time points allow for non-linear trajectory estimation and increasingly nuanced analyses that more adequately tease apart sources of variation and covariation among the repeated assessments (King et al. 2018)– a key aspect of developmental research.\nTo illustrate, developmental theories are useful for understanding patterns of within-individual change over time (discussed in further detail, below); however, two data points provide meager information on change at the person level. This point is further underscored in a recent review of statistical models commonly touted as distinguishing within-individual vs between-individual sources of variance in which the study authors concluded “… researchers are limited when attempting to differentiate these sources of variation in psychological phenomenon when using two waves of data” and perhaps more concerning, “…the models discussed here do not offer a feasible way to overcome these inherent limitations” (Andrew K. Littlefield et al. 2021). It is important to note, however, that despite the current focus on two-wave designs versus three or more assessment waves, garnering three assessment points is not a panacea for longitudinal modeling. Indeed, several contemporary longitudinal models designed to isolate within-individual variability (e.g., the Latent Curve Model with Structured Residuals; Patrick J. Curran et al. (2014)) require at least four assessments to parameterize fully and, more generally, increasingly accurate and nuanced parameter estimates are obtained as more assessment occasions are used (Duncan and Duncan 2009).\n\n\n3.1.2 Types of stability and change\nIf one were to try to sum up what developmental trajectories in a living organism are exactly, one could plausibly argue they are the patterns of stability and change in its phenotypes as the organism traverses the life course. Symbolically, developmental trajectories can be expressed as fi(t), a possibly multivariate function of time t, specific to the ith individual and typically taking values in the real numbers for continuous phenotypes and the integers for discrete phenotypes. Ideally, t is a biologically meaningful temporal index (e.g., calendar age) as opposed to an exogenous progression of events (e.g., study visit number). Properties of interest might include rate of change over time, degree of smoothness (e.g., continuously differentiable), shape (e.g., polynomial or asymptotic behavior), how and how much fi(t) differs across individuals, and what factors predict either within-individual variation (at different times) or between-individual variation (either overall or at specific times).\nThere are a few different ways to think about patterns of stability and change (see Figure 1). Consider measuring school disengagement at the start of middle school and the end of middle school . A common first step may be to compare sixth graders’ average disengagement values and eighth graders’ disengagement values. This comparison of the average scores for the same group of individuals at multiple time points is referred to as “mean-level”, as it provides information about change over time (or lack thereof) for an outcome of interest aggregated across members of a group. In contrast, “between-individual” stability could be assessed, e.g., by calculating the Spearman correlation between the values obtained at different time points (e.g., ‘disengagement in sixth grade’ with ’disengagement in eighth grade). This analysis focuses on the degree to which individuals retain their relative placement in a group across time. Consider someone who reported the lowest frequencies of disengagement in 6th grade and may report significantly higher disengagement over middle school (i.e., exhibit high levels of change), but report the lowest frequencies of disengagement in eighth grade. That is, the individual is manifesting rank-order stability, even in the context of high mean-level change.\nBoth types of stability and change are important. Mean-level change in certain traits might help to explain why, in general, populations of individuals tend to be particularly vulnerable to the effects of environmental factors in specific age ranges; rank-order stability might help to quantify the extent to which certain characteristics of the individual are more or less trait-like compared to others. For example, in some areas of development, considerable mean-level change occurs over time (e.g., changes in Big 5 personality traits; Bleidorn et al. (2022)), but exhibit relatively high rank-order stability, at least over shorter measurement intervals (Bleidorn et al. 2022; Roberts and DelVecchio 2000; Roberts, Walton, and Viechtbauer 2006). Despite the useful information afforded by examining mean-level and rank-order stability and change, these approaches are limited in that they provide little information about the overall patterns of within-individual change (i.e., trajectories) and, in turn, can result in fundamental misinterpretations about substantial or meaningful changes in an outcome of interest (Patrick J. Curran and Bauer 2011). For example, questions related to the impact of early-onset substance use on brain development focus on changes within a given individual (i.e., intraindividual differences). The longitudinal nature of the ABCD Study, which will provide researchers with over ten time points for some constructs (e.g., substance use) across a ten-year period, allows for the study of within-person processes.\n\n\n\nFigure 1: Types of Stability and Change\n\n\n\n\n3.1.3 Use of appropriate longitudinal models\nThere is growing recognition that statistical models commonly applied to longitudinal data often fail to align with the developmental theory they are being used to assess (e.g., Patrick J. Curran and Bauer 2011; Hoffman 2015; Andrew K. Littlefield et al. 2021).\nFirst, developmental studies typically involve the use of prospective data to inform theories that are concerned with clear within-person processes (e.g., how phenotypes change or remain stable within individuals over time, (e.g., Patrick J. Curran and Bauer 2011)). Despite this, methods generally unsuited for disaggregating between- and within-person effects (e.g., cross-lagged panel models [CLPM]) remain common within various extant literatures. Fortunately, there exists a range of models that have been proposed to tease apart between- and within-person sources of variance across time (see Andrew K. Littlefield et al. 2021; Orth et al. 2021). Most of these contemporary alternatives incorporate time-specific latent variables to capture between-person sources of variance and model within-person deviations around an individual’s mean (or trait) level across time (e.g., random-intercept cross-lagged panel model [RI-CLPM], Hamaker, Kuiper, and Grasman (2015); latent curve models with structured residuals [LCM-SR], Patrick J. Curran et al. (2014)). It is important to note however that these models require multiple assessments waves (e.g., four or more to fully specify the LCM-SR), additional expertise to overcome issues with model convergence, and appreciation of modeling assumptions when attempting to adjudicate among potential models in each research context (see Andrew K. Littlefield et al. 2021, for further discussion).\nSecond, many statistical models assume certain characteristics about the data to which they are being applied. Common assumptions of parametric statistical models (e.g., linear mixed-effects models) include normality and equality of variances. These assumptions must be carefully considered before finalizing analytical approaches, so that valid inferences can be made from the data, as violation of a model’s assumptions can substantively invalidate the interpretation of results. For example, longitudinal data can exhibit heterogeneous variability (i.e., the variance of the response changes over the duration of the study) that may need to be accounted for within a model. Another pertinent modeling assumption is whether trajectories are linear or non-linear. With 2-3 assessments per individual, only a linear model of within-person change is usually feasible; with more time points, higher level polynomials or models with more flexible trajectory shapes (e.g., curve smoothing via cubic splines) becomes possible [REFs]. Note, however, baseline age in the ABCD Study ranges over two full years; for some outcomes it may be desirable to include a (possibly nonlinear) effect of baseline age along with a linear effect of within-person change in age with only 2-3 assessment times (Wesley K. Thompson et al. 2013). As the study progresses and more times points are assessed, nonparametric curve estimation at the mean [GAMMs] and at the individual level may also become useful (Ramsay and Silverman 2002).\n\n\n3.1.4 Continuous and Discrete Outcomes\nImplementing valid and efficient statistical models requires an understanding of the type of data being analyzed. For example, repeated assessments within the ABCD Study can be based on continuous or discrete measures. Examples of discrete measures include repeated assessments of binary variables (e.g., past 12-month alcohol use disorder status measured across ten years), ordinal variables (e.g., caregiver-reported items measuring emotional and behavioral concerns via the Child Behavior Checklist including the categories of “Not True”, “Somewhat True”, and “Very True”), and count variables (e.g., number of cigarettes smoked per day). In many ways, the distributional assumptions of indicators used in longitudinal designs mirror the decision points and considerations when delineating across different types of discrete outcome variables, a topic that spans entire textbooks (e.g., see Lenz 2016). For example, the Mplus manual (Muthén 2017) includes examples of a) censored and censored-inflated models, b) linear growth models for binary or ordinal variables, c) linear growth models for a count outcome assuming a Poisson model, and d) linear growth models for a count outcome assuming a zero-inflated Poisson model. Beyond these highlighted examples, other distributions (e.g., negative binomial) can be assumed for the indicators when modeling longitudinal data (Ren et al. 2022). These models can account for issues that can occur when working with discrete outcomes, including overdispersion [when the variance is higher than would be expected based on a given distribution; see Lenz (2016)]. Given the sheer breadth of issues relevant to determining better models for discrete outcomes, it is not uncommon for texts on longitudinal data analysis to only cover models and approaches that assume continuous variables (e.g., T. D. Little 2013). However, some textbooks on categorical data analysis provide more detailed coverage of the myriad issues and modeling choices to consider when working with discrete outcomes [e.g., Lenz (2016), Chapter 11 for matched pair/two-assessment designs; Chapter 12 for marginal and transitional models for repeated designs, such as generalized estimating equations, and Chapter 13 for random effects models for discrete outcomes].\n\n\n3.1.5 Issues in attributing longitudinal change to development\nOne can observe systematic changes over time in a variable of interest and assume this change is attributable to development. However, various pitfalls with longitudinal data can complicate or even invalidate this conclusion. For example, if data missingness or participant dropout are related to the values of the outcome, changing sample composition as the study progresses can bias mean trajectory estimates (we describe this in more detail in Section 3.1.7 below). Another prerequisite for valid developmental interpretations of longitudinal data is to establish whether a construct is measured consistently over time, i.e., longitudinal measurement invariance (Liu et al. 2017; Van De Schoot et al. 2015; Willoughby, Wirth, and Blair 2012). Establishing longitudinal measurement invariance provides researchers with greater confidence that any change over time identified for a construct is attributable to individual change rather than a measurement artifact. For example, one study using data from the ABCD Study [Brislin et al. (2023)) found differential item functioning in two items from a brief delinquency measure, revealing significant bias in an arrest item across Black and White youth. More specifically, Black youth were more likely to report being arrested compared to White youth with similar levels of delinquency. Prevalence rates of delinquent behavior would have been severely biased if measurement invariance had not been tested.\nObserved patterns of growth and decline often differ between cross-sectional vs. longitudinal effects (Salthouse 2014) where subjects gain increasing experience with the assessment with each successive measurement occasion. Such experience effects on cognitive functioning have been demonstrated in adolescent longitudinal samples similar to ABCD (Sullivan et al. 2017) and highlight the need to consider these effects and address them analytically. In the case of performance-based measures [e.g., matrix reasoning related to neurocognitive functioning; see Salthouse (2014)], this can be due to “learning” the task from previous test administrations (e.g., someone taking the test a second time performs better than they did the first time simply as a function of having taken it before). Even in the case of non-performance-based measures (e.g., levels of depression), where one cannot easily make the argument that one has acquired some task-specific skill through learning, it has been observed that respondents tend to endorse lower levels on subsequent assessments (e.g., Beck et al. 1961; French and Sutton 2010) and this phenomenon has been well documented in research using structured diagnostic interviews (Robins 1985). While it is typically assumed that individuals are rescinding or telling us less information on follow-up interviews, there is reason to suspect that in some cases the initial assessment may be artefactually elevated (see Shrout et al. 2018).\nSome longitudinal studies, e.g., accelerated longitudinal designs [ALDs; Wesley K. Thompson et al. (2011)] are especially well suited for discovering these effects and modeling them. While ABCD is not an ALD, the variability in age (and grade in school) at the time of baseline recruitment (9 years, 0 months to 10 years, 11 months) allows some measures, collected every year, to be conceptualized as an ALD (e.g., substance use; prosocial behavior; family conflict; screen time). It is possible that in later waves, analyses will allow for disaggregating the confounded effects of age and the number of prior assessments. However, ABCD is fundamentally a single-cohort, longitudinal design, wherein number of prior assessments and age are mostly confounded, and for, perhaps, most analyses, the possible influence of experience effects needs to be kept in mind.\n\n\n3.1.6 Covariance Structures\nA central issue for repeated measurements on an individual is how to account for the correlated nature of the data. Traditional techniques, such as a standard regression or ANOVA model, assume residuals are independent and thus are inappropriate for designs that assess the same individuals across time. That is, given that residuals are no longer independent, the standard errors from the models are biased and can produce misleading inferential results. Although there are formal tests of independence for time series data (e.g., the Durbin-Watson statistic (Durbin and Watson 1950), more commonly, independence is assumed to be violated in study designs with repeated assessments. Therefore, an initial question to be addressed by a researcher analyzing prospective data is how to best model the covariance structure of said data.\nStatistical models for longitudinal data include two main components to account for assumptions that are commonly violated when working with repeated measures data: a model for the covariance among repeated measures (involving the covariance among pairs of repeated outcomes on an individual), coupled with a model for the mean response and its dependence on covariates (e.g., sex at birth). There exists a range of methods to model covariance structures, each with its own set of tradeoffs between model fit and parsimony and which may be more or less appropriate for each specific application (e.g., see Kincaid 2005).\nOne alternative structure that attempts to handle the reality that correlations between repeated assessments tend to diminish across time is the autoregressive design. As the name implies, the structure assumes a subsequent measurement occasion (e.g., assessment at Wave 2) is regressed onto (that is, is predicted by) a prior measurement occasion (e.g., assessment at Wave 1). The most common type of autoregressive design is the AR(1), where assessments at time T + 1 are regressed on assessments at Time T. Identical to compound symmetry, this model assumes the variances are homogenous across time. Diverting from compound symmetry, this model assumes the correlations between repeated assessments decline exponentially across measurement occasions rather than remaining constant. That is, we can think of the underlying process as a stochastic one that wears itself out over time. For example, per the AR(1) structure, if the correlation between Time 1 and Time 2 data is thought to be .5, then the correlation between Time 1 and Time 3 data would be assumed to be .5 × .5 = .25, and the correlation between Time 1 and Time 4 data would be assumed to be .5 × .5 × .5 = .125. As with compound symmetry, the basic AR(1) model is parsimonious in that it only requires two parameters (the variance of the assessments and the autoregressive coefficient). Notably, the assumption of constant autoregressive relations between assessments is often relaxed in commonly employed designs that use autoregressive modeling (e.g., cross-lagged panel models [CLPM]). These designs still typically assume an AR(1) process (e.g., it is sufficient to regress the Time 3 assessment onto the Time 2 assessment and is not necessary to also regress the Time 3 assessment onto the Time 1 assessment, which would result in an AR(2) process). However, the magnitude of these relations is often allowed to differ across different AR(1) pairs of assessment (e.g., the relation between Time 1 and Time 2 can be different from the relation between Time 2 and Time 3). These more commonly employed models also often relax the assumption of equal variances of the repeated assessments. Although the AR(1) structure may involve a more realistic set of assumptions compared to compound symmetry, in that the AR(1) model allows for diminishing correlations across time, the basic AR(1) model, as well as autoregressive models more generally, can also suffer from several limitations in contexts that are common in prospective designs. In particular, recent work demonstrates that if a construct being assessed prospectively across time is trait-like in nature, then a simple AR(1) process fail to adequately account for this trait-like structure, with the downstream consequence that estimates derived from models based on AR structures (such as the CLPM) can be misleading and fail to adequately demarcate between- vs. within-person sources of variance (Hamaker, Kuiper, and Grasman 2015). Finally, it is also worth noting that in many longitudinal contexts, the time intervals between assessments are not equidistant and researchers need to consider carefully how to appropriately model time in their model and what this model implies.\n\n\n3.1.7 Missing Data/Attrition\nAttrition from a longitudinal panel study such as ABCD is inevitable and represents a potential threat to the validity of longitudinal analyses and cross-sectional analyses conducted at later time points, especially since attrition can only be expected to grow over time(Andrew K. Littlefield et al. 2022). The ABCD Retention Workgroup (RW) employs a data-driven approach to examine, track, and intervene in these issues and while preliminary findings show participant race and parent education level to be associated with late and missing visits, although to date, attrition in ABCD has been minimal (Ewing et al. 2022). Ideally, one tries to minimize attrition through good retention practices from the outset via strategies designed to maintain engagement in the project (Cotter et al. 2005; Hill et al. 2016; Watson et al. 2018). However, even the best-executed studies need to anticipate growing attrition over the length of the study and implement analytic strategies designed to provide the most valid inferences. Perhaps the most key concern when dealing with data that is missing due to attrition is determining the degree of bias in retained variables that is a consequence of attrition. Such bias can attenuate generalizability, particularly if the pattern of missingness is not random (e.g., certain subsets of the population are more likely to drop out/not attend a visit). Assuming that the data are not missing completely at random, attention to the nature of the missingness and employing techniques designed to mitigate attrition-related biases need to be considered in all longitudinal analyses.\nThree types of missingness are considered in the literature (see R. J. Little and Rubin 1989), namely: a) missing completely at random (MCAR), b) missing at random (MAR), and c) missing not at random (MNAR). Data that are MCAR are a simple random sample of all data in a given dataset. MAR implies missing data are a random sample (i.e., does not hinge on some unmeasured variables) within strata of the measured covariates in a dataset (e.g., biological sex). Data that are MNAR are missing as a function of unobserved variables and may bias assocations even after conditioning on the observed covariates. Graham (2009) provides an excellent and easy-to-digest overview of further details involving missing data considerations.\nModern approaches for handling missing data, such as full information maximum likelihood, propensity weighting, auxiliary variables and multiple imputation avoid the biases of older approaches (see Enders 2010; Graham 2009). Graham (2009) noted several “myths” regarding missing data. For example, Graham notes many assume the data must be minimally MAR to permit estimating procedures (such as maximum likelihood or multiple imputation) compared to other, more traditional approaches (e.g., using only complete case data). Violations of MAR impact both traditional and more modern data estimation procedures, though as noted by Graham, violations of MAR tend to have a greater effect on older methods. Graham thus suggests that imputing missing data is a better approach compared to listwise deletion in most circumstances, regardless of the model of missingness [i.e., MCAR, MAR, MNAR; see Graham (2009)].\n\n\n3.1.8 Quantifying effect sizes longitudinally\nGiven that longitudinal data involve multiple sources of variance, quantifying effect sizes longitudinally is a more difficult task compared to deriving such estimates from cross-sectional data. An effect size can be defined as, “a population parameter (estimated in a sample) encapsulating the practical or clinical importance of a phenomenon under study.” (Kraemer 2014). Common effect size metrics include the correlation r between two variables and the standardized difference between two means, Cohen’s d (Cohen 1988). An extensive discussion of effect sizes and their relevance for ABCD is given in Dick et al. (2021). Adjustments to common effect size calculations, such as Cohen’s d, are required even when only two time points are considered (e.g., Morris and DeShon (2002)]. Wang et al. (2019) note there are multiple approaches to obtaining standardized within-person effects, and that commonly suggested approaches (e.g., global standardization) can be problematic (see Wang et al. 2019, for more details). Thus, obtaining effect size metrics based on standardized estimates that are relatively simple in cross-sectional data (such as r) becomes more complex in the context of prospective longitudinal data. Feingold (2009) noted that equations for effects sizes used in studies involving growth modeling analysis (e.g., latent growth curve modeling) were not mathematically equivalent, and the effect sizes were not in the same metric as effect sizes from traditional analysis (see Feingold 2009, for more details). Given this issue, there have been various proposals for adjusting effect size measures in repeated assessments. Feingold (2019) reviews the approach for effect size metrics for analyses based on growth modeling, including when considering linear and non-linear (i.e., quadratic) growth factors. Morris and DeShon (2002) review various equations for effect size calculations relevant to combining estimates in meta-analysis with repeated measures and independent-groups designs. Other approaches to quantifying effect sizes longitudinally may be based on standardized estimates from models that more optimally disentangle between- and within-person sources of variance. As an example, within a random-intercept cross-lagged panel model (RI-CLPM) framework, standardized estimates between random intercepts (i.e., the correlation between two random intercepts for two different constructs assessed repeatedly) could be used to index the between-person relation, whereas standardized estimates among the structured residuals could be used as informing the effect sizes of within-person relations.\n\n\n3.1.9 Longitudinal Data Structures\nAn ideal longitudinal study integrates (a) a well-articulated theoretical model, (b) an appropriate longitudinal data structure, and (c) a statistical model that is an operationalization of the theoretical model (Collins 2006). To accommodate various research questions and contexts, different types of longitudinal data and data structures have emerged (see Figure X). An understanding of these data structures is helpful, as they can warrant different types of longitudinal data analysis (LDA). Given that identifying a starting point for making comparisons is somewhat arbitrary, Curran and Bauer [2019; Bauer and Curran (2019)] provide a nice on-ramp in first distinguishing between the use of “time-to-event” and “repeated measures” data. Although both model time, the former is concerned with whether and when an event occurs, whereas the later is focused on growth and change (Bauer and Curran 2019). Time-to-event structures measure time from a well-defined origin point up to the occurrence of an event of interest. This data structure is most often analyzed using survival analysis methods (e.g., hazard rate models, event history analysis, failure-time models) and the time-to-event data can be based on a single assessment or include multiple recurrent or competing events. While much has been written about “time-to-event” data (see xxx; xxx; xxxx), including a recent analysis examining exclusionary discipline in schools using data from the ABCD Study (Brislin et al. (2023)), our emphasis will be given to the modeling of “repeated measures” data.\n\n\n\nLongitudinal Data Analysis Structural Diagram\n\n\nWhen discussing longitudinal analysis, we are most often talking about data collected on the same unit (e.g., individuals) across multiple measurement occasions. However, repeated-measures analysis is not a monolith, and it will serve us well to distinguish between a few of the most common types. One such approach to repeated measures analysis is the use of time-series models. These models generally consist of a long sequence of repeated measurements (≧ 50-100 measurements) on a single (or small number of) variable of interest. Time-series analysis is often used to predict/forecast temporal trends and cyclic patterns and is geared toward making inferences about prospective outcomes within a population (with relatively with less focus on inferring individual-level mechanisms and risk factors). A related type of repeated measures analysis is Intensive Longitudinal Data (ILD). Similar to time-series analysis, ILD models involve frequent measurements (~ 30-40 measurements) of the same individuals in a relatively circumspect period of time (e.g., experience sampling to obtain time series on many individuals). Although ILD models may include slightly fewer measurement occasions than time-series data, ILD models tend to have more subjects than time series models (~ 50-100 subjects). This allows ILD models to examine short-term patterns by incorporating a time series model that can sometimes fit parameter estimates to each individual’s data in order to model individual difference outcomes. The final type of repeated measures analysis that we will primarily focus on is the longitudinal panel model. These models follow a group of individuals— a panel (also referred to as a cohort) — across relatively fewer measurement occassions (~ 5-15), and are often interested in examining change across both, within-individuals and between-individuals.\nWhile other longitudinal designs have their own unique strengths and applications, the longitudinal panel design is particularly well-suited for investigating developmental processes in the context of the ABCD Study. In the following sections, we will discuss various analytic methods commonly used to analyze longitudinal panel data, including growth models, mixed models, and a number of additional trajectory models. These methods provide valuable insights into within- and between-individual differences and are highly relevant for researchers working with the ABCD Study dataset. By focusing on these methods, we aim to equip readers with the knowledge necessary to conduct longitudinal research and perform analyses using the rich, longitudinal, and publicly available data from the ABCD Study."
  },
  {
    "objectID": "explanations.html",
    "href": "explanations.html",
    "title": "Explanations: A Brief Overview",
    "section": "",
    "text": "The primary goal of the explanation section is to provide context, background information, and a deeper understanding of the concepts, methodologies, or processes related to a specific analysis or topic. Unlike tutorials and how-to guides, which are goal-oriented and problem-oriented respectively, explanations aim to clarify and enhance the user’s knowledge. The explanation section aims to create informative, engaging, and accessible content that helps users build a solid foundation of understanding and knowledge about the subject matter.\n\n\nThe explanation section should:\n\nBe informative: Explanations should offer valuable insights and information, helping users develop a deeper understanding of the topic. This includes discussing the rationale behind certain choices, exploring alternative approaches, and explaining the advantages and disadvantages of various options.\nAddress “why” questions: Explanations should aim to answer the “why” questions users may have, delving into the reasoning, motivations, or principles that underlie a specific concept or approach.\nBe clear and concise: While explanations may go into greater depth than tutorials or how-to guides, they should still be presented in a clear and concise manner. This means using plain language, avoiding jargon, and providing examples to help users grasp the concepts more easily.\nBe well-structured: Explanations should be organized in a logical and coherent manner, making it easy for users to follow the flow of information and understand the relationships between different concepts.\nSupport other documentation types: Explanations should complement and support other types of documentation, such as tutorials and how-to guides, by providing the necessary context and background information. This may involve cross-referencing other resources or linking to related documentation.\nEngage the audience: Explanations should be engaging and interesting, capturing the user’s attention and encouraging them to explore the topic further. This may involve using analogies, storytelling, or real-world examples to illustrate the concepts and make them more relatable.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "explanations.html#what-are-explanations",
    "href": "explanations.html#what-are-explanations",
    "title": "Explanations: A Brief Overview",
    "section": "",
    "text": "The primary goal of the explanation section is to provide context, background information, and a deeper understanding of the concepts, methodologies, or processes related to a specific analysis or topic. Unlike tutorials and how-to guides, which are goal-oriented and problem-oriented respectively, explanations aim to clarify and enhance the user’s knowledge. The explanation section aims to create informative, engaging, and accessible content that helps users build a solid foundation of understanding and knowledge about the subject matter.\n\n\nThe explanation section should:\n\nBe informative: Explanations should offer valuable insights and information, helping users develop a deeper understanding of the topic. This includes discussing the rationale behind certain choices, exploring alternative approaches, and explaining the advantages and disadvantages of various options.\nAddress “why” questions: Explanations should aim to answer the “why” questions users may have, delving into the reasoning, motivations, or principles that underlie a specific concept or approach.\nBe clear and concise: While explanations may go into greater depth than tutorials or how-to guides, they should still be presented in a clear and concise manner. This means using plain language, avoiding jargon, and providing examples to help users grasp the concepts more easily.\nBe well-structured: Explanations should be organized in a logical and coherent manner, making it easy for users to follow the flow of information and understand the relationships between different concepts.\nSupport other documentation types: Explanations should complement and support other types of documentation, such as tutorials and how-to guides, by providing the necessary context and background information. This may involve cross-referencing other resources or linking to related documentation.\nEngage the audience: Explanations should be engaging and interesting, capturing the user’s attention and encouraging them to explore the topic further. This may involve using analogies, storytelling, or real-world examples to illustrate the concepts and make them more relatable.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples: A Brief Overview",
    "section": "",
    "text": "The primary goal of the “examples” section is on providing practical solutions to specific methods or analytic problems users might encounter. Unlike tutorials, which aim to teach a concept, examples are akin to ‘how-to’ guides that concentrate on the efficient execution of specific analyses without going into in-depth background explanations. By following these principles, the examples (i.e., how-to) section aims to create practical, efficient, and user-friendly guides that help users quickly identify solutions and accomplish their goals.\n\n\nThe examples section should:\n\nBe problem-oriented: Examples (i.e., ‘how-to’ guides) should address specific problems or analyses users may face, offering clear and concise solutions. Each example should focus on a single issue, making it easy for users to find and apply the relevant information.\nOffer step-by-step instructions: Similar to tutorials, examples should provide a clear sequence of steps that users can follow to solve their problem or complete their analysis. Each step of the instructions should be explained in a straightforward manner.\nBe concise: Examples should be to the point, offering just enough information to help users accomplish their goal. This means avoiding unnecessary explanations or background information, as users are typically looking for a quick solution.\nBe actionable: The content in the example section should be practical, with a focus on actionable advice that users can apply immediately. This may involve providing code snippets, configuration settings, or other specific instructions that users can implement in their projects.\nCross-reference other resources: Examples guides should link to other relevant documentation, such as explanations or reference material, for users who may require additional background information or a deeper understanding of the topic.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "examples.html#what-are-examples",
    "href": "examples.html#what-are-examples",
    "title": "Examples: A Brief Overview",
    "section": "",
    "text": "The primary goal of the “examples” section is on providing practical solutions to specific methods or analytic problems users might encounter. Unlike tutorials, which aim to teach a concept, examples are akin to ‘how-to’ guides that concentrate on the efficient execution of specific analyses without going into in-depth background explanations. By following these principles, the examples (i.e., how-to) section aims to create practical, efficient, and user-friendly guides that help users quickly identify solutions and accomplish their goals.\n\n\nThe examples section should:\n\nBe problem-oriented: Examples (i.e., ‘how-to’ guides) should address specific problems or analyses users may face, offering clear and concise solutions. Each example should focus on a single issue, making it easy for users to find and apply the relevant information.\nOffer step-by-step instructions: Similar to tutorials, examples should provide a clear sequence of steps that users can follow to solve their problem or complete their analysis. Each step of the instructions should be explained in a straightforward manner.\nBe concise: Examples should be to the point, offering just enough information to help users accomplish their goal. This means avoiding unnecessary explanations or background information, as users are typically looking for a quick solution.\nBe actionable: The content in the example section should be practical, with a focus on actionable advice that users can apply immediately. This may involve providing code snippets, configuration settings, or other specific instructions that users can implement in their projects.\nCross-reference other resources: Examples guides should link to other relevant documentation, such as explanations or reference material, for users who may require additional background information or a deeper understanding of the topic.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials: A Brief Overview",
    "section": "",
    "text": "The primary goal of the tutorial section is to help users learn and understand a specific topic or method by providing an introductory and hands-on, step-by-step guide. This section aims to create engaging, accessible, and effective entry learning experiences that enable users to develop their skills and understanding of the subject matter.\n\n\nThe tutorial section should:\n\nBe goal-oriented: Tutorials should focus on a specific task the user wants to accomplish. By concentrating on a particular outcome, the tutorial becomes more engaging and relevant to the user.\nProvide clear steps: Tutorials should offer step-by-step instructions, guiding users through the process of achieving their goal. Each step should be clearly explained and easy to follow, making sure users can successfully complete the tutorial.\nUse examples: Tutorials should include real-world examples and use cases that are relatable and meaningful to the user. This helps users better understand the concepts being taught and how they can apply them in their own projects.\nBe practical: Tutorials should emphasize hands-on learning, encouraging users to actively engage with the content and practice the skills they are learning. This may involve providing code samples, interactive exercises, or other activities that help users develop their understanding.\nEncourage exploration: Tutorials should encourage users to explore and experiment with the concepts they are learning. This may involve providing opportunities for users to adapt the tutorial to their own needs or offering suggestions for further experimentation.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "tutorials.html#what-are-tutorials",
    "href": "tutorials.html#what-are-tutorials",
    "title": "Tutorials: A Brief Overview",
    "section": "",
    "text": "The primary goal of the tutorial section is to help users learn and understand a specific topic or method by providing an introductory and hands-on, step-by-step guide. This section aims to create engaging, accessible, and effective entry learning experiences that enable users to develop their skills and understanding of the subject matter.\n\n\nThe tutorial section should:\n\nBe goal-oriented: Tutorials should focus on a specific task the user wants to accomplish. By concentrating on a particular outcome, the tutorial becomes more engaging and relevant to the user.\nProvide clear steps: Tutorials should offer step-by-step instructions, guiding users through the process of achieving their goal. Each step should be clearly explained and easy to follow, making sure users can successfully complete the tutorial.\nUse examples: Tutorials should include real-world examples and use cases that are relatable and meaningful to the user. This helps users better understand the concepts being taught and how they can apply them in their own projects.\nBe practical: Tutorials should emphasize hands-on learning, encouraging users to actively engage with the content and practice the skills they are learning. This may involve providing code samples, interactive exercises, or other activities that help users develop their understanding.\nEncourage exploration: Tutorials should encourage users to explore and experiment with the concepts they are learning. This may involve providing opportunities for users to adapt the tutorial to their own needs or offering suggestions for further experimentation.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "References: A Brief Overview",
    "section": "",
    "text": "The primary goal of the “Reference” section is on providing accurate, concise, and structured information about specific components, functions, or features of a method, analysis, or framework. Unlike tutorials, how-to guides, and explanations, which aim to teach, solve problems, and provide context, respectively, reference materials serve as a quick lookup resource for users to find the exact information they need. By following these principles, the reference section aims to create a reliable, user-friendly, and easily searchable resource that provides users with the specific information they need to effectively use and understand the method, analysis, or framework.\n\n\nThe reference section should:\n\nBe comprehensive: Reference materials should cover all relevant aspects of the method, analysis, or framework, including components, functions, features, and configurations. The goal is to provide users with a complete and authoritative source of information.\nBe accurate: The information in the reference section should be precise and up-to-date, ensuring that users can rely on it for their projects. This may involve regular updates and checks to maintain accuracy.\nBe concise: Reference materials should be brief and to the point, providing just enough information for users to understand and use the specific analysis or method. This means avoiding unnecessary explanations, examples, or background information.\nFollow a consistent format: The reference section should use a consistent format and structure, making it easy for users to locate and access the information they need. This may involve using templates, standardized headings, or other organizational tools.\nBe easily searchable: Reference materials should be designed with searchability in mind, enabling users to quickly find the information they need. This may involve using clear and descriptive titles, headings, and keywords, as well as implementing search functionality or indexing.\nCross-reference other resources: The reference section should link to other relevant documentation, such as tutorials, how-to guides, or explanations, for users who may require additional information or a deeper understanding of the topic.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "reference.html#what-are-references",
    "href": "reference.html#what-are-references",
    "title": "References: A Brief Overview",
    "section": "",
    "text": "The primary goal of the “Reference” section is on providing accurate, concise, and structured information about specific components, functions, or features of a method, analysis, or framework. Unlike tutorials, how-to guides, and explanations, which aim to teach, solve problems, and provide context, respectively, reference materials serve as a quick lookup resource for users to find the exact information they need. By following these principles, the reference section aims to create a reliable, user-friendly, and easily searchable resource that provides users with the specific information they need to effectively use and understand the method, analysis, or framework.\n\n\nThe reference section should:\n\nBe comprehensive: Reference materials should cover all relevant aspects of the method, analysis, or framework, including components, functions, features, and configurations. The goal is to provide users with a complete and authoritative source of information.\nBe accurate: The information in the reference section should be precise and up-to-date, ensuring that users can rely on it for their projects. This may involve regular updates and checks to maintain accuracy.\nBe concise: Reference materials should be brief and to the point, providing just enough information for users to understand and use the specific analysis or method. This means avoiding unnecessary explanations, examples, or background information.\nFollow a consistent format: The reference section should use a consistent format and structure, making it easy for users to locate and access the information they need. This may involve using templates, standardized headings, or other organizational tools.\nBe easily searchable: Reference materials should be designed with searchability in mind, enabling users to quickly find the information they need. This may involve using clear and descriptive titles, headings, and keywords, as well as implementing search functionality or indexing.\nCross-reference other resources: The reference section should link to other relevant documentation, such as tutorials, how-to guides, or explanations, for users who may require additional information or a deeper understanding of the topic.\n\n\nFor additional information, see [https://documentation.divio.com/]"
  },
  {
    "objectID": "2_Tutorials_ResidualizedChangeScores.html",
    "href": "2_Tutorials_ResidualizedChangeScores.html",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Residualized change scores are a statistical approach that allows for examining the true change in a variable over time by controlling for the initial level of that variable. This method is commonly employed in longitudinal research, especially in fields such as psychology, education, and medicine. Residualized change scores offer a way to address a significant concern in repeated measures: the potential correlation between initial scores and their subsequent change. By using regression analyses, the score at Time 2 is predicted based on the score at Time 1. The residuals from this analysis, which represent the portion of the Time 2 score not predicted by the Time 1 score, give the residualized change score. This score effectively captures the ‘true change’ from Time 1 to Time 2. In essence, this method seeks to answer the question: Given where someone started (Time 1), did they end up higher or lower than expected at Time 2? By removing the influence of initial levels, researchers can gain clearer insights into the factors that drive genuine change over time.\n\n\n\nFigure x. Difference score add explanation or tag\n\n\n\n\nYou should use difference scores in the following scenario:\n\nYou want to know: The impact of interventions or time on a measured variable.\nYour variable: Has paired measurements, like pre-test and post-test scores.\nYou have: Matched data for each set, ensuring valid difference computations.\n\n\n\n\nIn this tutorial, we will guide you through two simple examples of using difference scores. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of difference scores.\nCalculate difference scores using example data.\nInterpret the results of the difference scores analysis."
  },
  {
    "objectID": "2_Tutorials_ResidualizedChangeScores.html#overview",
    "href": "2_Tutorials_ResidualizedChangeScores.html#overview",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Residualized change scores are a statistical approach that allows for examining the true change in a variable over time by controlling for the initial level of that variable. This method is commonly employed in longitudinal research, especially in fields such as psychology, education, and medicine. Residualized change scores offer a way to address a significant concern in repeated measures: the potential correlation between initial scores and their subsequent change. By using regression analyses, the score at Time 2 is predicted based on the score at Time 1. The residuals from this analysis, which represent the portion of the Time 2 score not predicted by the Time 1 score, give the residualized change score. This score effectively captures the ‘true change’ from Time 1 to Time 2. In essence, this method seeks to answer the question: Given where someone started (Time 1), did they end up higher or lower than expected at Time 2? By removing the influence of initial levels, researchers can gain clearer insights into the factors that drive genuine change over time.\n\n\n\nFigure x. Difference score add explanation or tag\n\n\n\n\nYou should use difference scores in the following scenario:\n\nYou want to know: The impact of interventions or time on a measured variable.\nYour variable: Has paired measurements, like pre-test and post-test scores.\nYou have: Matched data for each set, ensuring valid difference computations.\n\n\n\n\nIn this tutorial, we will guide you through two simple examples of using difference scores. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of difference scores.\nCalculate difference scores using example data.\nInterpret the results of the difference scores analysis."
  },
  {
    "objectID": "2_Tutorials_ResidualizedChangeScores.html#basic-example",
    "href": "2_Tutorials_ResidualizedChangeScores.html#basic-example",
    "title": "Residualized Change Scores",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at two time points: T1-T2.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 1000 rows (2 rows for each of the 500 individuals).\n\n# Creating the example dataset\nset.seed(123) # for reproducibility\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, \n                      0.30, 1), \n                      nrow=2)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data\nrandom_data &lt;- matrix(rnorm(n * 2), ncol=2)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=2),\n  TimePoint = rep(c(\"T1\", \"T2\"), times=n),\n  Job_Satisfaction = c(correlated_data[,1], correlated_data[,2])\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Residualized Change Score, we will conduct a basic regression analysis using base R. Residualized change scores allow us to estimate the change in an outcome variable after accounting for its initial value, effectively removing the effect of the baseline measurement.\n\nModel\n\n\nCode\n# Extracting Job_Satisfaction scores for the two time points\nt1_scores_large &lt;- data_large$Job_Satisfaction[data_large$TimePoint == \"T1\"]\nt2_scores_large &lt;- data_large$Job_Satisfaction[data_large$TimePoint == \"T2\"]\n\n\n\n\nFit Model\n\n\nCode\n# Perform the paired t-test\ntest_result &lt;- t.test(t1_scores_large, t2_scores_large, paired = TRUE)\n\n\n\n\n\nModel Summary\n\n\nCode\nprint(test_result)\n\n\n\n    Paired t-test\n\ndata:  t1_scores_large and t2_scores_large\nt = 2.2206, df = 499, p-value = 0.02682\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.01659222 0.27137979\nsample estimates:\nmean difference \n       0.143986 \n\n\n\n\nInterpreting the Results\nThe paired t-test was conducted to compare Life Satisfaction scores at two time points: T1 and T2. The test yielded a t-value of t=2.221 with 499 degrees of freedom. The associated p-value is p=0.027. This p-value is greater than the common significance level of 0.05, which means that we do not have enough evidence to reject the null hypothesis. The 95% confidence interval for the mean difference ranges from 0.017 to 0.271. The observed mean difference between the two time points is 0.144."
  },
  {
    "objectID": "2_Tutorials_ResidualizedChangeScores.html#conclusion",
    "href": "2_Tutorials_ResidualizedChangeScores.html#conclusion",
    "title": "Residualized Change Scores",
    "section": "Conclusion",
    "text": "Conclusion\nThere is no statistically significant difference in Life Satisfaction scores between the two time points, T1 and T2, based on the paired t-test results. The mean difference, although positive (0.144), is not significant at the 0.05 level."
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "This example assesses growth in a subsample of ABCD participants from Baseline (T0) to the 1-Year follow-up (T1), using height as a representative metric. The analysis is conducted in two primary steps: 1) a difference score is calculated between baseline and Year_1 height measurements for each participant; 2) a one-sample t-test is used to test whether the difference score is statistically different than a null hypothesis of zero change. Finally, a visual inspection is further conducted via a scatterplot to graphically represent the relationship between participant’s height at Baseline and Year_1. The ensuing analysis and interpretations are detailed in the subsequent sections.\n\n\n\n\n\nInstall PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)\n\n\n\n\n\n\n\n\n\n\nRead and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild Model: Step IBuild Model: Step 2Model Plots\n\n\n\nThe code snippet below tells R to compute a difference score by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nSTEP 1: Compute Difference Score\n\n\nCode\n# Define the function to compute difference scores for a given variable and provide a summary\n# Function to compute difference scores for a given variable and provide a summary\ncompute_difference_and_summary &lt;- function(df, variable_name) {\n    # Define the event names of interest\n    baseline_event &lt;- \"baseline_year_1_arm_1\"\n    followup_event &lt;- \"1_year_follow_up_y_arm_1\"\n\n    # Compute the difference between Baseline and Year 1 data for the given variable\n    diff_data &lt;- df %&gt;%\n        filter(eventname %in% c(baseline_event, followup_event)) %&gt;% # Filter for specific event names\n        select(src_subject_id, eventname, all_of(variable_name)) %&gt;% # Select required columns\n        spread(eventname, variable_name) %&gt;% # Convert data from long to wide format\n        mutate(diff = get(followup_event) - get(baseline_event)) %&gt;% # Compute difference between the two time points\n        drop_na(diff) # Exclude rows with NA in the computed difference\n\n    # Summarize the computed difference scores\n    diff_summary &lt;- summary(diff_data$diff)\n\n    # Return the difference data and its summary\n    list(data = diff_data, summary = diff_summary)\n}\n\n# List of variables for which difference scores are to be computed\nvariables_of_interest &lt;- c(\"anthroheightcalc\")\n\n# Compute and store difference scores and summaries for each variable in a list\ndifference_and_summary_list &lt;- lapply(variables_of_interest, function(var) {\n    compute_difference_and_summary(df, var)\n})\n\n# Extract the difference data for the 'anthroheightcalc' variable\nheight_diff_data &lt;- difference_and_summary_list[[1]]$data\n\n# Merge the 'diff' column back to the main df using 'src_subject_id' as the key\ndf &lt;- left_join(df, height_diff_data %&gt;% select(src_subject_id, diff), by = \"src_subject_id\")\n\n\nDescriptive Statistics for the Difference Score\n\n\nCode\n# Compute statistical summaries for the difference score variable\nlapply(difference_and_summary_list, function(item) {\n    print(item$summary)\n})\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\n[[1]]\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\nThis summary of the difference score variable indicates that the differences range from a decrease of -51.875 units to an increase of 67 units. The median difference is 2.25 units, and the average difference is approximately 2.3660611 units. There are 0 missing values in this difference score dataset.\nDescriptive Statistics for the Height Variable\n\n\nCode\n## Summary statistics\n# Compute summary statistics for Height by eventname\nsummary &lt;- df %&gt;%\n    group_by(eventname) %&gt;%\n    get_summary_stats(anthroheightcalc, type = \"mean_sd\")\n\ndata.frame(summary)\n\n\n\n\n  \n\n\n\nThe summary statistics provide insights into the Height variable across different eventname categories. At baseline, the average height is approximately 55.241 units with a standard deviation of 3.331. Over the years, there’s a noticeable increase in average height: by the first year, it’s about 57.595 units, and it continues to rise, reaching approximately 64.694 units by the fourth year. The standard deviation remains relatively consistent over the years, suggesting similar variability in height measurements across these time points.\n\n\n\n\nA one-sample t-test is computed to examine whether the average difference score is different than zero (indicating change).\nSTEP 2: Conduct t-test on Difference Score\n\n\nCode\n# Extract the difference scores for height from the computed list\ndiff_data &lt;- difference_and_summary_list[[1]]$data$diff\n\n# Perform a one-sample t-test on the difference scores for height\ntest_result &lt;- t.test(diff_data, mu = 0, na.rm = TRUE)\n\ntest_result\n\n\n\n    One Sample t-test\n\ndata:  diff_data\nt = 99, df = 11135, p-value &lt;2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.32 2.41\nsample estimates:\nmean of x \n     2.37 \n\n\nThis one-sample t-test evaluates whether the average difference score deviates significantly from 0. The model output provides:\n\na t-statistic of 98.8615605;\ndegrees of freedom of 1.1135^{4};\na p-value of 0;\na mean estimate of 2.3660611;\n95% confidence intervals from 2.3191481 to 2.4129742.\n95% confidence intervals from 2.319 to 2.413.\n\nWith a t-value of 98.8615605 and a p-value of 0, we can determine the significance of the difference. The average difference score across the sample is approximately 2.37. Furthermore, we can be 95% confident that the true mean difference score in the population lies between approximately 2.32 and 2.41. This indicates the degree to which there’s a change in height from Baseline to Year 1.\n\n\n\n\n\n\ntesting\n# Filter data for Baseline\nbaseline_data &lt;- df %&gt;% filter(eventname == \"Baseline\")\n\n# Filter data for Year_1 and join with Baseline data\njoined_data &lt;- baseline_data %&gt;%\n    left_join(df %&gt;% filter(eventname == \"Year_1\"), by = \"src_subject_id\", suffix = c(\"_baseline\", \"_year1\"))\n\n# Create scatterplot\nscatterplot &lt;- ggplot(df, aes(x = anthroheightcalc, y = diff)) +\n    geom_point(aes(color = sex), alpha = 0.5) + # Setting alpha to ensure points are visible\n    geom_smooth(method = \"lm\", color = \"red\") + # Adding linear regression line\n    labs(\n        title = \"Scatterplot of anthroheightcalc vs. Difference Score\",\n        x = \"anthroheightcalc\",\n        y = \"Difference Score\"\n    ) +\n    theme_minimal()\n\nscatterplot\n\n\n\n\n\n\n\n\n\nThe scatterplot visually depicts the relationship between individuals’ heights at baseline and their heights at Year_1. Each point on the plot represents an individual, with their baseline height plotted on the x-axis and their Year_1 height on the y-axis. A noticeable positive linear trend can be observed, as highlighted by the blue regression line, indicating that those who were taller at baseline generally remained taller at Year_1. The strength and direction of this relationship can be further quantified with correlation coefficients, but visually, the data suggests a strong positive association between baseline and Year_1 heights.\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\nIn the study sample, the average height at baseline was approximately 55.241 units with a standard deviation of 3.331, which showed a noticeable increase to an average of 57.595 units by Year_1. A one-sample t-test was conducted to determine if the mean difference in height from baseline to Year_1 significantly deviated from zero. The results indicated a statistically significant increase with a mean difference of approximately 2.37 units (2.32 to 2.41), t(1.1135^{4}) = 98.862, p &lt; 0. Further, a scatterplot visualizing the relationship between baseline and Year_1 heights showed a strong positive linear trend. This suggests that participants who were taller at baseline generally remained taller at Year_1, reaffirming the consistent growth trend observed in the data."
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html#overview",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html#overview",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "This example assesses growth in a subsample of ABCD participants from Baseline (T0) to the 1-Year follow-up (T1), using height as a representative metric. The analysis is conducted in two primary steps: 1) a difference score is calculated between baseline and Year_1 height measurements for each participant; 2) a one-sample t-test is used to test whether the difference score is statistically different than a null hypothesis of zero change. Finally, a visual inspection is further conducted via a scatterplot to graphically represent the relationship between participant’s height at Baseline and Year_1. The ensuing analysis and interpretations are detailed in the subsequent sections."
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html#preliminary-setup",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html#preliminary-setup",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "Install PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)"
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html#descriptives-overview",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html#descriptives-overview",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "Read and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title"
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html#results",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html#results",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "Build Model: Step IBuild Model: Step 2Model Plots\n\n\n\nThe code snippet below tells R to compute a difference score by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nSTEP 1: Compute Difference Score\n\n\nCode\n# Define the function to compute difference scores for a given variable and provide a summary\n# Function to compute difference scores for a given variable and provide a summary\ncompute_difference_and_summary &lt;- function(df, variable_name) {\n    # Define the event names of interest\n    baseline_event &lt;- \"baseline_year_1_arm_1\"\n    followup_event &lt;- \"1_year_follow_up_y_arm_1\"\n\n    # Compute the difference between Baseline and Year 1 data for the given variable\n    diff_data &lt;- df %&gt;%\n        filter(eventname %in% c(baseline_event, followup_event)) %&gt;% # Filter for specific event names\n        select(src_subject_id, eventname, all_of(variable_name)) %&gt;% # Select required columns\n        spread(eventname, variable_name) %&gt;% # Convert data from long to wide format\n        mutate(diff = get(followup_event) - get(baseline_event)) %&gt;% # Compute difference between the two time points\n        drop_na(diff) # Exclude rows with NA in the computed difference\n\n    # Summarize the computed difference scores\n    diff_summary &lt;- summary(diff_data$diff)\n\n    # Return the difference data and its summary\n    list(data = diff_data, summary = diff_summary)\n}\n\n# List of variables for which difference scores are to be computed\nvariables_of_interest &lt;- c(\"anthroheightcalc\")\n\n# Compute and store difference scores and summaries for each variable in a list\ndifference_and_summary_list &lt;- lapply(variables_of_interest, function(var) {\n    compute_difference_and_summary(df, var)\n})\n\n# Extract the difference data for the 'anthroheightcalc' variable\nheight_diff_data &lt;- difference_and_summary_list[[1]]$data\n\n# Merge the 'diff' column back to the main df using 'src_subject_id' as the key\ndf &lt;- left_join(df, height_diff_data %&gt;% select(src_subject_id, diff), by = \"src_subject_id\")\n\n\nDescriptive Statistics for the Difference Score\n\n\nCode\n# Compute statistical summaries for the difference score variable\nlapply(difference_and_summary_list, function(item) {\n    print(item$summary)\n})\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\n[[1]]\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -51.9     1.8     2.2     2.4     3.0    67.0 \n\n\nThis summary of the difference score variable indicates that the differences range from a decrease of -51.875 units to an increase of 67 units. The median difference is 2.25 units, and the average difference is approximately 2.3660611 units. There are 0 missing values in this difference score dataset.\nDescriptive Statistics for the Height Variable\n\n\nCode\n## Summary statistics\n# Compute summary statistics for Height by eventname\nsummary &lt;- df %&gt;%\n    group_by(eventname) %&gt;%\n    get_summary_stats(anthroheightcalc, type = \"mean_sd\")\n\ndata.frame(summary)\n\n\n\n\n  \n\n\n\nThe summary statistics provide insights into the Height variable across different eventname categories. At baseline, the average height is approximately 55.241 units with a standard deviation of 3.331. Over the years, there’s a noticeable increase in average height: by the first year, it’s about 57.595 units, and it continues to rise, reaching approximately 64.694 units by the fourth year. The standard deviation remains relatively consistent over the years, suggesting similar variability in height measurements across these time points.\n\n\n\n\nA one-sample t-test is computed to examine whether the average difference score is different than zero (indicating change).\nSTEP 2: Conduct t-test on Difference Score\n\n\nCode\n# Extract the difference scores for height from the computed list\ndiff_data &lt;- difference_and_summary_list[[1]]$data$diff\n\n# Perform a one-sample t-test on the difference scores for height\ntest_result &lt;- t.test(diff_data, mu = 0, na.rm = TRUE)\n\ntest_result\n\n\n\n    One Sample t-test\n\ndata:  diff_data\nt = 99, df = 11135, p-value &lt;2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 2.32 2.41\nsample estimates:\nmean of x \n     2.37 \n\n\nThis one-sample t-test evaluates whether the average difference score deviates significantly from 0. The model output provides:\n\na t-statistic of 98.8615605;\ndegrees of freedom of 1.1135^{4};\na p-value of 0;\na mean estimate of 2.3660611;\n95% confidence intervals from 2.3191481 to 2.4129742.\n95% confidence intervals from 2.319 to 2.413.\n\nWith a t-value of 98.8615605 and a p-value of 0, we can determine the significance of the difference. The average difference score across the sample is approximately 2.37. Furthermore, we can be 95% confident that the true mean difference score in the population lies between approximately 2.32 and 2.41. This indicates the degree to which there’s a change in height from Baseline to Year 1.\n\n\n\n\n\n\ntesting\n# Filter data for Baseline\nbaseline_data &lt;- df %&gt;% filter(eventname == \"Baseline\")\n\n# Filter data for Year_1 and join with Baseline data\njoined_data &lt;- baseline_data %&gt;%\n    left_join(df %&gt;% filter(eventname == \"Year_1\"), by = \"src_subject_id\", suffix = c(\"_baseline\", \"_year1\"))\n\n# Create scatterplot\nscatterplot &lt;- ggplot(df, aes(x = anthroheightcalc, y = diff)) +\n    geom_point(aes(color = sex), alpha = 0.5) + # Setting alpha to ensure points are visible\n    geom_smooth(method = \"lm\", color = \"red\") + # Adding linear regression line\n    labs(\n        title = \"Scatterplot of anthroheightcalc vs. Difference Score\",\n        x = \"anthroheightcalc\",\n        y = \"Difference Score\"\n    ) +\n    theme_minimal()\n\nscatterplot\n\n\n\n\n\n\n\n\n\nThe scatterplot visually depicts the relationship between individuals’ heights at baseline and their heights at Year_1. Each point on the plot represents an individual, with their baseline height plotted on the x-axis and their Year_1 height on the y-axis. A noticeable positive linear trend can be observed, as highlighted by the blue regression line, indicating that those who were taller at baseline generally remained taller at Year_1. The strength and direction of this relationship can be further quantified with correlation coefficients, but visually, the data suggests a strong positive association between baseline and Year_1 heights."
  },
  {
    "objectID": "1a_Examples_DifferenceScores_PairedTtests.html#wrapping-up",
    "href": "1a_Examples_DifferenceScores_PairedTtests.html#wrapping-up",
    "title": "Difference Scores: Paired Samples T-test",
    "section": "",
    "text": "Write-up\n\n\n\nIn the study sample, the average height at baseline was approximately 55.241 units with a standard deviation of 3.331, which showed a noticeable increase to an average of 57.595 units by Year_1. A one-sample t-test was conducted to determine if the mean difference in height from baseline to Year_1 significantly deviated from zero. The results indicated a statistically significant increase with a mean difference of approximately 2.37 units (2.32 to 2.41), t(1.1135^{4}) = 98.862, p &lt; 0. Further, a scatterplot visualizing the relationship between baseline and Year_1 heights showed a strong positive linear trend. This suggests that participants who were taller at baseline generally remained taller at Year_1, reaffirming the consistent growth trend observed in the data."
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html",
    "href": "2_Examples_ResidualizedChangeScores.html",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "This example assesses whether height, measured at the 1-Year follow-up (T1) in a subsample of ABCD participants, is predicted by weight and height measured at baseline (T0). A visual inspection is further conducted via a scatterplot to graphically represent the relationship between height at Year_1 and weight at baseline, controlling for height at baseline. The ensuing analysis and interpretations are detailed in the subsequent sections.\n\n\n\n\n\nInstall PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)\n\n\n\n\n\n\n\n\n\n\nRead and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild ModelModel SummaryModel Plots\n\n\n\nThe code snippet below tells R to conduct a multiple regression analysis by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nCompute Multiple Regression Model\n\n\nCode\n# Split data using the correct eventname values\nbaseline_data &lt;- df %&gt;% \n  filter(eventname == \"baseline_year_1_arm_1\") %&gt;% \n  select(src_subject_id, Height_baseline = anthroheightcalc, Weight_baseline = anthroweightcalc)\n\nfollowup_data &lt;- df %&gt;% \n  filter(eventname == \"1_year_follow_up_y_arm_1\") %&gt;% \n  select(src_subject_id, Height_followup = anthroheightcalc)\n\n# Merge baseline and follow-up data\nmerged_data &lt;- baseline_data %&gt;%\n  left_join(followup_data, by = \"src_subject_id\")\n\n# Check the first few rows of the merged data\n#head(merged_data)\n\n# Fit the regression model excluding NA values\nmodel &lt;- lm(Height_followup ~ Height_baseline + Weight_baseline, data = merged_data, na.action = na.exclude)\n\n\n\n\n\n\nA plot to show xxxxx.\n\n\nCode\nmodel_summary &lt;- summary(model)\nprint(model_summary)\n\n\n\nCall:\nlm(formula = Height_followup ~ Height_baseline + Weight_baseline, \n    data = merged_data, na.action = na.exclude)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.49  -0.83  -0.09   0.74  65.88 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     17.18159    0.41712    41.2   &lt;2e-16 ***\nHeight_baseline  0.68665    0.00847    81.0   &lt;2e-16 ***\nWeight_baseline  0.03026    0.00120    25.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.38 on 11130 degrees of freedom\n  (735 observations deleted due to missingness)\nMultiple R-squared:  0.575, Adjusted R-squared:  0.575 \nF-statistic: 7.54e+03 on 2 and 11130 DF,  p-value: &lt;2e-16\n\n\nCode\npar(mfrow = c(2, 2))  # Set up a 2x2 plotting area\nplot(model)  # Generate diagnostic plots\n\n\n\n\n\n\n\n\n\nThis regression analysis evaluates how baseline height and weight predict children’s height at a 1-year follow-up. The output from our model provides several key statistics:\n\nan F-statistic of 7539.2168865;\ndegrees of freedom of 11130;\nparameter estimates for baseline height and weight;\nstandard errors for these estimates; and\np-values for the significance of these estimates.\n\nFor every one unit increase in baseline height, there’s an associated increase of approximately 0.687 units in the follow-up height, and this effect was statistically significant with a p-value of 0. Similarly, for every one unit increase in baseline weight, there’s an associated increase of approximately 0.0303 units in the follow-up height, which is also statistically significant with a p-value of 3.9902186^{-137}. Overall, this model explains a substantial portion of the variance in follow-up height, with an adjusted R-squared value of 0.5753. The overall model is highly significant with a p-value less than 0, indicating that the predictors, collectively, have a significant relationship with the dependent variable.\n\n\n\n\n\n\nCode\n# Scatterplot with regression line for Weight_baseline\nggplot(merged_data, aes(x = Weight_baseline, y = Height_followup)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"black\") + \n  labs(title = \"Relationship between Baseline Weight and Follow-up Height\",\n       x = \"Baseline Weight\",\n       y = \"Follow-up Height\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scatterplot visually depicts the relationship between children’s weights at baseline (Weight_baseline) and their heights at Year_1 (Height_followup). Each point on the plot represents a child, with their baseline weight plotted on the x-axis and their Year_1 height on the y-axis. The clear positive linear trend, as illustrated by the black regression line, indicates that children who had higher weights at baseline generally had higher heights at Year_1. While further statistical analyses can quantify the strength and direction of this relationship, visually, the data suggests a pronounced positive association between baseline weight and follow-up height.\n\n\n\n\n\n\n\n\n\n\nWrite-up\n\n\n\nThe regression analysis was conducted to predict children’s height at the 1-Year follow-up using their baseline height (Height_baseline) and weight (Weight_baseline). Both predictors were statistically significant. Specifically, for every one-unit increase in baseline height, the height at Year_1 increased by approximately 0.687 units, while holding weight constant. Similarly, for every one-unit increase in baseline weight, the height at Year_1 increased by about 0.0303 units, while holding baseline height constant. The overall model explained a substantial 57.53% of the variance in height at the 1-Year follow-up, as indicated by the adjusted R-squared value."
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html#overview",
    "href": "2_Examples_ResidualizedChangeScores.html#overview",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "This example assesses whether height, measured at the 1-Year follow-up (T1) in a subsample of ABCD participants, is predicted by weight and height measured at baseline (T0). A visual inspection is further conducted via a scatterplot to graphically represent the relationship between height at Year_1 and weight at baseline, controlling for height at baseline. The ensuing analysis and interpretations are detailed in the subsequent sections."
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html#preliminary-setup",
    "href": "2_Examples_ResidualizedChangeScores.html#preliminary-setup",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Install PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)"
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html#descriptives-overview",
    "href": "2_Examples_ResidualizedChangeScores.html#descriptives-overview",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Read and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title"
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html#results",
    "href": "2_Examples_ResidualizedChangeScores.html#results",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Build ModelModel SummaryModel Plots\n\n\n\nThe code snippet below tells R to conduct a multiple regression analysis by subtracting each participant’s height at T1 from their height at T0. Relevant summary statistics are also provided.\nCompute Multiple Regression Model\n\n\nCode\n# Split data using the correct eventname values\nbaseline_data &lt;- df %&gt;% \n  filter(eventname == \"baseline_year_1_arm_1\") %&gt;% \n  select(src_subject_id, Height_baseline = anthroheightcalc, Weight_baseline = anthroweightcalc)\n\nfollowup_data &lt;- df %&gt;% \n  filter(eventname == \"1_year_follow_up_y_arm_1\") %&gt;% \n  select(src_subject_id, Height_followup = anthroheightcalc)\n\n# Merge baseline and follow-up data\nmerged_data &lt;- baseline_data %&gt;%\n  left_join(followup_data, by = \"src_subject_id\")\n\n# Check the first few rows of the merged data\n#head(merged_data)\n\n# Fit the regression model excluding NA values\nmodel &lt;- lm(Height_followup ~ Height_baseline + Weight_baseline, data = merged_data, na.action = na.exclude)\n\n\n\n\n\n\nA plot to show xxxxx.\n\n\nCode\nmodel_summary &lt;- summary(model)\nprint(model_summary)\n\n\n\nCall:\nlm(formula = Height_followup ~ Height_baseline + Weight_baseline, \n    data = merged_data, na.action = na.exclude)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.49  -0.83  -0.09   0.74  65.88 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     17.18159    0.41712    41.2   &lt;2e-16 ***\nHeight_baseline  0.68665    0.00847    81.0   &lt;2e-16 ***\nWeight_baseline  0.03026    0.00120    25.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.38 on 11130 degrees of freedom\n  (735 observations deleted due to missingness)\nMultiple R-squared:  0.575, Adjusted R-squared:  0.575 \nF-statistic: 7.54e+03 on 2 and 11130 DF,  p-value: &lt;2e-16\n\n\nCode\npar(mfrow = c(2, 2))  # Set up a 2x2 plotting area\nplot(model)  # Generate diagnostic plots\n\n\n\n\n\n\n\n\n\nThis regression analysis evaluates how baseline height and weight predict children’s height at a 1-year follow-up. The output from our model provides several key statistics:\n\nan F-statistic of 7539.2168865;\ndegrees of freedom of 11130;\nparameter estimates for baseline height and weight;\nstandard errors for these estimates; and\np-values for the significance of these estimates.\n\nFor every one unit increase in baseline height, there’s an associated increase of approximately 0.687 units in the follow-up height, and this effect was statistically significant with a p-value of 0. Similarly, for every one unit increase in baseline weight, there’s an associated increase of approximately 0.0303 units in the follow-up height, which is also statistically significant with a p-value of 3.9902186^{-137}. Overall, this model explains a substantial portion of the variance in follow-up height, with an adjusted R-squared value of 0.5753. The overall model is highly significant with a p-value less than 0, indicating that the predictors, collectively, have a significant relationship with the dependent variable.\n\n\n\n\n\n\nCode\n# Scatterplot with regression line for Weight_baseline\nggplot(merged_data, aes(x = Weight_baseline, y = Height_followup)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"black\") + \n  labs(title = \"Relationship between Baseline Weight and Follow-up Height\",\n       x = \"Baseline Weight\",\n       y = \"Follow-up Height\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scatterplot visually depicts the relationship between children’s weights at baseline (Weight_baseline) and their heights at Year_1 (Height_followup). Each point on the plot represents a child, with their baseline weight plotted on the x-axis and their Year_1 height on the y-axis. The clear positive linear trend, as illustrated by the black regression line, indicates that children who had higher weights at baseline generally had higher heights at Year_1. While further statistical analyses can quantify the strength and direction of this relationship, visually, the data suggests a pronounced positive association between baseline weight and follow-up height."
  },
  {
    "objectID": "2_Examples_ResidualizedChangeScores.html#wrapping-up",
    "href": "2_Examples_ResidualizedChangeScores.html#wrapping-up",
    "title": "Residualized Change Scores",
    "section": "",
    "text": "Write-up\n\n\n\nThe regression analysis was conducted to predict children’s height at the 1-Year follow-up using their baseline height (Height_baseline) and weight (Weight_baseline). Both predictors were statistically significant. Specifically, for every one-unit increase in baseline height, the height at Year_1 increased by approximately 0.687 units, while holding weight constant. Similarly, for every one-unit increase in baseline weight, the height at Year_1 increased by about 0.0303 units, while holding baseline height constant. The overall model explained a substantial 57.53% of the variance in height at the 1-Year follow-up, as indicated by the adjusted R-squared value."
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html",
    "href": "3b_Examples_LinearMixedModels.html",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "The Linear Mixed Model with a random intercept and slope (LMM:ris) extends the traditional fixed-effect linear regression by incorporating both a subject-specific random intercept and a random slope. This allows each participant to have their own unique intercept and slope values, reflecting individual starting points and rates of change, in addition to the overall mean-level (fixed-effect) trajectory.\nIn this example, we will utilize the LMM:ris to analyze height trajectories obtained across multiple measurement occasions for a sample of youth participating in the ABCD Study. Our primary objective is to understand the stability and evolution in height measurements, while factoring in the clustered nature of observations within individuals over time. The LMM:ris facilitates this by concurrently modeling an overarching sample mean trajectory (fixed effect) and individual variations (random effects) in both starting points (intercepts) and growth rates (slopes) around this mean trajectory.\n\n\n\n\n\nInstall PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\",\"report\",\"broom\",\"gridExtra\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\nlibrary(report)       # Easy reporting of regression analyses\nlibrary(broom)        # Tidy and augment statistical models output\nlibrary(gridExtra)    # Arrange multiple grid-based plots on a page\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)\n\n\n\n\n\n\n\n\n\n\nRead and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: panel-tabset ### Build Model {.tabset .tabset-fade .tabset-pills}\n::: blue\nThe code fits a linear mixed model to examine the ‘Height’ variable across time points (‘eventname’). It incorporates both random intercepts and slopes for the time points (‘eventname’) within each participant (‘src_subject_id’) to capture individual-level variability in both starting values and rates of change over time. The results of the model are then printed to provide a detailed summary of the fitted model parameters.\nSTEP 1: Compute LMM with Random Intercepts and Slopes\n\n\nCode\n## Linear Mixed Model with a random intercept and random slope (LMM-ris)\nrandom_intercepts_slopes &lt;- lmer(anthroheightcalc ~ 1 + eventname + (1|src_subject_id), data = df, REML=T)\n\nprint(random_intercepts_slopes)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + (1 | src_subject_id)\n   Data: df\nREML criterion at convergence: 201982\nRandom effects:\n Groups         Name        Std.Dev.\n src_subject_id (Intercept) 2.96    \n Residual                   2.26    \nNumber of obs: 40172, groups:  src_subject_id, 11867\nFixed Effects:\n(Intercept)  eventname.L  eventname.Q  eventname.C  eventname^4  \n    60.0627       7.5412      -0.1736      -0.0703       0.1542  \n\n\n\n\nCode\n## Output and reports extending from the LMM-ris analyses\nsummary(random_intercepts_slopes)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + (1 | src_subject_id)\n   Data: df\n\nREML criterion at convergence: 201982\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-21.00  -0.31  -0.01   0.32  65.93 \n\nRandom effects:\n Groups         Name        Variance Std.Dev.\n src_subject_id (Intercept) 8.76     2.96    \n Residual                   5.12     2.26    \nNumber of obs: 40172, groups:  src_subject_id, 11867\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  60.0627     0.0304 1972.86\neventname.L   7.5412     0.0308  244.55\neventname.Q  -0.1736     0.0290   -5.99\neventname.C  -0.0703     0.0320   -2.20\neventname^4   0.1542     0.0286    5.39\n\nCorrelation of Fixed Effects:\n            (Intr) evnt.L evnt.Q evnt.C\neventname.L  0.218                     \neventname.Q  0.024  0.258              \neventname.C -0.075 -0.031  0.389       \neventname^4 -0.072 -0.089  0.208  0.435\n\n\nCode\nconfint(random_intercepts_slopes, level = 0.95, method = \"Wald\")\n\n\n              2.5 %   97.5 %\n.sig01           NA       NA\n.sigma           NA       NA\n(Intercept) 60.0030 60.12236\neventname.L  7.4808  7.60168\neventname.Q -0.2304 -0.11682\neventname.C -0.1330 -0.00762\neventname^4  0.0981  0.21032\n\n\nCode\nreport_performance(random_intercepts_slopes)\n\n\nThe model's total explanatory power is substantial (conditional R2 = 0.78) and the part related to the fixed effects alone (marginal R2) is of 0.41\n\n\nThe code fits a Linear Mixed Model (LMM:ris) to predict children’s height based on different time points (denoted as eventname). Individual variability is captured through random intercepts associated with each participant (src_subject_id). The results suggest a significant change in height across the time points, with the model accounting for 89% (r round(conditional_R2, 2)) of the total variation and 37% (r round(marginal_R2, 2)) attributed to the fixed effects alone.\nIn the model, the linear term for eventname has an estimated effect of r round(model_summary$coefficients[“eventname.L”, “Estimate”], 2), indicating a significant increase in height across time points. Additionally, there’s variability in initial height across participants, reflected by a random intercept standard deviation of r round(random_effects[“src_subject_id”, “Std.Dev.”], 2).\n\n\nThe following set of plots are used to facilitate model diagnostics. The first is a histogram showcasing the distribution of random intercepts for individual subjects, indicating variations in height not explained by the fixed effects. The second depicts residuals versus fitted values, helping assess the model’s fit and potential heteroscedasticity. The third contrasts observed and predicted height values across different time points, offering a side-by-side evaluation of the model’s predictions against actual observations.\n::: blue\n\n\nCode\n# Assuming your model is named `random_intercepts_slope`\n# 1. Extract the random effects\nrandom_effects &lt;- ranef(random_intercepts_slopes)[[1]]\n\n# 2. Convert to dataframe\nrandom_effects_df &lt;- data.frame(Intercept = random_effects$`(Intercept)`)\n\n# Plot 1: Histogram\n# Plot 1: Histogram\nhist_plot &lt;- ggplot(random_effects_df, aes(x = Intercept)) +\n  geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Histogram of Random Effects\", x = \"Random Intercept Values\", y = \"Density\") +\n  theme_minimal()\n\nprint(hist_plot)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Extract the data frame used in the model\nmodel_data &lt;- random_intercepts_slopes@frame\n\n# Extract unique subject IDs from the model's data\noriginal_subject_ids &lt;- unique(model_data$src_subject_id)\n\n# Subset the original data to include only those subjects\ndf_subset &lt;- df %&gt;% filter(src_subject_id %in% original_subject_ids)\n\neventname_map &lt;- c(\n  \"baseline_year_1_arm_1\" = \"Baseline\",\n  \"1_year_follow_up_y_arm_1\" = \"Year_1\",\n  \"2_year_follow_up_y_arm_1\" = \"Year_2\",\n  \"3_year_follow_up_y_arm_1\" = \"Year_3\",\n  \"4_year_follow_up_y_arm_1\" = \"Year_4\"\n)\n\n# Plot\nggplot(df, aes(x = eventname, y = anthroheightcalc, group = src_subject_id)) +\n  \n# Individual estimated height trajectories in faded lines\ngeom_line(aes(group = src_subject_id), alpha = 0.3, color = \"grey50\") +\n\n# Overall group-mean trajectory in blue with increased thickness\nstat_summary(aes(group = 1), fun = mean, geom = \"line\", color = \"blue\", linewidth = 1) +\nlabs(title = \"Individual and Group-Mean Height Trajectories\",\n     x = \"Event Name\",\n     y = \"Height\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe provided code visualizes individual and group-mean height trajectories over different event names. Individual height trajectories for each subject are depicted as faded gray lines, allowing for a clear view of the variability among subjects. In contrast, the overall group-mean trajectory, which represents the average trend across all individuals for each event name, is highlighted in blue. The average height at r mean(df_descriptableHeight) has increased by r mean(df_descriptableHeight_followup - df_descriptable$Height) units from baseline.\n\n\n\n\n\n\n\nWrite-up\n\n\n\nThe linear mixed model analysis was conducted to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4). The eventname predictor was statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[2], digits = 3). The model's overall ability to explain the variance in height was xxxxx, with a conditional R^2 of r report_performance(random_intercepts)Conditional R2[1], indicating that it accounted for this proportion of the variability in height when considering both fixed and random effects. The marginal R^2 was r report_performance(random_intercepts)$Marginal R2[1], meaning that the fixed effects alone explained this proportion of the variability."
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html#overview",
    "href": "3b_Examples_LinearMixedModels.html#overview",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "The Linear Mixed Model with a random intercept and slope (LMM:ris) extends the traditional fixed-effect linear regression by incorporating both a subject-specific random intercept and a random slope. This allows each participant to have their own unique intercept and slope values, reflecting individual starting points and rates of change, in addition to the overall mean-level (fixed-effect) trajectory.\nIn this example, we will utilize the LMM:ris to analyze height trajectories obtained across multiple measurement occasions for a sample of youth participating in the ABCD Study. Our primary objective is to understand the stability and evolution in height measurements, while factoring in the clustered nature of observations within individuals over time. The LMM:ris facilitates this by concurrently modeling an overarching sample mean trajectory (fixed effect) and individual variations (random effects) in both starting points (intercepts) and growth rates (slopes) around this mean trajectory."
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html#preliminary-setup",
    "href": "3b_Examples_LinearMixedModels.html#preliminary-setup",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "Install PackagesLoad PackagesConfig Options\n\n\n\n\nThis code installs the r packages necessary for this example, if they are not already installed\n\n\n\nCode\n# Create a list of required packages\npackages_required &lt;- c(\"tidyverse\",\"rstatix\",\"DT\",\"lme4\",\"report\",\"broom\",\"gridExtra\")\n\n# Check which packages are not installed and install them\npackages_to_install &lt;- setdiff(packages_required, rownames(installed.packages()))\nif (length(packages_to_install) &gt; 0) {\n    install.packages(packages_to_install)\n}\n\n# Load the required packages\nlapply(packages_required, library, character.only = TRUE)\n\n\n\n\n\n\nThis code loads the r libraries necessary for this example\n\n\nCode\nlibrary(tidyverse)    # Collection of R packages for data science\nlibrary(rstatix)      # Pipe-friendly framework for basic statistical tests\nlibrary(DT)           # Rendering interactive data tables\nlibrary(lme4)         # Linear mixed-effects models\nlibrary(report)       # Easy reporting of regression analyses\nlibrary(broom)        # Tidy and augment statistical models output\nlibrary(gridExtra)    # Arrange multiple grid-based plots on a page\n\n\n\n\n\n\nThis code configures knitr code chunk options\n\n\nCode\nknitr::opts_chunk$set(echo = T, message=F, warning=F, error=F, \n                      comment=NA, cache=T, code_folding=T,\n                      R.options=list(width=220, digits = 3),\n                      fig.align='center', \n                      out.width='75%', fig.asp=.75)"
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html#descriptives-overview",
    "href": "3b_Examples_LinearMixedModels.html#descriptives-overview",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "Read and View DataDescriptives\n\n\n\nThis code reads in and shows the data to be used in the current example\n\n\nCode\n# Set the data paths\ndata_path_1 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/demo5.0.rds\"\ndata_path_2 &lt;- \"/Users/shawes/ABCD/data/rds/abcd_5.0_rds/core-rds-5.0/non-imaging_excluding_nt_5.0.rds\"\n\n# Read the data\ndata_demographics &lt;- readRDS(data_path_1)\ndata_nonimaging &lt;- readRDS(data_path_2)\n\n# Subset the nonimaging data to include desired variables\nselected_vars &lt;- c(\"src_subject_id\", \"eventname\", \"nihtbx_totalcomp_fc\", \"anthroweightcalc\", \"anthroheightcalc\")\nsubset_data &lt;- data_nonimaging[, selected_vars]\n\nlibrary(dplyr)\n# # Merge the datasets on 'src_subject_id' and 'eventname'\nmerged_data &lt;- data_demographics %&gt;%\n  full_join(subset_data, by = c(\"src_subject_id\", \"eventname\"))\n\n# Inspect the merged data structure\nstr(merged_data)\n\n# Define event names to be retained in the analysis and convert variables to appropriate data types\neventnames_to_include &lt;- c(\"baseline_year_1_arm_1\",\n                           \"1_year_follow_up_y_arm_1\",\n                           \"2_year_follow_up_y_arm_1\",\n                           \"3_year_follow_up_y_arm_1\",\n                           \"4_year_follow_up_y_arm_1\")\n\ndf &lt;- merged_data %&gt;%\n  filter(eventname %in% eventnames_to_include) %&gt;%\n  mutate(\n    src_subject_id = as.factor(src_subject_id),\n    eventname = factor(eventname, levels = eventnames_to_include, ordered = TRUE),\n    age = as.numeric(age),\n    sex = as.factor(sex),\n    race.4level = as.factor(race.4level),\n    hisp = as.factor(hisp),\n    high.educ.bl = as.factor(high.educ.bl),\n    household.income.bl = as.factor(household.income.bl),\n    acs_raked_propensity_score = as.numeric(acs_raked_propensity_score),\n    rel_family_id.bl = as.factor(rel_family_id.bl),\n    site_id_l = as.factor(site_id_l),\n    nihtbx_totalcomp_fc = as.numeric(nihtbx_totalcomp_fc),\n    anthroweightcalc = as.numeric(anthroweightcalc),\n    anthroheightcalc = as.numeric(anthroheightcalc)\n  ) %&gt;%\n  # Exclude cases from unused assessment waves\n  filter(!is.na(eventname))\n\n\n\n\n\n\nThis code creates a descriptives table\n\n\nCode\n# Define a function to compute descriptives\ncompute_descriptives &lt;- function(data, event_name) {\n  # For factor variables\n  sex_desc &lt;- paste0(table(data$sex), \" (\", round(100 * prop.table(table(data$sex)), 1), \"%)\")\n  race_desc &lt;- paste0(table(data$race.4level), \" (\", round(100 * prop.table(table(data$race.4level)), 1), \"%)\")\n  \n  # For numeric variables\n  age_desc &lt;- paste0(round(mean(data$age, na.rm = TRUE), 2), \" (\", round(sd(data$age, na.rm = TRUE), 2), \")\")\n  weight_desc &lt;- paste0(round(mean(data$anthroweightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroweightcalc, na.rm = TRUE), 2), \")\")\n  height_desc &lt;- paste0(round(mean(data$anthroheightcalc, na.rm = TRUE), 2), \" (\", round(sd(data$anthroheightcalc, na.rm = TRUE), 2), \")\")\n  \n  # Combine into a data frame\n  desc_df &lt;- data.frame(\n    Variable = c(\"Sex - Female\", \"Sex - Male or other\", \"Race - Asian\", \"Race - Black\", \"Race - Other/Mixed\", \"Race - White\", \"Age\", \"Weight\", \"Height\"),\n    Value = c(sex_desc, race_desc, age_desc, weight_desc, height_desc)\n  )\n  \n  # Rename the Value column based on event name\n  colnames(desc_df)[2] &lt;- event_name\n  return(desc_df)\n}\n\n# Compute descriptives for each event\nbaseline_desc &lt;- compute_descriptives(subset(df, eventname == \"baseline_year_1_arm_1\"), \"baseline_year_1_arm_1\")\none_year_desc &lt;- compute_descriptives(subset(df, eventname == \"1_year_follow_up_y_arm_1\"), \"1_year_follow_up_y_arm_1\")\ntwo_year_desc &lt;- compute_descriptives(subset(df, eventname == \"2_year_follow_up_y_arm_1\"), \"2_year_follow_up_y_arm_1\")\nthree_year_desc &lt;- compute_descriptives(subset(df, eventname == \"3_year_follow_up_y_arm_1\"), \"3_year_follow_up_y_arm_1\")\n\n# Join all data frames side-by-side\nfinal_table &lt;- baseline_desc %&gt;%\n  left_join(one_year_desc, by = \"Variable\") %&gt;%\n  left_join(two_year_desc, by = \"Variable\") %&gt;%\n  left_join(three_year_desc, by = \"Variable\")\n\n# Adjust for the required format\nfinal_table[1:6, 3:5] &lt;- NA\n\n# Round numeric values to two decimal places\nnumeric_cols &lt;- sapply(final_table, is.numeric)\nfinal_table[numeric_cols] &lt;- lapply(final_table[numeric_cols], round, 2)\n\n# Create heading rows with the same columns as final_table\nheading_rows &lt;- data.frame(\n  Variable = c(\"Sex\", \"Race\"),\n  baseline_year_1_arm_1 = NA_real_,\n  `1_year_follow_up_y_arm_1` = NA_real_,\n  `2_year_follow_up_y_arm_1` = NA_real_,\n  `3_year_follow_up_y_arm_1` = NA_real_\n)\n\n# Set column names of heading_rows to match final_table\ncolnames(heading_rows) &lt;- colnames(final_table)\n\n# Introduce group labels and adjust the \"Variable\" column\nfinal_table &lt;- rbind(\n                    heading_rows[1,], \n                    final_table[1:2,],\n                    heading_rows[2,], \n                    final_table[3:6,], \n                    final_table[7:9,]\n                    )\n\n# Update the Variable column to remove redundant factor variable name\nfinal_table$Variable &lt;- gsub(\"Sex - \", \"\", final_table$Variable)\nfinal_table$Variable &lt;- gsub(\"Race - \", \"\", final_table$Variable)\nfinal_table$Variable[final_table$Variable == \"Male or other\"] &lt;- \"Male\"\n\n# Add non-breaking spaces for increased indentation\nfinal_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")] &lt;- \n  paste0(rep(\"&nbsp;\", 6), final_table$Variable[final_table$Variable %in% c(\"Female\", \"Male\", \"Asian\", \"Black\", \"Other/Mixed\", \"White\")])\n\n# Update column headers\ncolnames(final_table)[2:5] &lt;- c(\"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\")\n\n# Display the table interactively without row names, with updated column headers, and with HTML entities rendered\ndatatable(final_table, \n          colnames = c(\"\", \"Baseline\", \"Year 1\", \"Year 2\", \"Year 3\"),\n          options = list(pageLength = nrow(final_table), autoWidth = TRUE),\n          rownames = FALSE, escape = FALSE,\n          caption = \"Descriptives Table\")  # Add table title"
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html#results",
    "href": "3b_Examples_LinearMixedModels.html#results",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "::: panel-tabset ### Build Model {.tabset .tabset-fade .tabset-pills}\n::: blue\nThe code fits a linear mixed model to examine the ‘Height’ variable across time points (‘eventname’). It incorporates both random intercepts and slopes for the time points (‘eventname’) within each participant (‘src_subject_id’) to capture individual-level variability in both starting values and rates of change over time. The results of the model are then printed to provide a detailed summary of the fitted model parameters.\nSTEP 1: Compute LMM with Random Intercepts and Slopes\n\n\nCode\n## Linear Mixed Model with a random intercept and random slope (LMM-ris)\nrandom_intercepts_slopes &lt;- lmer(anthroheightcalc ~ 1 + eventname + (1|src_subject_id), data = df, REML=T)\n\nprint(random_intercepts_slopes)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + (1 | src_subject_id)\n   Data: df\nREML criterion at convergence: 201982\nRandom effects:\n Groups         Name        Std.Dev.\n src_subject_id (Intercept) 2.96    \n Residual                   2.26    \nNumber of obs: 40172, groups:  src_subject_id, 11867\nFixed Effects:\n(Intercept)  eventname.L  eventname.Q  eventname.C  eventname^4  \n    60.0627       7.5412      -0.1736      -0.0703       0.1542  \n\n\n\n\nCode\n## Output and reports extending from the LMM-ris analyses\nsummary(random_intercepts_slopes)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: anthroheightcalc ~ 1 + eventname + (1 | src_subject_id)\n   Data: df\n\nREML criterion at convergence: 201982\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-21.00  -0.31  -0.01   0.32  65.93 \n\nRandom effects:\n Groups         Name        Variance Std.Dev.\n src_subject_id (Intercept) 8.76     2.96    \n Residual                   5.12     2.26    \nNumber of obs: 40172, groups:  src_subject_id, 11867\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  60.0627     0.0304 1972.86\neventname.L   7.5412     0.0308  244.55\neventname.Q  -0.1736     0.0290   -5.99\neventname.C  -0.0703     0.0320   -2.20\neventname^4   0.1542     0.0286    5.39\n\nCorrelation of Fixed Effects:\n            (Intr) evnt.L evnt.Q evnt.C\neventname.L  0.218                     \neventname.Q  0.024  0.258              \neventname.C -0.075 -0.031  0.389       \neventname^4 -0.072 -0.089  0.208  0.435\n\n\nCode\nconfint(random_intercepts_slopes, level = 0.95, method = \"Wald\")\n\n\n              2.5 %   97.5 %\n.sig01           NA       NA\n.sigma           NA       NA\n(Intercept) 60.0030 60.12236\neventname.L  7.4808  7.60168\neventname.Q -0.2304 -0.11682\neventname.C -0.1330 -0.00762\neventname^4  0.0981  0.21032\n\n\nCode\nreport_performance(random_intercepts_slopes)\n\n\nThe model's total explanatory power is substantial (conditional R2 = 0.78) and the part related to the fixed effects alone (marginal R2) is of 0.41\n\n\nThe code fits a Linear Mixed Model (LMM:ris) to predict children’s height based on different time points (denoted as eventname). Individual variability is captured through random intercepts associated with each participant (src_subject_id). The results suggest a significant change in height across the time points, with the model accounting for 89% (r round(conditional_R2, 2)) of the total variation and 37% (r round(marginal_R2, 2)) attributed to the fixed effects alone.\nIn the model, the linear term for eventname has an estimated effect of r round(model_summary$coefficients[“eventname.L”, “Estimate”], 2), indicating a significant increase in height across time points. Additionally, there’s variability in initial height across participants, reflected by a random intercept standard deviation of r round(random_effects[“src_subject_id”, “Std.Dev.”], 2).\n\n\nThe following set of plots are used to facilitate model diagnostics. The first is a histogram showcasing the distribution of random intercepts for individual subjects, indicating variations in height not explained by the fixed effects. The second depicts residuals versus fitted values, helping assess the model’s fit and potential heteroscedasticity. The third contrasts observed and predicted height values across different time points, offering a side-by-side evaluation of the model’s predictions against actual observations.\n::: blue\n\n\nCode\n# Assuming your model is named `random_intercepts_slope`\n# 1. Extract the random effects\nrandom_effects &lt;- ranef(random_intercepts_slopes)[[1]]\n\n# 2. Convert to dataframe\nrandom_effects_df &lt;- data.frame(Intercept = random_effects$`(Intercept)`)\n\n# Plot 1: Histogram\n# Plot 1: Histogram\nhist_plot &lt;- ggplot(random_effects_df, aes(x = Intercept)) +\n  geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", fill = \"lightblue\") +\n  labs(title = \"Histogram of Random Effects\", x = \"Random Intercept Values\", y = \"Density\") +\n  theme_minimal()\n\nprint(hist_plot)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Extract the data frame used in the model\nmodel_data &lt;- random_intercepts_slopes@frame\n\n# Extract unique subject IDs from the model's data\noriginal_subject_ids &lt;- unique(model_data$src_subject_id)\n\n# Subset the original data to include only those subjects\ndf_subset &lt;- df %&gt;% filter(src_subject_id %in% original_subject_ids)\n\neventname_map &lt;- c(\n  \"baseline_year_1_arm_1\" = \"Baseline\",\n  \"1_year_follow_up_y_arm_1\" = \"Year_1\",\n  \"2_year_follow_up_y_arm_1\" = \"Year_2\",\n  \"3_year_follow_up_y_arm_1\" = \"Year_3\",\n  \"4_year_follow_up_y_arm_1\" = \"Year_4\"\n)\n\n# Plot\nggplot(df, aes(x = eventname, y = anthroheightcalc, group = src_subject_id)) +\n  \n# Individual estimated height trajectories in faded lines\ngeom_line(aes(group = src_subject_id), alpha = 0.3, color = \"grey50\") +\n\n# Overall group-mean trajectory in blue with increased thickness\nstat_summary(aes(group = 1), fun = mean, geom = \"line\", color = \"blue\", linewidth = 1) +\nlabs(title = \"Individual and Group-Mean Height Trajectories\",\n     x = \"Event Name\",\n     y = \"Height\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe provided code visualizes individual and group-mean height trajectories over different event names. Individual height trajectories for each subject are depicted as faded gray lines, allowing for a clear view of the variability among subjects. In contrast, the overall group-mean trajectory, which represents the average trend across all individuals for each event name, is highlighted in blue. The average height at r mean(df_descriptableHeight) has increased by r mean(df_descriptableHeight_followup - df_descriptable$Height) units from baseline."
  },
  {
    "objectID": "3b_Examples_LinearMixedModels.html#wrapping-up",
    "href": "3b_Examples_LinearMixedModels.html#wrapping-up",
    "title": "Linear Mixed Models: Random Intercept and Slope",
    "section": "",
    "text": "Write-up\n\n\n\nThe linear mixed model analysis was conducted to predict children’s height across different time points (Baseline, Year_1, Year_2, Year_3, and Year_4). The eventname predictor was statistically significant with a p-value of r format.pval(fixed_effectsPr(&gt;|t|)[2], digits = 3). The model's overall ability to explain the variance in height was xxxxx, with a conditional R^2 of r report_performance(random_intercepts)Conditional R2[1], indicating that it accounted for this proportion of the variability in height when considering both fixed and random effects. The marginal R^2 was r report_performance(random_intercepts)$Marginal R2[1], meaning that the fixed effects alone explained this proportion of the variability."
  },
  {
    "objectID": "5_Tutorials_MarginalModels.html",
    "href": "5_Tutorials_MarginalModels.html",
    "title": "Marginal Models",
    "section": "",
    "text": "Marginal models, often termed population-averaged models, are a statistical method used to analyze longitudinal or clustered data. The marginal model estimates the average effect of the independent variables on the outcome while accounting for the within-subject correlation. These models allow for the estimation of population-averaged effects, in contrast to subject-specific effects estimated by random-effects models. Similar to the GEE approach, marginal models account for correlations within repeated measures, but using a different estimation technique that does not account for subject-specific effects. Marginal models expand upon the general linear model, providing a repeated measures framework that is robust to non-normality and non-constant variance, and can handle unbalanced or unequally spaced data. The term marginal in this context is used to emphasize that the model for the mean response at each occasion depends only on the covariates of interest, and not on any random effects or previous responses.\n\n\nCode\ngraph LR\nA(( )) -- Intercept --&gt; B(( ))\nB -- Slope --&gt; C(( ))\nC -- Quadratic --&gt; D(( ))\n\nB -- Y1 --&gt; E[square]\nC -- Y2 --&gt; F[square]\nD -- Y3 --&gt; G[square]\n\n\n\n\ngraph LR\nA(( )) -- Intercept --&gt; B(( ))\nB -- Slope --&gt; C(( ))\nC -- Quadratic --&gt; D(( ))\n\nB -- Y1 --&gt; E[square]\nC -- Y2 --&gt; F[square]\nD -- Y3 --&gt; G[square]\n\n\n\n\n\n\n\nYou should use longitudinal marginal models in the following scenario:\n\nYou want to know: How the average response of a population changes over time or under different conditions.\nYour variable: Is measured repeatedly over time or under varying conditions, like multiple visits to a doctor.\nYou have: Repeated measures that might be correlated, and you want to account for this correlation without specifying a full covariance structure.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal marginal models and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of longitudinal marginal models.\nFit a marginal model to longitudinal data using an example dataset in R.\nInterpret the results and understand the implications of the longitudinal marginal model analysis."
  },
  {
    "objectID": "5_Tutorials_MarginalModels.html#overview",
    "href": "5_Tutorials_MarginalModels.html#overview",
    "title": "Marginal Models",
    "section": "",
    "text": "Marginal models, often termed population-averaged models, are a statistical method used to analyze longitudinal or clustered data. The marginal model estimates the average effect of the independent variables on the outcome while accounting for the within-subject correlation. These models allow for the estimation of population-averaged effects, in contrast to subject-specific effects estimated by random-effects models. Similar to the GEE approach, marginal models account for correlations within repeated measures, but using a different estimation technique that does not account for subject-specific effects. Marginal models expand upon the general linear model, providing a repeated measures framework that is robust to non-normality and non-constant variance, and can handle unbalanced or unequally spaced data. The term marginal in this context is used to emphasize that the model for the mean response at each occasion depends only on the covariates of interest, and not on any random effects or previous responses.\n\n\nCode\ngraph LR\nA(( )) -- Intercept --&gt; B(( ))\nB -- Slope --&gt; C(( ))\nC -- Quadratic --&gt; D(( ))\n\nB -- Y1 --&gt; E[square]\nC -- Y2 --&gt; F[square]\nD -- Y3 --&gt; G[square]\n\n\n\n\ngraph LR\nA(( )) -- Intercept --&gt; B(( ))\nB -- Slope --&gt; C(( ))\nC -- Quadratic --&gt; D(( ))\n\nB -- Y1 --&gt; E[square]\nC -- Y2 --&gt; F[square]\nD -- Y3 --&gt; G[square]\n\n\n\n\n\n\n\nYou should use longitudinal marginal models in the following scenario:\n\nYou want to know: How the average response of a population changes over time or under different conditions.\nYour variable: Is measured repeatedly over time or under varying conditions, like multiple visits to a doctor.\nYou have: Repeated measures that might be correlated, and you want to account for this correlation without specifying a full covariance structure.\n\n\n\n\nIn this tutorial, we will introduce the concept of longitudinal marginal models and guide you through a simple example using a small dataset. By the end of this tutorial, you will be able to:\n\nUnderstand the basic concepts of longitudinal marginal models.\nFit a marginal model to longitudinal data using an example dataset in R.\nInterpret the results and understand the implications of the longitudinal marginal model analysis."
  },
  {
    "objectID": "5_Tutorials_MarginalModels.html#basic-example",
    "href": "5_Tutorials_MarginalModels.html#basic-example",
    "title": "Marginal Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 2000 rows (4 rows for each of the 500 individuals).\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, 0.30, 0.30, \n                      0.30, 1, 0.30, 0.30, \n                      0.30, 0.30, 1, 0.30,\n                      0.30, 0.30, 0.30, 1),\n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data for 4 timepoints\nrandom_probs &lt;- matrix(runif(n * 4), ncol=4)\ncorrelated_probs &lt;- random_probs %*% cholesky\n\n# Convert the probabilities to binary outcomes (using 0.5 as the threshold)\nbinary_outcomes &lt;- ifelse(correlated_probs &gt; 0.5, 1, 0)\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=4),\n  TimePoint = rep(c(\"T1\", \"T2\", \"T3\", \"T4\"), times=n),\n  Job_Satisfaction = as.vector(t(binary_outcomes))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Longitudinal Marginal Model, we will employ the geepack (or nlme) package in R. These models are adept at examining changes over time while considering the non-independence of repeated measurements on the same subjects. In a marginal model, we aim to estimate average population effects, taking into account the correlations between repeated measures on the same individuals.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lme4\" %in% installed.packages())) install.packages(\"geepack\")\nlibrary(geepack)\n\n\n\n\nModel\n\n\nCode\n# Fitting the longitudinal marginal model\nmodel &lt;- geeglm(Job_Satisfaction ~ TimePoint, data = data_large, id = Individual, family = binomial(link = \"logit\"), corstr = \"exchangeable\")\n\n\n\n\nModel Summary\n\n\nCode\nprint(model)\n\n\n\nCall:\ngeeglm(formula = Job_Satisfaction ~ TimePoint, family = binomial(link = \"logit\"), \n    data = data_large, id = Individual, corstr = \"exchangeable\")\n\nCoefficients:\n(Intercept) TimePointT2 TimePointT3 TimePointT4 \n -0.2900158   0.7795640   1.2344774   1.8063633 \n\nDegrees of Freedom: 2000 Total (i.e. Null);  1996 Residual\n\nScale Link:                   identity\nEstimated Scale Parameters:  [1] 1\n\nCorrelation:  Structure = exchangeable    Link = identity \nEstimated Correlation Parameters:\n    alpha \n0.1469716 \n\nNumber of clusters:   500   Maximum cluster size: 4 \n\n\n\n\n\nInterpreting the Results\nA marginal model was fitted using the Generalized Estimating Equations (GEE) approach to assess the effect of TimePoint on Job Satisfaction while accounting for the correlation of observations within individuals. The model used an exchangeable correlation structure, assuming that observations within the same individual have the same correlation. The output of the geeglm function provides estimates for the coefficients, along with their standard errors, Wald statistics, and p-values. The coefficient for TimePoint indicates how the log odds of Job Satisfaction change from one time point to another. If the p-value associated with this coefficient is less than 0.05, it suggests that there is a statistically significant change in Job Satisfaction across the time points. For a more detailed interpretation, we would look at the specific values obtained from the print(model) output, such as:\n\nCoefficient (Estimate) for TimePoint: NA\nAssociated p-value: NA"
  },
  {
    "objectID": "5_Tutorials_MarginalModels.html#conclusion",
    "href": "5_Tutorials_MarginalModels.html#conclusion",
    "title": "Marginal Models",
    "section": "Conclusion",
    "text": "Conclusion\nIf the p-value for the TimePoint coefficient is less than 0.05, we conclude that there is a statistically significant change in Job Satisfaction scores between the time points. Otherwise, there is no significant change based on the marginal model results. The exact nature (increase/decrease) of the change can be inferred from the sign (+/-) of the coefficient."
  },
  {
    "objectID": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html",
    "href": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html",
    "title": "Generalized Linear Mixed Effects Models",
    "section": "",
    "text": "Generalized linear mixed-effects models (GLMMs) represent a sophisticated statistical methodology adept at analyzing intricate data that exhibits both fixed effects and random variability. GLMMs have garnered attention in numerous arenas like ecology, psychology, and medicine, owing to their prowess in accommodating non-normal data and dealing with hierarchical or nested data structures. They can be seen as an evolution of the general linear model (GLM), enriched to include both fixed effects, which capture population-level trends, and random effects, which account for individual or group-specific variability. This becomes invaluable in longitudinal studies, where data points are chronologically collected, and the need to capture both overarching trends and individual-specific trajectories is paramount. In essence, the GLMM paradigm extends the capabilities of GLMs, marrying them with the mixed-effects framework. This fusion allows the model to cater to diverse data distributions while simultaneously capturing both the grand mean trend (fixed effect) and deviations specific to individual or groups (random effects). The seamless blend of general and specific effects is the hallmark of GLMMs, granting them a unique niche in the statistical modeling realm.\n[+add diagrams/figures]\n\n\nYou should employ longitudinal generalized linear mixed effects models in the following scenarios:\n\nYou want to know: How individual trajectories change over time while accounting for within-subject correlations.\nYour variable: Is recorded over multiple time points and may not necessarily follow a normal distribution.\nYou have: Longitudinal data where measurements within subjects are correlated and you want to capture both fixed (population-level) and random (subject-specific) effects in your analysis.\n\n\n\n\nIn this tutorial, we will delve into the world of longitudinal generalized linear mixed effects models, guiding you through an illustrative example using a small dataset. By the conclusion of this tutorial, you will be poised to:\n\nUnderstand the basic principles of generalized linear mixed effects models in longitudinal settings.\nFit these models to longitudinal data in R.\nAnalyze and interpret the results derived from the generalized linear mixed effects model analysis."
  },
  {
    "objectID": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#overview",
    "href": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#overview",
    "title": "Generalized Linear Mixed Effects Models",
    "section": "",
    "text": "Generalized linear mixed-effects models (GLMMs) represent a sophisticated statistical methodology adept at analyzing intricate data that exhibits both fixed effects and random variability. GLMMs have garnered attention in numerous arenas like ecology, psychology, and medicine, owing to their prowess in accommodating non-normal data and dealing with hierarchical or nested data structures. They can be seen as an evolution of the general linear model (GLM), enriched to include both fixed effects, which capture population-level trends, and random effects, which account for individual or group-specific variability. This becomes invaluable in longitudinal studies, where data points are chronologically collected, and the need to capture both overarching trends and individual-specific trajectories is paramount. In essence, the GLMM paradigm extends the capabilities of GLMs, marrying them with the mixed-effects framework. This fusion allows the model to cater to diverse data distributions while simultaneously capturing both the grand mean trend (fixed effect) and deviations specific to individual or groups (random effects). The seamless blend of general and specific effects is the hallmark of GLMMs, granting them a unique niche in the statistical modeling realm.\n[+add diagrams/figures]\n\n\nYou should employ longitudinal generalized linear mixed effects models in the following scenarios:\n\nYou want to know: How individual trajectories change over time while accounting for within-subject correlations.\nYour variable: Is recorded over multiple time points and may not necessarily follow a normal distribution.\nYou have: Longitudinal data where measurements within subjects are correlated and you want to capture both fixed (population-level) and random (subject-specific) effects in your analysis.\n\n\n\n\nIn this tutorial, we will delve into the world of longitudinal generalized linear mixed effects models, guiding you through an illustrative example using a small dataset. By the conclusion of this tutorial, you will be poised to:\n\nUnderstand the basic principles of generalized linear mixed effects models in longitudinal settings.\nFit these models to longitudinal data in R.\nAnalyze and interpret the results derived from the generalized linear mixed effects model analysis."
  },
  {
    "objectID": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#basic-example",
    "href": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#basic-example",
    "title": "Generalized Linear Mixed Effects Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# This will create a dataset with 2000 rows (4 rows for each of the 500 individuals).\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.30, 0.30, 0.30, \n                      0.30, 1, 0.30, 0.30, \n                      0.30, 0.30, 1, 0.30,\n                      0.30, 0.30, 0.30, 1),\n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data for 4 timepoints\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Convert the correlated data to probabilities using the plogis function\ncorrelated_probs &lt;- plogis(correlated_data)\n\n# Convert the probabilities to binary outcomes (using 0.5 as the threshold)\nbinary_outcomes &lt;- ifelse(correlated_probs &gt; 0.5, 1, 0)\n\n# Converting the generated data into a structured data frame\ndata_large &lt;- data.frame(\n  Individual = rep(1:n, each=4),\n  TimePoint = rep(c(\"T1\", \"T2\", \"T3\", \"T4\"), times=n),\n  Job_Satisfaction = as.vector(t(binary_outcomes))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_large)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Generalized Linear Mixed Effects Model (GLMM), we will use the lme4 package in R to establish both the fixed and random effects in our model, accounting for the nested or hierarchical structure of the data by introducing random effects. This approach allows us to model non-normally distributed outcome variables, while capturing both fixed and random effects.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lme4\" %in% installed.packages())) install.packages(\"lme4\")\nlibrary(lme4)\n\n\n\n\nModel\n\n\nCode\nmodel &lt;- glmer(Job_Satisfaction ~ TimePoint + (1 | Individual), data = data_large, family = binomial(link = \"logit\"))\n\n\n\n\nModel Summary\n\n\nCode\nprint(model)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Job_Satisfaction ~ TimePoint + (1 | Individual)\n   Data: data_large\n      AIC       BIC    logLik  deviance  df.resid \n 2708.245  2736.249 -1349.122  2698.245      1995 \nRandom effects:\n Groups     Name        Std.Dev.\n Individual (Intercept) 0.9539  \nNumber of obs: 2000, groups:  Individual, 500\nFixed Effects:\n(Intercept)  TimePointT2  TimePointT3  TimePointT4  \n  -0.019309    -0.106919     0.068009     0.009717  \n\n\n\n\n\nInterpreting the Results\nA Generalized linear mixed-effects model (GLMM) was employed to understand the relationship between Job Satisfaction and TimePoint while accounting for the random effects associated with individuals. The model assumes a binomial distribution for Job Satisfaction with a logit link function.\nFrom the glmer output, we can gather key information: - Fixed effects coefficients, which give us information about the change in the log odds of Job Satisfaction for a unit change in TimePoint. - Random effects variance components, which quantify the variability in intercepts across individuals.\nFor a comprehensive interpretation, we would look at: - Coefficient (Estimate) for TimePoint: r round(coef(model)[“TimePoint”], 3) - Associated p-value: r round(summary(model)$coefficients[“TimePoint”, “Pr(&gt;|z|)”], 3)"
  },
  {
    "objectID": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#conclusion",
    "href": "7_Tutorials_GeneralizedLinearMixedEffectsModels.html#conclusion",
    "title": "Generalized Linear Mixed Effects Models",
    "section": "Conclusion",
    "text": "Conclusion\nIf the p-value for the TimePoint coefficient is less than 0.05, we can conclude that there’s a statistically significant change in Job Satisfaction across the TimePoints according to the GLMM. Otherwise, there isn’t sufficient evidence to support such a change. The direction (increase or decrease) of this change is indicated by the sign of the coefficient for TimePoint."
  },
  {
    "objectID": "9_Tutorials_LatentChangeScoresModels.html",
    "href": "9_Tutorials_LatentChangeScoresModels.html",
    "title": "Latent Change Score Models",
    "section": "",
    "text": "Latent change score models (LCSMs) are a sophisticated statistical method designed to delve deep into the intricacies of individual change over time. LCSMs have gained prominence in fields such as psychology, health sciences, and education for their ability to decipher the underlying structure and magnitude of change. These models represent an evolution of traditional growth curve models, emphasizing the differences between consecutive time points to capture the essence of change. Particularly adept for longitudinal data, LCSMs shine when there’s a need to understand how and why individuals change over multiple time points, offering insights into both the rate and direction of change. At their core, LCSMs utilize a latent variable framework to model the difference scores, ensuring that the observed change is disentangled from measurement error. This unique approach to modeling change, by focusing on the “latent” or hidden aspects of growth or decline, is what lends the method its distinctive name.\n[+add diagrams/figures]\n\n\nYou should consider latent change score models in the following situations:\n\nYou aim to understand: The dynamics of individual change over time.\nYour data: Contains repeated measures spanning multiple time points.\nYou are interested in: Quantifying both the magnitude and direction of change for each individual in your sample.\n\n\n\n\nIn this tutorial, we will guide you through an example of latent change score model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of latent change score models.\nImplement these models on your own longitudinal data using appropriate statistical tools.\nAnalyze and discern the implications derived from the latent change score model analysis."
  },
  {
    "objectID": "9_Tutorials_LatentChangeScoresModels.html#overview",
    "href": "9_Tutorials_LatentChangeScoresModels.html#overview",
    "title": "Latent Change Score Models",
    "section": "",
    "text": "Latent change score models (LCSMs) are a sophisticated statistical method designed to delve deep into the intricacies of individual change over time. LCSMs have gained prominence in fields such as psychology, health sciences, and education for their ability to decipher the underlying structure and magnitude of change. These models represent an evolution of traditional growth curve models, emphasizing the differences between consecutive time points to capture the essence of change. Particularly adept for longitudinal data, LCSMs shine when there’s a need to understand how and why individuals change over multiple time points, offering insights into both the rate and direction of change. At their core, LCSMs utilize a latent variable framework to model the difference scores, ensuring that the observed change is disentangled from measurement error. This unique approach to modeling change, by focusing on the “latent” or hidden aspects of growth or decline, is what lends the method its distinctive name.\n[+add diagrams/figures]\n\n\nYou should consider latent change score models in the following situations:\n\nYou aim to understand: The dynamics of individual change over time.\nYour data: Contains repeated measures spanning multiple time points.\nYou are interested in: Quantifying both the magnitude and direction of change for each individual in your sample.\n\n\n\n\nIn this tutorial, we will guide you through an example of latent change score model using a small dataset. By the time you complete this tutorial, you will be equipped to:\n\nUnderstand the basic principles of latent change score models.\nImplement these models on your own longitudinal data using appropriate statistical tools.\nAnalyze and discern the implications derived from the latent change score model analysis."
  },
  {
    "objectID": "9_Tutorials_LatentChangeScoresModels.html#basic-example",
    "href": "9_Tutorials_LatentChangeScoresModels.html#basic-example",
    "title": "Latent Change Score Models",
    "section": "Basic Example",
    "text": "Basic Example\nIn this tutorial, we will begin by generating a sample dataset directly within R to ensure everyone has the same starting point and can follow along without needing to download or access external files. This generated dataset will serve as our example throughout this tutorial. In your own analyses, you’ll likely start by importing your own data.\nThe simulated dataset created for this example consists of scores on a single outcome variable (“Job_Satisfaction”) for 500 individuals each measured at four time points: T1-T4.\n\nCreate Example Dataset\n\n\nCode\n# Setting the seed for reproducibility\nset.seed(123)\n\n# Setting the number of individuals for our sample data\nn &lt;- 500\n\n# Specifying the desired correlation matrix for our data\ncor_matrix &lt;- matrix(c(1, 0.3, 0.3, 0.3, \n                      0.3, 1, 0.3, 0.3,\n                      0.3, 0.3, 1, 0.3,\n                      0.3, 0.3, 0.3, 1), \n                      nrow=4)\n\n# Using Cholesky decomposition to generate correlated data\ncholesky &lt;- chol(cor_matrix)\n\n# Generating correlated random data\nrandom_data &lt;- matrix(rnorm(n * 4), ncol=4)\ncorrelated_data &lt;- random_data %*% cholesky\n\n# Converting the generated data into a structured data frame in wide format\ndata_wide &lt;- data.frame(\n  Individual = 1:n,\n  Job_Satisfaction_T1 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T2 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T3 = round(runif(n, 5, 10)),\n  Job_Satisfaction_T4 = round(runif(n, 5, 10))\n)\n\n\n\n\nView Dataset\n\n\nCode\nhead(data_wide)\n\n\n\n\n  \n\n\n\n\n\nModel Specification and Estimation\nTo specify a Latent Change Score Model (LCSM), we will employ the lavaan syntax to detail both the measurement model and the structural model. In an LCSM, we focus on modeling the changes in an outcome variable from one time point to the next. This enables us to capture both the magnitude and direction of these changes, while also considering the influences of time-invariant and time-varying predictors.\n\nInstall and Load Necessary Libraries\n\n\nCode\nif (!(\"lavaan\" %in% installed.packages())) {\n  install.packages(\"lavaan\")\n}\n\nlibrary(lavaan)\n\n\n\n\nModel\n\n\nCode\nlibrary(lavaan)\n\n# Define the Latent Change Score Model\nmodel &lt;- '\n    # Measurement part\n    delta12 =~ Job_Satisfaction_T2\n    delta23 =~ Job_Satisfaction_T3\n    delta34 =~ Job_Satisfaction_T4\n    \n    # Structural part (relationships)\n    delta12 ~ Job_Satisfaction_T1\n    delta23 ~ Job_Satisfaction_T2\n    delta34 ~ Job_Satisfaction_T3\n    \n    # Variances\n    delta12 ~~ 1*delta12\n    delta23 ~~ 1*delta23\n    delta34 ~~ 1*delta34\n'\n\n\n\n\nFit Model\n\n\nCode\n# Fit the model\nfit &lt;- sem(model, data = data_wide)\n\n\n\n\nModel Summary\n\n\nCode\n# Summary of the model\nsummary(fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)\n\n\nlavaan 0.6.16 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                           500\n\nModel Test User Model:\n                                                      \n  Test statistic                               606.408\n  Degrees of freedom                                 3\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                                 4.780\n  Degrees of freedom                                 6\n  P-value                                        0.572\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.000\n  Tucker-Lewis Index (TLI)                     990.264\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3015.489\n  Loglikelihood unrestricted model (H1)      -2712.285\n                                                      \n  Akaike (AIC)                                6042.979\n  Bayesian (BIC)                              6068.266\n  Sample-size adjusted Bayesian (SABIC)       6049.222\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.634\n  90 Percent confidence interval - lower         0.592\n  90 Percent confidence interval - upper         0.677\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.298\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  delta12 =~                                                            \n    Job_Stsfctn_T2    1.000                               1.001    1.000\n  delta23 =~                                                            \n    Job_Stsfctn_T3    1.000                               1.000    1.000\n  delta34 =~                                                            \n    Job_Stsfctn_T4    1.000                               1.000    1.000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  delta12 ~                                                             \n    Job_Stsfctn_T1   -0.034    0.031   -1.095    0.274   -0.034   -0.049\n  delta23 ~                                                             \n    Job_Stsfctn_T2   -0.019    0.913   -0.020    0.984   -0.019   -0.019\n  delta34 ~                                                             \n    Job_Stsfctn_T3   -0.022   48.666   -0.000    1.000   -0.022   -0.022\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .delta12 ~~                                                            \n   .delta23          -0.001    0.914   -0.001    1.000   -0.001   -0.001\n   .delta34          -0.032    0.934   -0.034    0.973   -0.032   -0.032\n .delta23 ~~                                                            \n   .delta34          -0.001   48.667   -0.000    1.000   -0.001   -0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .delta12           1.000                               0.998    0.998\n   .delta23           1.000                               1.000    1.000\n   .delta34           1.000                               0.999    0.999\n   .Job_Stsfctn_T2    0.000                               0.000    0.000\n   .Job_Stsfctn_T3    0.000                               0.000    0.000\n   .Job_Stsfctn_T4    0.000                               0.000    0.000\n\nR-Square:\n                   Estimate\n    delta12           0.002\n    delta23           0.000\n    delta34           0.001\n    Job_Stsfctn_T2    1.000\n    Job_Stsfctn_T3    1.000\n    Job_Stsfctn_T4    1.000\n\n\n\n\n\nInterpreting the Results\nA Latent Change Score Model (LCSM) was employed to understand the dynamic changes in scores over time. This model captures the latent change (or difference) between consecutive time points, allowing for a detailed analysis of the trajectory of change.\nFrom the sem output, we can interpret: - The estimated change score from T1 to T2: r round(fit@coef[“delta12”], 3) - The estimated change score from T2 to T3: r round(fit@coef[“delta23”], 3)\nThese coefficients represent the latent change in scores from one time point to the next. A positive coefficient suggests an increase, while a negative coefficient indicates a decrease."
  },
  {
    "objectID": "9_Tutorials_LatentChangeScoresModels.html#conclusion",
    "href": "9_Tutorials_LatentChangeScoresModels.html#conclusion",
    "title": "Latent Change Score Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe LCSM provides insights into how scores evolve over time. By examining the estimated change scores, we can determine whether there’s an increasing or decreasing trend in the scores across the specified time points. The statistical significance of these change scores can further inform us about the reliability of these observed trends."
  }
]